# OCR Результаты: Книга_Python_К_вершинам_мастерства_Рамальо_Лучано

Модель: qwen3-vl-4b
Дата: 2025-12-23 09:51:41
================================================================================


────────────────────────────────────────────────────────────────────────────────
СТРАНИЦА 1
Файл: page_001.png
Время: 0.10с
Символов: 0
❌ ОШИБКА: Error code: 400 - {'error': 'Trying to keep the first 16265 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}

────────────────────────────────────────────────────────────────────────────────
СТРАНИЦА 2
Файл: page_002.png
Время: 0.06с
Символов: 0
❌ ОШИБКА: Error code: 400 - {'error': 'Trying to keep the first 22904 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}

────────────────────────────────────────────────────────────────────────────────
СТРАНИЦА 3
Файл: page_003.png
Время: 0.06с
Символов: 0
❌ ОШИБКА: Error code: 400 - {'error': 'Trying to keep the first 16793 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}

────────────────────────────────────────────────────────────────────────────────
СТРАНИЦА 4
Файл: page_004.png
Время: 0.07с
Символов: 0
❌ ОШИБКА: Error code: 400 - {'error': 'Trying to keep the first 17114 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}

────────────────────────────────────────────────────────────────────────────────
СТРАНИЦА 5
Файл: page_005.png
Время: 0.07с
Символов: 0
❌ ОШИБКА: Error code: 400 - {'error': 'Trying to keep the first 23854 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}
