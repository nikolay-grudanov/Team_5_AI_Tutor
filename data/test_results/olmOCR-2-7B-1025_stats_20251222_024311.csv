folder_name,filename,page_number,text_length,tokens,processing_time,error
o-predelnom-mnogomernom-raspredelenii,page_001.png,1,0,0,0.45738887786865234,"Error code: 400 - {'error': 'Trying to keep the first 500039 tokens when context the overflows. However, the model is loaded with context length of only 63212 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}"
o-predelnom-mnogomernom-raspredelenii,page_002.png,2,0,0,0.41819024085998535,"Error code: 400 - {'error': 'Trying to keep the first 468436 tokens when context the overflows. However, the model is loaded with context length of only 63212 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}"
o-predelnom-mnogomernom-raspredelenii,page_003.png,3,0,0,0.3934440612792969,"Error code: 400 - {'error': 'Trying to keep the first 438816 tokens when context the overflows. However, the model is loaded with context length of only 63212 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}"
o-predelnom-mnogomernom-raspredelenii,page_004.png,4,0,0,0.32460641860961914,"Error code: 400 - {'error': 'Trying to keep the first 278036 tokens when context the overflows. However, the model is loaded with context length of only 63212 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}"
standartizatsiya-i-kachestvo-zhizni,page_001.png,1,0,0,0.5623047351837158,"Error code: 400 - {'error': 'Trying to keep the first 634808 tokens when context the overflows. However, the model is loaded with context length of only 63212 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}"
standartizatsiya-i-kachestvo-zhizni,page_002.png,2,0,0,0.702721357345581,"Error code: 400 - {'error': 'Trying to keep the first 808705 tokens when context the overflows. However, the model is loaded with context length of only 63212 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}"
standartizatsiya-i-kachestvo-zhizni,page_003.png,3,0,0,0.5872361660003662,"Error code: 400 - {'error': 'Trying to keep the first 669332 tokens when context the overflows. However, the model is loaded with context length of only 63212 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}"
Книга_Python_К_вершинам_мастерства_Рамальо_Лучано,page_001.png,1,0,0,0.5098907947540283,"Error code: 400 - {'error': 'Trying to keep the first 437536 tokens when context the overflows. However, the model is loaded with context length of only 63212 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}"
Книга_Python_К_вершинам_мастерства_Рамальо_Лучано,page_002.png,2,0,0,0.9254341125488281,"Error code: 400 - {'error': 'Trying to keep the first 809013 tokens when context the overflows. However, the model is loaded with context length of only 63212 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}"
Книга_Python_К_вершинам_мастерства_Рамальо_Лучано,page_003.png,3,0,0,0.579766035079956,"Error code: 400 - {'error': 'Trying to keep the first 503469 tokens when context the overflows. However, the model is loaded with context length of only 63212 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}"
Книга_Python_К_вершинам_мастерства_Рамальо_Лучано,page_004.png,4,0,0,0.5272352695465088,"Error code: 400 - {'error': 'Trying to keep the first 467934 tokens when context the overflows. However, the model is loaded with context length of only 63212 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}"
Книга_Python_К_вершинам_мастерства_Рамальо_Лучано,page_005.png,5,0,0,0.7428371906280518,"Error code: 400 - {'error': 'Trying to keep the first 663443 tokens when context the overflows. However, the model is loaded with context length of only 63212 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}"
