# OCR Результаты: Книга_Python_К_вершинам_мастерства_Рамальо_Лучано

Модель: olmOCR-2-7B-1025
Дата: 2025-12-22 02:58:17
================================================================================


────────────────────────────────────────────────────────────────────────────────
СТРАНИЦА 1
Файл: page_001.png
Время: 0.05с
Символов: 0
❌ ОШИБКА: Error code: 400 - {'error': 'Trying to keep the first 16276 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}

────────────────────────────────────────────────────────────────────────────────
СТРАНИЦА 2
Файл: page_002.png
Время: 0.06с
Символов: 0
❌ ОШИБКА: Error code: 400 - {'error': 'Trying to keep the first 22915 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}

────────────────────────────────────────────────────────────────────────────────
СТРАНИЦА 3
Файл: page_003.png
Время: 0.05с
Символов: 0
❌ ОШИБКА: Error code: 400 - {'error': 'Trying to keep the first 16804 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}

────────────────────────────────────────────────────────────────────────────────
СТРАНИЦА 4
Файл: page_004.png
Время: 0.05с
Символов: 0
❌ ОШИБКА: Error code: 400 - {'error': 'Trying to keep the first 17125 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}

────────────────────────────────────────────────────────────────────────────────
СТРАНИЦА 5
Файл: page_005.png
Время: 0.06с
Символов: 0
❌ ОШИБКА: Error code: 400 - {'error': 'Trying to keep the first 23865 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}
