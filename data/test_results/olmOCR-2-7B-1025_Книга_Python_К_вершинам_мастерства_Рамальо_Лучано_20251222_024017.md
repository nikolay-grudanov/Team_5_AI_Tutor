# OCR Результаты: Книга_Python_К_вершинам_мастерства_Рамальо_Лучано

Модель: olmOCR-2-7B-1025
Дата: 2025-12-22 02:40:17
================================================================================


────────────────────────────────────────────────────────────────────────────────
СТРАНИЦА 1
Файл: page_001.png
Время: 0.51с
Символов: 0
❌ ОШИБКА: Error code: 400 - {'error': 'Trying to keep the first 437536 tokens when context the overflows. However, the model is loaded with context length of only 32768 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}

────────────────────────────────────────────────────────────────────────────────
СТРАНИЦА 2
Файл: page_002.png
Время: 0.94с
Символов: 0
❌ ОШИБКА: Error code: 400 - {'error': 'Trying to keep the first 809013 tokens when context the overflows. However, the model is loaded with context length of only 32768 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}

────────────────────────────────────────────────────────────────────────────────
СТРАНИЦА 3
Файл: page_003.png
Время: 0.59с
Символов: 0
❌ ОШИБКА: Error code: 400 - {'error': 'Trying to keep the first 503469 tokens when context the overflows. However, the model is loaded with context length of only 32768 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}

────────────────────────────────────────────────────────────────────────────────
СТРАНИЦА 4
Файл: page_004.png
Время: 0.57с
Символов: 0
❌ ОШИБКА: Error code: 400 - {'error': 'Trying to keep the first 467934 tokens when context the overflows. However, the model is loaded with context length of only 32768 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}

────────────────────────────────────────────────────────────────────────────────
СТРАНИЦА 5
Файл: page_005.png
Время: 0.77с
Символов: 0
❌ ОШИБКА: Error code: 400 - {'error': 'Trying to keep the first 663443 tokens when context the overflows. However, the model is loaded with context length of only 32768 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}
