---
source_image: page_062.png
page_number: 62
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 36.97
tokens: 7685
characters: 2569
timestamp: 2025-12-24T02:22:56.976966
finish_reason: stop
---

равной 0.5, но на практике может понадобиться подобрать значение этого гиперпараметра (на основе оценочного набора данных) так, чтобы добиться желаемой точности классификации.

Выбор функции потерь

После подготовки данных и выбора архитектуры модели остается выбрать два жизненно важных компонента машинного обучения с учителем: функцию потерь и оптимизатор. В ситуациях, когда выходные данные модели представляют собой вероятность, самое подходящее семейство функций потерь — функции потерь на основе перекрестной энтропии. Поскольку у нас целевые переменные — бинарные, для нашего модельного примера воспользуемся функцией потерь BCE.

Выбор оптимизатора

Последнее, что осталось выбрать в этом упрощенном примере машинного обучения с учителем, — оптимизатор. Модель генерирует предсказания, функция потерь оценивает расхождение между предсказаниями и целевыми переменными, а оптимизатор обновляет значения весов модели на основе сигнала рассогласования. При простейшем варианте поведение оптимизатора регулируется одним гиперпараметром. Этот гиперпараметр — скорость обучения (learning rate) — определяет, насколько сильно сигнал рассогласования влияет на обновление весов. Скорость обучения — критически важный гиперпараметр, рекомендуется попробовать несколько различных его значений и сравнить результаты. Высокая скорость обучения приводит к большим изменениям параметров и может влиять на сходимость. Слишком низкая скорость обучения может приводить к очень медленному про-движению обучения.

В библиотеке PyTorch имеется несколько вариантов оптимизаторов. Традиционно используется алгоритм стохастического градиентного спуска (stochastic gradient descent, SGD), но в сложных задачах оптимизации в нем часто возникают проблемы со сходимостью, приводящие к худшим моделям. В настоящее время предпочитают использовать адаптивные оптимизаторы, в частности Adagrad (adaptive gradient — «адаптивный градиент». — Примеч. пер.) и Adam, учитывающие историю обновлений1. В следующем примере мы воспользуемся оптимизатором Adam, но никогда не помешает рассмотреть несколько вариантов. В оптимизаторе Adam скорость обучения по умолчанию равна 0.001. Для таких гиперпараметров, как скорость обучения, всегда рекомендуется использовать сначала значения по умолчанию, разве что вы основываетесь на статье, в которой рекомендуется какое-либо другое конкретное значение.

1 В сообществах, посвященных машинному обучению и оптимизации, непрерывно ведутся споры относительно достоинств и недостатков SGD. Нам кажется, что, хотя подобные споры и познавательны, они только мешают.