---
source_image: page_104.png
page_number: 104
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 36.40
tokens: 7663
characters: 2441
timestamp: 2025-12-24T02:24:04.052928
finish_reason: stop
---

Итоговый тензор. Значение tensor.shape должно быть (batch, output_dim).
"""
intermediate = F.relu(self.fc1(x_in))
output = self.fc2(F.dropout(intermediate, p=0.5))

if apply_softmax:
    output = F.softmax(output, dim=1)
return output

Важно отметить, что дропаут применяется только во время обучения, а не оценки. В качестве упражнения рекомендуем вам поэкспериментировать с моделью SurnameClassifier с дропаутом и посмотреть, как изменятся результаты.

Сверточные нейронные сети

В первой части главы мы подробно изучили MLP — нейронные сети, построенные из ряда линейных слоев и нелинейных функций. MLP — не лучший инструмент для того, чтобы использовать возможности последовательных паттернов¹. Например, в наборе данных фамилий определенные участки могут в значительной степени раскрывать национальность их носителей (как O’ в O’Neil, opulos в Antonopulos, sawa в Nagasawa или Zh в Zhu). Длины этих участков могут быть различными, сложность состоит в том, чтобы уловить их без явного кодирования.

Рассмотрим сверточные нейронные сети — тип нейронных сетей, хорошо подходящий для обнаружения пространственной субструктуры (а значит, и создания осмысленной пространственной субструктуры). CNN добиваются этого за счет небольшого количества весов, используемых для просмотра входящих тензоров данных. В результате этого просмотра генерируются выходные тензоры, отражающие обнаружение (или не обнаружение) субструктур.

Мы начнем с описания способов функционирования CNN и вопросов их проектирования. Подробно обсудим гиперпараметры CNN, чтобы вы научились интуитивно чувствовать их поведение и влияние на выходные результаты. И наконец, мы шаг за шагом пройдем по нескольким простым примерам, иллюстрирующим механизм работы CNN. В разделе «Пример: классификация фамилий с помощью CNN» на с. 130 мы рассмотрим более масштабный пример.

¹ Можно спроектировать такой MLP, который бы принимал на входе символные биграммы, чтобы уловить некоторые из подобных зависимостей. Количество биграмм для английского языка (алфавит которого состоит из 26 букв) составляет 325. Так что при скрытом слое из 100 узлов число параметров для входного скрытого слоя будет \( 325 \times 100 \). А если рассматривать все возможные символные триграммы, то получим еще \( 2600 \times 100 \) параметров. Как мы увидим, сверточные нейронные сети позволяют уловить ту же информацию при намного меньшем числе параметров благодаря совместному использованию параметров.