---
source_image: page_201.png
page_number: 201
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 33.58
tokens: 7471
characters: 2008
timestamp: 2025-12-24T02:26:37.234480
finish_reason: stop
---

модели, позволяющие выжать из последовательности максимум. Мы также научились использовать механизм внимания для эффективного захвата зависимостей из более «далекого» контекста. Наконец, поговорили об оценке моделей преобразования последовательностей в последовательности и рассмотрели соответствующий комплексный пример машинного перевода. До сих пор каждая глава книги была посвящена конкретной архитектуре нейронной сети. В следующей главе мы соберем изложенное во всех предыдущих главах воедино и рассмотрим примеры создания реальных систем путем сочетания различных архитектур моделей.

Библиография

1. Bengio Y., Simard P., Frasconi P. Learning Long-Term Dependencies with Gradient Descent is Difficult // IEEE Transactions on Neural Networks 5. 1994.
2. Bahdanau D., Cho K., Bengio Y. Neural Machine Translation by Jointly Learning to Align and Translate // Proceedings of the International Conference on Learning Representations. 2015.
3. Papineni K. et al. BLEU: A Method for Automatic Evaluation of Machine Translation // Proceedings of the 40th Annual Meeting of the ACL. 2002.
4. Daumé III H., Langford J., Marcu D. Search-Based Structured Prediction // Machine Learning Journal. 2009.
5. Bengio S. et al. Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks // Proceedings of NIPS. 2015.
6. Luong M.-T., Pham H., Manning C. D. Effective Approaches to Attention-Based Neural Machine Translation // Proceedings of EMNLP. 2015.
7. Le P., Zuidema W. Quantifying the Vanishing Gradient and Long Distance Dependency Problem in Recursive Neural Networks and Recursive LSTMs // Proceedings of the 1st Workshop on Representation Learning for NLP. 2016.
8. Koehn P., Knowles R. Six Challenges for Neural Machine Translation // Proceedings of the 1st Workshop on Neural Machine Translation. 2017.
9. Neubig G. Neural Machine Translation and Sequence-to-Sequence Models: A Tutorial. 2017. arXiv: 1703.01619.
10. Vaswani A. et al. Attention is all you need // Proceedings of NIPS. 2017.