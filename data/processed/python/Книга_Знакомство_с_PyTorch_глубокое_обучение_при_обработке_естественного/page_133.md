---
source_image: page_133.png
page_number: 133
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 23.75
tokens: 7459
characters: 1793
timestamp: 2025-12-24T02:24:37.998175
finish_reason: stop
---

Пример: обучение вложениям модели непрерывного мультимножества слов

Аргументы:
    x_in (torch.Tensor): тензор входных данных
        Значение x_in.shape должно быть (batch, input_dim)
    apply_softmax (bool): флаг для многомерной логистической функции активации. При использовании функции потерь на основе перекрестной энтропии должен равняться false
Возвращает:
    итоговый тензор. Значение tensor.shape должно быть (batch, output_dim).
"""
x_embedded_sum = self.embedding(x_in).sum(dim=1)
y_out = self.fc1(x_embedded_sum)

if apply_softmax:
    y_out = F.softmax(y_out, dim=1)

return y_out

Процедура обучения

В этом примере процедура обучения следует образцу, применяемому на протяжении всей книги. Сначала инициализируем набор данных, векторизатор, модель, функцию потерь и оптимизатор. Затем проходим в цикле по обучающему и проверочному фрагментам набора данных на протяжении определенного количества эпох, минимизируем потери на обучающем фрагменте и оцениваем эффективность модели на проверочном. Больше подробностей относительно процедуры обучения вы можете найти в разделе «Пример: классификация тональностей обзоров ресторанов» на с. 76, где мы рассмотрели ее подробнее. В примере 5.10 приведены используемые при обучении аргументы.

Пример 5.10. Аргументы для сценария обучения CBOW

Input[0]
args = Namespace(
    # Информация о данных и путях
    cbow_csv="data/books/frankenstein_with_splits.csv",
    vectorizer_file="vectorizer.json",
    model_state_file="model.pth",
    save_dir="model_storage/ch5/cbow",
    # Гиперпараметры модели
    embedding_size=300,
    # Гиперпараметры обучения
    seed=1337,
    num_epochs=100,
    learning_rate=0.001,
    batch_size=128,
    early_stopping_criteria=5,
    # Настройки времени выполнения не приводятся для экономии места
)