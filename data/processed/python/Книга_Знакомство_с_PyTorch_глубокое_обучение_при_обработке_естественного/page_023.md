---
source_image: page_023.png
page_number: 23
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 33.80
tokens: 7581
characters: 2347
timestamp: 2025-12-24T02:21:50.486142
finish_reason: stop
---

Существует множество вариантов общего алгоритма SGD, нацеленных на ускорение сходимости. В следующих главах мы рассмотрим некоторые из них, а также поговорим об использовании градиентов при обновлении значений параметров. Этот процесс итеративного обновления значений параметров называется методом обратного распространения ошибки (backpropagation). Каждый шаг (так называемая эпоха) алгоритма обратного распространения ошибки состоит из прямого прохода (forward pass) и обратного прохода (backward pass). При прямом проходе выполняется вычисление наблюдаемых величин при текущих значениях параметров и рассчитывается функция потерь. На обратном шаге значения параметров обновляются на основе градиента потерь.

Обратите внимание, что все вышеизложенное относится отнюдь не только к глубокому обучению или нейронным сетям1. Стрелки на рис. 1.1 указывают направление «движения» данных при обучении системы. Мы еще вернемся к обучению и понятию «движения» данных в разделе «Графы вычислений» далее, но сначала взглянем на возможные способы численного представления наших входных данных и целевых переменных в задачах NLP для обучения моделей и предсказания целевых переменных.

Кодирование наблюдаемых величин и целевых переменных

Чтобы использовать наблюдаемые величины (текст) в алгоритмах машинного обучения, необходимо представить их в числовом виде. Наглядная иллюстрация этого процесса приведена на рис. 1.2.

Использование числового вектора — простой способ представления текста. Существует множество способов выполнения подобного отображения/представления. На самом деле значительная часть данной книги посвящена тому, как путем обучения получить для задачи подобное представление на основе имеющихся данных. Однако мы начнем с нескольких простых эвристических представлений, в основе которых лежат подсчеты слов в тексте. Несмотря на простоту, они могут оказаться очень полезными в качестве отправного пункта для более многообещающего обучения представлениям. Все эти представления на основе подсчетов начинаются с вектора фиксированной размерности.

1 Глубокое обучение отличается от традиционных нейронных сетей, обсуждавшихся в литературе до 2006 года, тем, что относится к постоянно расширяющемуся набору методик обеспечения надежности за счет добавления дополнительных уровней сети. Мы расскажем, почему это так важно, в главах 3 и 4.