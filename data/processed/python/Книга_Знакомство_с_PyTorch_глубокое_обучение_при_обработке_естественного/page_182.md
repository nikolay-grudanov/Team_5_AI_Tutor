---
source_image: page_182.png
page_number: 182
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 24.33
tokens: 7387
characters: 1645
timestamp: 2025-12-24T02:25:59.289253
finish_reason: stop
---

глобальным вниманием (global attention). Локальное же внимание представляет собой механизм, зависящий только от некоего окна входных данных около текущего временного шага.

![Внимание в действии на временному шаге t = 0 декодировщика](../images/chapter8/fig8_9.png)

Рис. 8.9. Внимание в действии на временному шаге t = 0 декодировщика. Предсказанные выходные данные — for, и блок внимания учитывает скрытые состояния кодировщика \( \phi_w \) для всех входных слов

Иногда, особенно при машинном переводе, можно явным образом передать информацию о выравнивании в виде части обучающих данных. В подобных случаях можно создать механизм внимания с учителем (supervised attention mechanism), при котором функция внимания усваивается с помощью отдельной, совместно обучаемой нейронной сети. Для входных данных большого размера (например, документов) можно разработать более грубый или более тонкий (иерархический) механизм внимания, который не только концентрируется на непосредственных входных данных, но и учитывает структуру документа — абзац, раздел, главу и т. д.

В посвященной нейронным сетям с преобразованиями (transformer networks) работе Васвани и др. (Vaswani et al., 2017) описывается механизм мультивнимания (multiheaded attention), при котором несколько векторов внимания используются для отслеживания различных областей входных данных. В ней также описано аутовнимание (self-attention), механизм, с помощью которого модель узнает об областях входных данных и их влиянии друг на друга.

Если входные данные комбинированные (например, включают как речь, так и изображение), можно создать механизм комбинированного (multimodal) внимания.