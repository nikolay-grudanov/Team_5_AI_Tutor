---
source_image: page_089.png
page_number: 89
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 33.44
tokens: 7662
characters: 2473
timestamp: 2025-12-24T02:23:33.540798
finish_reason: stop
---

Реализация многослойных перцептронов в PyTorch

В предыдущем подразделе мы вкратце описали основные идеи MLP. Здесь шаг за шагом рассмотрим его реализацию на PyTorch. Как мы описали выше, MLP, в отличие от обычного перцептрона из главы 3, включает еще один слой вычислений. В реализации, представленной в примере 4.1, воплощена идея с двумя модулями Linear PyTorch. Объекты Linear называются здесь fc1 и fc2 по общепринятому соглашению, именующему модуль Linear полносвязным слоем (fully connected layer, или для краткости fc layer)¹. Помимо этих двух линейных слоев, в нем присутствует нелинейность — выпрямленный линейный блок (ReLU, с которым мы познакомились в разделе «Функции активации» на с. 60) — он применяется к выходным результатам первого линейного слоя до отправки их на вход второму линейному слою. По причине последовательной сущности слоев необходимо убедиться в том, что количество выходных значений слоя равно количеству входных значений следующего слоя. Нелинейный блок между двумя линейными слоями нужен, поскольку без него два линейных слоя математически эквивалентны одному², а значит, не смогут моделировать сложные паттерны. В нашей реализации MLP выполняется только прямой проход метода обратного распространения ошибки. Дело в том, что фреймворк PyTorch автоматически выясняет, как выполнить обратный проход и обновить градиенты, на основе определения модели и реализации прямого прохода.

Пример 4.1. Реализация многослойного перцептрона с помощью фреймворка PyTorch

import torch.nn as nn
import torch.nn.functional as F

class MultilayerPerceptron(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        """
        Аргументы:
            input_dim (int): размер входных векторов
            hidden_dim (int): размер выходных результатов первого линейного слоя
            output_dim (int): размер выходных результатов второго линейного слоя
        """
        super(MultilayerPerceptron, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, output_dim)

    def forward(self, x_in, apply_softmax=False):
        """
        Прямой проход MLP

        Аргументы:
        """
_____________________
¹ Это распространенная практика в литературе по глубокому обучению. Если полносвязных слоев более одного, они нумеруются слева направо: fc-1, fc-2 и т. д.
² Это можно легко доказать, записав уравнения линейных слоев. Предлагаем вам сделать это в качестве упражнения.