---
source_image: page_137.png
page_number: 137
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 41.87
tokens: 7814
characters: 2718
timestamp: 2025-12-24T02:25:02.208169
finish_reason: stop
---

вой фильтрации при определенной частоте. Основная цель этого действия — улучшение качества сигнала для модели и снижение использования моделью памяти за счет отброса зашумленных, редко встречающихся слов.

<table>
  <tr>
    <th>SequenceVocabulary</th>
    <th>Jerry is happy</th>
    <th>Шаг 0: исходное предложение</th>
  </tr>
  <tr>
    <td>
      {
        '<MASK>': 0,
        '<UNK>': 1,
        '<BEGIN-OF-SEQUENCE>': 2,
        '<END-OF-SEQUENCE>': 3,
        'is': 4,
        'happy': 5
      }
    </td>
    <td></td>
    <td></td>
  </tr>
  <tr>
    <td></td>
    <td>1 4 5</td>
    <td>Шаг 1: используем Vocabulary для отображения слов в целые числа</td>
  </tr>
  <tr>
    <td></td>
    <td>2 1 4 5 3</td>
    <td>Шаг 2: охватываем предложение маркерами границ</td>
  </tr>
  <tr>
    <td></td>
    <td>2 1 4 5 3 0 0</td>
    <td>Шаг 3: дополняем векторы нулями для выравнивания их длины</td>
  </tr>
</table>

Рис. 5.3. Простой пример конвейера векторизации с сокращенным до минимума классом SequenceVocabulary, содержащим четыре описанных выше специальных токена. Во-первых, он используется для отображения слов в последовательность целых чисел. Поскольку слова Jerry нет в SequenceVocabulary, оно отображается в целочисленное значение UNK. Далее к целым числам в начале и конце присоединяются специальные токены, отмечающие границы предложения. Наконец, целочисленные значения дополняются справа нулями до заданной длины, чтобы все векторы в наборе данных были одинаковой длины

После создания экземпляра класса Vectorizer его метод vectorize() принимает на входе название новостной статьи и возвращает вектор, длина которого соответствует самому длинному заголовку статьи из набора данных (пример 5.12). Он отвечает за два основных действия. Во-первых, локально сохраняет максимальную длину последовательности. Обычно за ее отслеживание отвечает набор данных, и во время вывода в качестве вектора применяется длина контрольной последовательности. Впрочем, при использовании CNN-модели важно поддерживать статический размер даже во время вывода. Во-вторых, как показано во фрагменте кода из примера 5.11, он возвращает дополненный нулями вектор целых чисел, отражающий слова из последовательности. Кроме того, в начало этого вектора целых чисел добавлено число для токена BEGIN-OF-SEQUENCE, а в конец — для END-OF-SEQUENCE. С точки зрения классификатора эти специальные токены отмечают границы последовательности, благодаря чему он может обрабатывать слова возле границ не так, как слова ближе к центру¹.

¹ Важно отметить, что такое поведение допустимо, но не обязательно. В наборе данных должны быть свидетельства практической пользы такого поведения и его благоприятного влияния на итоговые потери.