---
source_image: page_079.png
page_number: 79
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 40.55
tokens: 7784
characters: 2986
timestamp: 2025-12-24T02:23:27.598475
finish_reason: stop
---

и точности от пакета к пакету. Более подробную информацию о примененной здесь «формуле скользящего среднего» вы можете найти в статье «Википедии» (http://bit.ly/2Ezb9DP и https://ru.wikipedia.org/wiki/Скользящая_средняя). Наконец, мы вызываем метод .train() классификатора, указывающий, что модель находится в «режиме обучения» и параметры модели изменяемые. Это также активизирует механизмы регуляризации, например дропаут (см. подраздел «Регуляризация многослойных перцептронов: регуляризация весов и структурная регуляризация» на с. №№).

В следующей части цикла обучения мы проходим по пакетам обучения в batch_generator и выполняем нужные для обновления параметров модели операции. В процессе итерации по каждому пакету сначала сбрасываются значения градиентов с помощью метода optimizer.zero_grad(). Далее вычисляются выходные значения модели. После этого с помощью функции потерь вычисляются потери — разница между выходными значениями модели и целевыми значениями (фактическими метками классов). Вслед за этим вызывается метод loss.backward() объекта потерь (не объекта функции потерь), в результате чего градиенты транслируются всем параметрам. Наконец, на основе этих транслированных значений оптимизатор обновляет параметры с помощью метода optimizer.step(). Перечисленные пять шагов — необходимые ступеньки градиентного спуска. Помимо них, требуется еще несколько дополнительных операций для учета и отслеживания. Если точнее, то вычисляются (и сохраняются в обычных переменных Python) значения потерь и точности, а затем они используются для обновления переменных для текущих потерь и текущей точности.

После завершения внутреннего цикла по пакетам обучающего фрагмента выполняется несколько дополнительных операций учета и создания/инициализации. Сначала состояние обучения обновляется на основе окончательных значений потерь и точности. Далее создаются новый генератор пакетов и переменные для текущих потерь и текущей точности. Цикл по проверочным данным почти ничем не отличается от цикла по обучающим, так что мы повторно воспользуемся теми же переменными. Впрочем, есть одно существенное отличие: вызывается метод .eval() классификатора, выполняющий обратную по отношению к методу .train() классификатора операцию. Метод .eval() делает параметры модели неизменяемыми и отключает дропаут. В режиме проверки также отключается вычисление потерь и трансляция градиентов обратно в параметры. Это важно, поскольку подстройка параметров модели под проверочные данные нежелательна. Взамен эти данные должны служить мерой эффективности работы модели. Если эффективность ее работы на обучающих и проверочных данных сильно расходится, то модель, вероятно, переобучилась на обучающих данных и следует поменять модель или процедуру обучения (например, использовать ранний останов, как мы делаем в прилагаемых к книге материалах).

Внешний цикл for завершается проходом по проверочным данным и сохранением полученных значений потерь и точности. Все реализованные в этой книге про-