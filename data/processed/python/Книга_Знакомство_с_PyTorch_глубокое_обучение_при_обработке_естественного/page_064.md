---
source_image: page_064.png
page_number: 64
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 33.36
tokens: 7593
characters: 2207
timestamp: 2025-12-24T02:22:53.552125
finish_reason: stop
---

В литературе, в том числе в этой книге, попеременно используются термины «пакет» и «мини-пакет», подчеркивая то, что пакеты по отдельности существенно меньше общего размера обучающей последовательности; например, размер обучающей последовательности может исчисляться миллионами, а мини-пакета — лишь несколькими сотнями.

Обработка определенного числа пакетов (обычно числа пакетов в наборе данных конечной величины) в цикле обучения называется эпохой (epoch). Эпоха — одна полная итерация обучения. Если количество пакетов в одной эпохе равно количеству пакетов в наборе данных, то эпоха представляет собой полную итерацию обработки набора данных. Обучение модели состоит из определенного числа эпох. Выбор этого числа — задача нетривиальная, но есть способы, которые мы вскоре обсудим, позволяющие выбрать момент прекращения обучения. Как видно из примера 3.11, цикл обучения с учителем является, таким образом, вложенным: внутренний цикл обрабатывает набор данных или заданное число пакетов, а внешний цикл повторяет внутренний заданное число эпох или использует другой критерий завершения.

Пример 3.11. Цикл обучения с учителем для перцептрона и бинарной классификации

# Каждая из эпох представляет собой полный проход
# по обучающей последовательности
for epoch_i in range(n_epochs):
    # Проходим во внутреннем цикле по пакетам набора данных
    for batch_i in range(n_batches):
        # Шаг 0: получаем данные
        x_data, y_target = get_toy_data(batch_size)

        # Шаг 1: очищаем значения градиентов
        perceptron.zero_grad()

        # Шаг 2: вычисляем прямой проход модели
        y_pred = perceptron(x_data, apply_sigmoid=True)

        # Шаг 3: вычисляем оптимизируемую величину потерь
        loss = bce_loss(y_pred, y_target)

        # Шаг 4: транслируем сигнал потерь обратно по графу вычислений
        loss.backward()

        # Шаг 5: запускаем выполнение обновления оптимизатором
        optimizer.step()

Вспомогательные понятия машинного обучения

Основная идея градиентного машинного обучения с учителем проста: описывается модель, вычисляются выходные значения, вычисляются градиенты на основе выбранной функции потерь, после чего применяется алгоритм оптимизации для