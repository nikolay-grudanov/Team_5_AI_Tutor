---
source_image: page_116.png
page_number: 116
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 37.00
tokens: 7751
characters: 2487
timestamp: 2025-12-24T02:24:23.291545
finish_reason: stop
---

Резюме

В главе мы познакомили вас с двумя основными упреждающими архитектурами: многослойным перцептроном (MLP; также называемым полносвязной сетью) и сверточной нейронной сетью (CNN). Мы наблюдали широкие возможности MLP по аппроксимации произвольных нелинейных функций и показали применение сверточных нейронных сетей в NLP на примере классификации фамилий по национальности. Мы исследовали один из основных недостатков/ограничений MLP — отсутствие разделения параметров — и продемонстрировали архитектуру сверточных нейронных сетей в качестве возможного решения этой проблемы. CNN, изначально предназначенные для машинного зрения, стали главным направлением NLP в основном благодаря высокоэффективным реализациям и низким требованиям к памяти. Мы изучили несколько вариантов сверток — с дополнением нулями, расширением и шагом свертки — и преобразование ими входного пространства. Немалая часть дискуссии в этой главе посвящена также практическому вопросу выбора входных и выходных размеров фильтров сверток. Мы показали, как операция свертки помогает захватывать информацию о субструктурах языка, для чего воспользовались в примере классификации фамилий сверточными нейронными сетями. Наконец, мы обсудили некоторые побочные, но важные вопросы, связанные с архитектурой сверточных нейронных сетей: 1) субдискретизацию; 2) пакетную нормализацию; 3) свертку 1 × 1; 4) остаточные связи. В современной архитектуре CNN часто используется одновременно несколько подобных трюков. Например, в архитектуре Inception (см. статью Шегеди и др. [Szegedy et al., 2015]), где благодаря внимательному применению подобных уловок стало возможно создавать сверточные нейронные сети глубиной в сотни слоев — не только точные, но и быстро обучаемые. В главе 5 мы исследуем вопрос обучения и использования представлений для таких дискретных блоков, как слова, предложения, документы и другие типы признаков, с применением вложений.

Библиография

1. Lin M., Chen Q., Yan S. Network in network. arXiv preprint arXiv: 1312.4400. 2013.
2. Szegedy C., Liu W., Jia Y., Sermanet P., Reed S., Anguelov D., Erhan D., Vanhoucke V., Rabinovich A. Going deeper with convolutions // CVPR. 2015.
3. Clevert D.-A., Unterthiner T., Hochreiter S. Fast and accurate deep network learning by exponential linear units (elus). arXiv preprint arXiv: 1511.07289. 2015.
4. Ioffe S., Szegedy C. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv: 1502.03167. 2015.