---
source_image: page_080.png
page_number: 80
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 32.39
tokens: 7461
characters: 1979
timestamp: 2025-12-24T02:23:21.077536
finish_reason: stop
---

цедуры обучения следуют практически одному паттерну проектирования. На самом деле вообще все алгоритмы градиентного спуска следуют схожим паттернам проектирования. Когда написание этого цикла с нуля станет для вас привычным, вы поймете, что такое градиентный спуск.

Оценка, вывод и просмотр

Следующие шаги после обучения модели — проверка ее эффективности на заранее выделенной порции данных, использование ее для выполнения вывода на основе новых данных или осмотр весов модели и выяснение, чему же она обучилась. В данном подразделе мы разберем все эти три шага.

Оценка на контрольных данных

Код оценки эффективности модели на выделенном контрольном наборе почти ничем не отличается от цикла проверки в процедуре обучения из предыдущего примера с одним лишь небольшим нюансом: задан фрагмент 'test', а не 'val'. Различие между этими двумя фрагментами набора данных состоит в том, что контрольный набор следует использовать как можно меньше. При каждом запуске обученной модели на контрольном наборе, принятии на основании этого новых решений относительно модели (например, изменения размера слоев) и повторной оценки заново обученной модели на контрольном наборе модельные решения смещаются в сторону контрольных данных. Другими словами, при частом повторении этого процесса контрольные данные не смогут служить точной мерой, как действительно выделенные данные. В этом можно убедиться на примере 3.22.

Пример 3.22. Оценка на контрольном наборе

Input[0]

dataset.set_split('test')
batch_generator = generate_batches(dataset,
    batch_size=args.batch_size,
    device=args.device)

running_loss = 0.
running_acc = 0.
classifier.eval()

for batch_index, batch_dict in enumerate(batch_generator):
    # вычисление выходных значений
    y_pred = classifier(x_in=batch_dict['x_data'].float())

    # вычисление потерь
    loss = loss_func(y_pred, batch_dict['y_target'].float())
    loss_batch = loss.item()
    running_loss += (loss_batch - running_loss) / (batch_index + 1)