---
source_image: page_056.png
page_number: 56
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 30.30
tokens: 7535
characters: 1808
timestamp: 2025-12-24T02:22:40.184762
finish_reason: stop
---

с дискретной перекрестной энтропией, описанной в подразделе «Углубляемся в обучение с учителем» на с. 68.

Пример 3.6. Многомерная логистическая функция активации

Input[0]
import torch.nn as nn
import torch

softmax = nn.Softmax(dim=1)
x_input = torch.randn(1, 3)
y_output = softmax(x_input)
print(x_input)
print(y_output)
print(torch.sum(y_output, dim=1))

Output[0]
tensor([[ 0.5836, -1.3749, -1.1229]])
tensor([[ 0.7561,  0.1067,  0.1372]])
tensor([ 1.])

В этом разделе мы изучили четыре важные функции активации: сигма-функцию, гиперболический тангенс, ReLU и многомерную логистическую функцию. Это лишь четыре из множества возможных функций активации, применимых при создании нейронных сетей. По мере чтения данной книги вам станет понятно, где и какие функции активации следует применять, но общий совет: используйте то, что работало раньше.

Функции потерь

В главе 1 мы рассмотрели общую архитектуру машинного обучения с учителем и поговорили о том, как функции потерь (целевые функции) помогают обучаемому алгоритму в выборе правильных параметров на основе данных. Напомним, что функция потерь принимает на входе контрольные значения (\( y \)) и предсказания (\( \hat{y} \)) и генерирует вещественный показатель. Чем больше значение этого показателя, тем хуже предсказание, производимое данной моделью. PyTorch реализует в пакете \( nn \) больше функций потерь, чем мы можем здесь охватить, но несколько наиболее используемых мы все же рассмотрим.

Среднеквадратичная погрешность

Для задач регрессии, в которых выходные значения сети (\( \hat{y} \)) и целевая переменная (\( y \)) представляют собой непрерывные значения, в качестве функции потерь часто применяется среднеквадратичная погрешность (mean squared error, MSE):

\[
L_{MSE}(y, \hat{y}) = \frac{1}{n} \sum_{i=1}^{n} (y - \hat{y})^2.
\]