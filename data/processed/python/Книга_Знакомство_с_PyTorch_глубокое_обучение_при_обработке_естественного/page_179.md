---
source_image: page_179.png
page_number: 179
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 33.85
tokens: 7696
characters: 2454
timestamp: 2025-12-24T02:26:03.246318
finish_reason: stop
---

Захватываем больше информации из последовательности: внимание

Одна из проблем с формой S2S-модели, описанной в начале главы на с. №№, состоит в том, что она «втискивает» (кодирует) целое входное предложение в один вектор, φ, на основе которого и генерирует выходные данные (рис. 8.7). Хотя для очень коротких предложений это может сработать, в случае длинных подобные модели оказываются неспособны уловить информацию из всех входных данных¹. Такие ограничения накладывает использование в качестве кодировки одного лишь итогового скрытого состояния. Еще одна проблема длинных элементов входных данных — «исчезновение» градиентов по мере обратного распространения ошибки во времени, что затрудняет обучение.

![Рис. 8.7. Перевод длинного предложения на французском языке на английский с помощью модели типа «кодировщик-декодировщик». Итоговое представление φ оказывается неспособно захватить «далекие» зависимости во входных данных, что затрудняет обучение](../images/chapter_8/fig_8_7.png)

Рис. 8.7. Перевод длинного предложения на французском языке на английский с помощью модели типа «кодировщик-декодировщик». Итоговое представление φ оказывается неспособно захватить «далекие» зависимости во входных данных, что затрудняет обучение

Такой процесс кодирования и последующего декодирования может показаться странным двуязычным/многоязычным читателям, которым приходилось когда-либо заниматься переводами. Живым людям не свойственно докапываться до смысла предложения, генерируя перевод на основе этого смысла. Например, на рис. 8.7, видя французское слово pour, мы понимаем, что в английском переводе будет for, аналогично при виде petit-déjeuner у нас в уме всплывает слово breakfast и т. д. Другими словами, при генерации выходных данных наш мозг концентрируется на соответствующих частях входных. Этот феномен носит название внимания (attention). Внимание широко изучалось в нейронауках и смежных дисциплинах, именно оно обеспечивает наши успехи, несмотря на ограниченность нашей памяти. Внимание встречается повсеместно. На самом деле это происходит прямо сейчас с вами, дорогой читатель. Вы. Уделяете. Внимание. Каждому. Слову. Которое. Сейчас. Читаете. Даже если у вас феноменальная память, вряд ли вы читаете эту книгу строго последовательно. Читая слово, вы обращаете внимание на соседние слова, а возможно, и на название раздела и главы и т. д.

¹ См., например, статьи Бенжио и др. (Bengio et al., 1994) или Ли и Зёйдема (Le, Zuidema, 2016).