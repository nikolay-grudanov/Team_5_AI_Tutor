---
source_image: page_202.png
page_number: 202
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 24.40
tokens: 7416
characters: 1608
timestamp: 2025-12-24T02:26:31.082946
finish_reason: stop
---

9 Классические методы и перспективные направления

В этой главе мы взглянем на предыдущие главы в целом и покажем, как связаны различные, вроде бы не зависящие друг от друга темы и как исследователи могут сочетать и комбинировать описанные идеи для решения имеющихся задач. Мы также вкратце резюмируем кое-какие классические вопросы NLP, для подробного обсуждения которых не хватило места в этой книге. Наконец мы укажем на перспективные направления в данной сфере, по состоянию на 2018 год.

Какие темы мы уже изучили

Мы начали с парадигмы обучения с учителем и использования абстракции графа вычислений для кодирования сложных идей в виде модели, обучаемой путем обратного распространения ошибок (backpropagation). В качестве вычислительной платформы выбрали фреймворк PyTorch. В главе 2 мы познакомили вас с основными понятиями NLP и лингвистики, заложив фундамент для остальной книги. Для последующих глав вам также пригодились такие базовые понятия из главы 3, как функции активации, функции потерь, градиентная оптимизация для обучения с учителем и цикл обучения-оценки. Мы разобрали два примера упреждающих нейронных сетей — многослойный перцептрон и сверточную нейронную сеть. Научились использовать механизмы регуляризации, такие как L1/L2-normalизация и дропаут для повышения ошибкоустойчивости сетей. Многослойные перцептроны могут захватывать в скрытых слоях n-граммподобные связи, но делают это не слишком эффективно. Сверточные нейронные сети, с другой стороны, усваивают подобные субструктуры достаточно эффективно с вычислительной точки зрения благодаря совместному использованию параметров.