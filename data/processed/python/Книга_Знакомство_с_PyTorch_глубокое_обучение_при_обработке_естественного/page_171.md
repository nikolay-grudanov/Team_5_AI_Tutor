---
source_image: page_171.png
page_number: 171
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 36.55
tokens: 7599
characters: 2252
timestamp: 2025-12-24T02:25:47.816501
finish_reason: stop
---

Полезные советы по обучению моделей последовательностей

По возможности применяйте шлюзовые варианты архитектуры. Шлюзовые архитектуры упрощают обучение, решая многие проблемы с численной устойчивостью, присущие нешлюзовым вариантам.

По возможности используйте GRU вместо LSTM. Эффективность работы GRU близка к LSTM при намного меньшем числе параметров и требуемых ресурсов. К счастью, с точки зрения фреймворка PyTorch, для перехода с LSTM на GRU требуется просто другой класс Module.

Используйте Adam в качестве оптимизатора. В главах 6–8 в качестве оптимизатора мы применяем только Adam, и не без причин: он надежен и обычно сходится быстрее, чем альтернативные оптимизаторы. В наибольшей степени это справедливо для моделей последовательностей. Если по какой-либо причине ваша модель не сходится с помощью Adam, попробуйте воспользоваться стохастическим градиентным спуском.

Обрезка градиентов. Если вы заметили многочисленные числовые ошибки при реализации описанных в этой главе приемов, добавьте в свой код инструменты для вывода графиков значений градиентов в процессе обучения. Выяснив диапазон этих значений, обрежьте все аномальные значения. Это сделает обучение более плавным. Во фреймворке PyTorch предусмотрена удобная утилита для этой цели — clip_grad_norm() (пример 7.12). Мы рекомендуем вам выработать у себя привычку выполнять обрезку градиентов.

Пример 7.12. Применение обрезки градиентов в PyTorch

# описание модели последовательности
model = ..
# описание функции потерь
loss_function = ..

# цикл обучения
for _ in ...:
    ...
    model.zero_grad()
    output, hidden = model(data, hidden)
    loss = loss_function(output, targets)
    loss.backward()
    torch.nn.utils.clip_grad_norm(model.parameters(), 0.25)
    ...

Ранняя остановка. В случае моделей последовательностей нередко происходит переобучение. Мы рекомендуем вам останавливать процедуру обучения пораньше, как только начинает расти погрешность оценки, определенная на проверочном наборе данных.

В главе 8 мы продолжим обсуждение моделей последовательностей и обсудим предсказание и генерацию последовательностей с длинами, отличающимися от длин входных последовательностей, с применением моделей «последовательность в последовательность» и других вариантов.