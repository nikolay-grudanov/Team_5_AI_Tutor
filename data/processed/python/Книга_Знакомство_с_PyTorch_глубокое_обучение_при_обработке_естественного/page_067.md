---
source_image: page_067.png
page_number: 67
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 39.20
tokens: 7690
characters: 2447
timestamp: 2025-12-24T02:23:06.796474
finish_reason: stop
---

Поиск правильных значений гиперпараметров

Мы уже выяснили, что параметры (веса) принимают вещественные значения, подстраиваемые оптимизатором с учетом фиксированного поднабора обучающей последовательности, называемого мини-пакетом. Гиперпараметром (hyperparameter) называется любая настройка, влияющая на несколько параметров и принимаемые ими значения. Существует множество различных вариантов выбора, определяющих то, как будет обучаться модель. В их числе выбор функции потерь, оптимизатора, скорости (-ей) обучения для оптимизатора, размеров слоев (обсудим это в главе 4), терпения для ранней остановки и различных решений, связанных с регуляризацией (также описываются в главе 4). Важно отдавать себе отчет в том, что эти решения могут существенно влиять на сходимость модели и ее эффективность, так что желательно систематически исследовать все возможные альтернативы.

Регуляризация

Одно из важнейших понятий глубокого обучения (и машинного обучения вообще) — регуляризация (regularization). Истоки понятия регуляризации лежат в теории численной оптимизации. Как вы помните, большинство алгоритмов машинного обучения оптимизируют функцию потерь для поиска наиболее вероятных значений параметров (или «модели»), объясняющих наблюдаемые значения, то есть минимизирующих потери. Для большинства наборов данных и задач существует несколько решений (возможных моделей) задачи оптимизации. Так какое из них нам (или оптимизатору) выбрать? Рассмотрим рис. 3.3, иллюстрирующий задачу подбора кривой для множества точек.

Обе кривые подходят для указанных точек, но какая из них менее вероятна? Обращаясь к бритве Оккама, мы интуитивно понимаем, что простое объяснение лучше сложного. Подобное ограничение гладкости в машинном обучении называется L2-регуляризацией. Управлять ею в PyTorch можно с помощью параметра weight_decay оптимизатора. Чем больше значение параметра weight_decay, тем вероятнее выбор оптимизатором более гладкого (smoother) обоснования (то есть тем сильнее L2-регуляризация).

Кроме L2-регуляризации, часто используется и L1-регуляризация, обычно для того, чтобы подтолкнуть алгоритм к более разреженным решениям, то есть таким, в которых большинство параметров модели близки к нулю. В главе 4 мы рассмотрим еще один метод структурной регуляризации — dropout (dropout). Тема регуляризации моделей сейчас активно исследуется, а PyTorch представляет собой гибкий фреймворк для реализации пользовательских методов регуляризации.