---
source_image: page_186.png
page_number: 186
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 31.26
tokens: 7573
characters: 2309
timestamp: 2025-12-24T02:26:06.631654
finish_reason: stop
---

Начнем с общего описания набора данных и вспомогательных структур данных, необходимых для нейронного машинного перевода. Набор данных представляет собой корпус параллельных текстов, состоящий из пар английских предложений и соответствующих французских переводов. Поскольку речь идет о парах предложений, длины которых могут различаться, нужно отслеживать максимальные длины и словари как входных, так и выходных предложений. По большей части пример представляет собой естественное расширение того, что наши внимательные читатели уже видели в предыдущих главах.

Описав набор данных и вспомогательные структуры данных, мы рассмотрим модель и генерацию ее целевой последовательности, обращая внимание на различные места исходной последовательности. Кодировщик в нашей модели вычисляет векторы для каждого из мест исходной последовательности на основе информации от всех частей последовательности, используя двунаправленный шлюзовый рекуррентный блок (bi-GRU). Для этого воспользуемся структурой данных PackedSequence фреймворка PyTorch. К выходным данным bi-GRU применяется механизм внимания (см. раздел «Захватываем больше информации из последовательности: внимание» на с. 209), контекстно обусловливающий генерацию целевой последовательности. Мы обсудим результаты работы модели и возможности их улучшения в подразделе «Процедура обучения и результаты» на с. 232.

Набор данных для машинного перевода

Для этого примера мы воспользуемся набором данных англо-французских пар предложений из проекта Tatoeba1. Предварительная обработка данных начинается с преобразования всех предложений в нижний регистр и применения к каждой паре предложений токенизаторов для английского и французского языков из пакета NLTK. Далее мы создадим списки токенов с помощью токенизаторов слов для соответствующих языков. И хотя далее мы выполняем дополнительные действия, этот список токенов представляет собой предварительно обработанный набор данных.

Мы не только выполним описанную выше стандартную предварительную обработку, но и воспользуемся приведенным ниже списком синтаксических паттернов для выбора поднабора данных, чтобы упростить задачу обучения. По существу, мы сужаем область определения данных до ограниченного диапазона синтаксических паттернов. В свою очередь, это значит, что модель при обучении будет наблюдать