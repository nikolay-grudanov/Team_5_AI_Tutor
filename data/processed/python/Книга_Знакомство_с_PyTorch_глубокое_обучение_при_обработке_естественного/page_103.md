---
source_image: page_103.png
page_number: 103
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 35.88
tokens: 7632
characters: 2519
timestamp: 2025-12-24T02:24:01.893201
finish_reason: stop
---

получается выполнять какую-либо задачу, будучи мертвецки пьяным, то в трезвом состоянии вы тем более сможете ее выполнить. Этому наблюдению обязаны собой множество достижений современной науки, так что зарождается целое движение против использования дропаута в нейронных сетях».

Нейронные сети — особенно глубокие сети с большим количеством слоев — способны создавать интересные коадаптации элементов. «Коадаптация» — термин из нейронаук, означающий просто ситуацию, при которой связь между двумя элементами становится чрезвычайно сильной за счет ослабления связей между другими элементами. Обычно это приводит к переобучению модели на текущих данных. Вероятностное отбрасывание связей между элементами позволяет гарантировать, что никакой конкретный элемент не окажется в постоянной зависимости от другого конкретного элемента, благодаря чему модели становятся ошибкоустойчивыми. Дропаут не требует добавления в модель дополнительных параметров, а лишь одного гиперпараметра — «вероятности отброса»1. Это, как легко догадаться, вероятность, при которой отбрасываются связи между элементами. Обычно ее задают равной 0.5. Пример 4.13 демонстрирует реализацию MLP с дропаутом.

Пример 4.13. MLP с дропаутом

import torch.nn as nn
import torch.nn.functional as F

class MultilayerPerceptron(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        """
        Аргументы:
            input_dim (int): размер входных векторов
            hidden_dim (int): размер на выходе первого линейного слоя
            output_dim (int): размер на выходе второго линейного слоя
        """
        super(MultilayerPerceptron, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, output_dim)

    def forward(self, x_in, apply_softmax=False):
        """
        Прямой проход MLP

        Аргументы:
            x_in (torch.Tensor): входной тензор данных
                Значение x_in.shape должно быть (batch, input_dim)
            apply_softmax (bool): флаг для многомерной логистической функции активации. При использовании функции потерь на основе перекрестной энтропии должен равняться false
        Возвращает:
        """
        x = F.relu(self.fc1(x_in))
        x = self.fc2(x)
        if apply_softmax:
            x = F.softmax(x, dim=1)
        return x

1 Некоторые библиотеки глубокого обучения по непонятной причине называют (и интерпретируют) эту вероятность «вероятностью сохранения», то есть придают ей в точности противоположный смысл.