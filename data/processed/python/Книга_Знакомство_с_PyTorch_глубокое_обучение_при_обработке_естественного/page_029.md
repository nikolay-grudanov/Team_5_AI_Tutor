---
source_image: page_029.png
page_number: 29
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 36.03
tokens: 7605
characters: 2170
timestamp: 2025-12-24T02:22:03.085962
finish_reason: stop
---

Графы вычислений

Рисунок 1.1 характеризует парадигму обучения с учителем как архитектуру движения данных. Она состоит из модели (математического выражения), преобразующей входные данные для получения предсказаний, и функции потерь (еще одного выражения), которое генерирует сигнал обратной связи, предназначенный для подстройки параметров модели. Для реализации подобного движения данных удобно использовать такую структуру, как граф вычислений¹. Формально граф вычислений — это абстракция для моделирования математических выражений. В контексте глубокого обучения реализации графов вычислений, такие как Theano, TensorFlow и PyTorch, обеспечивают также автоматическое дифференцирование, необходимое для получения градиентов параметров при обучении в рамках парадигмы машинного обучения с учителем. Мы обсудим это подробнее в следующем разделе — «Основы PyTorch».

Вывод (inference), или предсказание, представляет собой просто вычисление выражения (движение вперед по графу вычислений). Посмотрим, как граф вычислений моделирует выражения. Рассмотрим выражение:

\[
y = wx + b.
\]

Его можно разложить на два подвыражения: \( z = wx \) и \( y = z + b \). После этого исходное выражение можно представить в виде ориентированного ациклического графа (directed acyclic graph, DAG), в котором вершинами являются математические операции, например умножение и сложение. Входные данные для операций — входящие в вершины ребра, а результаты операций — исходящие ребра. Граф вычислений для выражения \( y = wx + b \) приведен на рис. 1.6. В следующем разделе мы увидим, как с помощью PyTorch можно без труда создавать графы вычислений и вычислять градиенты без каких-либо дополнительных вспомогательных действий.

![Диаграмма графа вычислений для выражения y = wx + b](https://i.imgur.com/3Q5z5QG.png)

Рис. 1.6. Представление выражения y = wx + b с помощью графа вычислений

¹ Сеппо Линнайнмаа (Seppo Linnainmaa) (http://bit.ly/2Rnmdao) впервые упомянул автоматическое дифференцирование на графах вычислений в своей магистерской дипломной работе в 1970 году! Различные варианты этой идеи легли в основу современных фреймворков глубокого обучения: Theano, TensorFlow и PyTorch.