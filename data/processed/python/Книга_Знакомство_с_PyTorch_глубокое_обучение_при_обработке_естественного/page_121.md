---
source_image: page_121.png
page_number: 121
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 33.21
tokens: 7655
characters: 2526
timestamp: 2025-12-24T02:24:27.371134
finish_reason: stop
---

представляет собой строку, соответствующую ненулевому элементу унитарного вектора. С учетом этого наблюдения можно пропустить шаг умножения и воспользоваться целочисленным значением в качестве индекса для извлечения нужной строки.

Еще одно, последнее примечание относительно эффективности вложений: несмотря на пример с рис. 5.1, где размерность матрицы весов совпадает с размерностью входного унитарного вектора, так бывает далеко не всегда. На самом деле вложения часто применяются для представления слов из пространства меньшей размерности, чем понадобилось бы при использовании унитарного вектора или представления количества вхождений. Типичный размер вложений в научных статьях — от 25 до 500 измерений, причем выбор конкретного значения сводится к объему доступной памяти GPU.

Подходы к обучению вложениям слов

Задача этой главы — не научить вас конкретным методикам вложений слов, а помочь разобраться, что такое вложения, как и где их можно применять, как лучше использовать их в моделях, а также каковы их ограничения. Дело в том, что на практике редко приходится писать новые алгоритмы обучения вложениям слов. Однако в данном подразделе мы сделаем краткий обзор современных подходов к такому обучению. Обучение во всех методах вложения слов производится с помощью одних только слов (то есть немаркированных данных), однако с учителем. Это становится возможно благодаря созданию вспомогательных задач обучения с учителем, в которых данные маркируются неявно, из тех соображений, что оптимизированное для решения вспомогательной задачи представление должно захватывать множество статистических и лингвистических свойств корпуса текста, чтобы приносить хоть какую-то пользу. Вот несколько примеров подобных вспомогательных задач.

□ Предсказать следующее слово по заданной последовательности слов. Носит также название задачи моделирования языка (language modelling).

□ Предсказать пропущенное слово по словам, расположенным до и после него.

□ Предсказать слова в рамках определенного окна, независимо от позиции, по заданному слову.

Конечно, этот список не полон и выбор вспомогательной задачи зависит от интуиции разработчика алгоритма и вычислительных затрат. В числе примеров GloVe, непрерывное мультимножество слов (Continuous Bag-of-Words, CBOW), Skipgrams и т. д. Подробности можно найти в главе 10 книги Голдберга (Goldberg, 2017), но модель CBOW мы вкратце обсудим здесь. Впрочем, в большинстве случаев вполне достаточно воспользоваться предобученными вложениями слов и подогнать их к имеющейся задаче.