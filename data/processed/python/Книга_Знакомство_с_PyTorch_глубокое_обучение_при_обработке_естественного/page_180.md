---
source_image: page_180.png
page_number: 180
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 27.23
tokens: 7496
characters: 1946
timestamp: 2025-12-24T02:25:57.095941
finish_reason: stop
---

Хотелось бы, чтобы наши модели генерации последовательностей точно так же уделяли внимание различным частям входных данных, а не только общим итогам входных данных в целом. Это называется механизмом внимания (attention mechanism). Кстати, первыми моделями, в которых он использовался для обработки написанных на естественных языках текстов, были как раз модели машинного перевода Богданова (Bahdanau et al., 2015). С тех пор было предложено несколько типов механизмов внимания и несколько подходов к повышению внимания.

В этом разделе мы рассмотрим пару простейших механизмов внимания и приведем соответствующую терминологию. Внимание оказалось чрезвычайно полезным для повышения эффективности систем глубокого обучения со сложными входными и выходными данными. Фактически Богданов и его коллеги показали, что эффективность системы машинного перевода, согласно показателю BLEU (который мы обсудим в следующем разделе), без механизма внимания ухудшается по мере удлинения элементов входных данных (рис. 8.8). Добавление механизма внимания решает эту проблему.

![График изменения показателей BLEU с механизмом внимания](../images/chapter8/fig8_8.png)

Рис. 8.8. График демонстрирует изменения показателей BLEU систем машинного перевода с механизмом внимания (RNNsearch-30, RNNsearch-50) и без него (RNNenc-30, RNNenc-50).
Системы RNN*-30 и RNN*-50 обучались на предложениях длиной до 30 и до 50 слов соответственно. В системах машинного перевода без механизма внимания эффективность системы ухудшается по мере увеличения длины предложений. При наличии механизма внимания показатели перевода более длинных предложений улучшаются, но стабильность эффективности машинного перевода зависит от длины предложений, на которых они обучались (рисунок взят из статьи Богданова и др. [Bahdanau et al., 2015])

Внимание в глубоких нейронных сетях. Внимание — общий механизм, который можно использовать с любыми обсуждавшимися ранее в книге моделями. Но здесь