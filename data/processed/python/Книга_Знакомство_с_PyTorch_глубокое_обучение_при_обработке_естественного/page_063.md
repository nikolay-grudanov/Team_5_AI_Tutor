---
source_image: page_063.png
page_number: 63
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 33.90
tokens: 7617
characters: 2286
timestamp: 2025-12-24T02:22:53.962914
finish_reason: stop
---

Пример 3.10. Оптимизатор Adam

Input[0]
import torch.nn as nn
import torch.optim as optim

input_dim = 2
lr = 0.001

perceptron = Perceptron(input_dim=input_dim)
bce_loss = nn.BCELoss()
optimizer = optim.Adam(params=perceptron.parameters(), lr=lr)

Собираем все вместе: градиентное машинное обучение с учителем

Обучение начинается с вычисления функции потерь, то есть отклонения предсказаний модели от целевых переменных. Градиент функции потерь в свою очередь сигнализирует, «насколько» необходимо поменять параметры. Градиент каждого параметра отражает мгновенную скорость изменений значения потерь при заданном значении параметра. Фактически это значит, что нам известна величина вклада каждого из параметров в функцию потерь. Образно можно представить, что каждый параметр стоит на своем холме и хочет сделать шаг вверх или вниз. В минимальном варианте градиентное обучение модели состоит в итеративном обновлении каждого из параметров с учетом градиента функции потерь относительно этого параметра.

Посмотрим, как этот пошаговый градиентный метод работает. Во-первых, вся вспомогательная информация, хранящаяся внутри объекта (perceptron) модели, очищается с помощью функции zero_grad(). Далее модель вычисляет выходные значения (y_pred) по указанным входным данным (x_data). Затем вычисляется функция потерь путем сравнения выходных значений (y_pred) с намеченными целевыми переменными (y_target). Это непосредственно «учительская часть» сигнала обучения с учителем. В объекте потерь (criterion) PyTorch есть функция backward(), итеративно транслирующая потери обратно по графу вычислений и сообщающая всем параметрам их градиенты. Наконец, оптимизатор (opt) обновляет значения параметров по известным градиентам с помощью функции step().

Весь обучающий набор данных разбивается на пакеты (batches¹). Все шаги градиентного этапа выполняются над пакетами данных. Размер пакетов задается гиперпараметром batch_size. А поскольку общий размер обучающего набора данных фиксирован, то увеличение размера пакетов ведет к уменьшению их количества.

¹ В литературе также часто можно встретить прямую кальку с английского «батч». Например, мини-пакетный стохастический градиентный спуск, упоминавшийся в главе 1, называют стохастическим градиентным спуском по мини-батчам. — Примеч. пер.