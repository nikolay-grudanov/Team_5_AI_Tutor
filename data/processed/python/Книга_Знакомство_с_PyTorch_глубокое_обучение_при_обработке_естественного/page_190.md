---
source_image: page_190.png
page_number: 190
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 38.47
tokens: 7809
characters: 2811
timestamp: 2025-12-24T02:26:21.360464
finish_reason: stop
---

В целом, кодировщик принимает на входе последовательность целочисленных значений и создает по вектору признаков для каждой позиции. На выходе кодировщик выдает эти векторы и завершающее скрытое состояние bi-GRU, использовавшееся для создания векторов признаков. Это скрытое состояние применяется для инициализации скрытого состояния декодировщика в следующем подразделе.

Если углубиться в описание работы кодировщика, то сначала выполняется вложение входной последовательности с помощью слоя вложений. Обычно, чтобы модель могла работать с последовательностями переменной длины, достаточно установить флаг padding_idx для слоя вложений, поскольку любой равной padding_idx позиции присваивается нулевой вектор, который не обновляется в процессе оптимизации. Напомним, что это называется маской. Впрочем, в такой модели типа «кодировщик-декодировщик» маскированные позиции необходимо обрабатывать по-другому, поскольку для кодирования исходной последовательности мы применяем bi-GRU. Основная причина состоит в том, что маскированные позиции могут влиять на обратную компоненту архитектуры bi-GRU пропорционально числу встреченных им маскированных позиций до начала обработки последовательности¹.

Для обработки маскированных позиций в последовательностях переменной длины в bi-GRU мы используем структуру данных PackedSequence фреймворка PyTorch. Вид PackedSequence определяет способ пакетной обработки последовательностей переменной длины в CUDA. Любую дополненную нулями последовательность, например, вложенную исходную последовательность из представленного в примере 8.6 кодировщика можно преобразовать в PackedSequence при соблюдении двух условий: указания длин всех последовательностей и сортировки мини-пакета в соответствии с этими длинами. Это наглядно показано на рис. 8.11, а поскольку это непростая задача, мы проиллюстрируем ее в примере 8.6 и результатах его выполнения².

Пример 8.6. Простая демонстрация функций pack_padded_sequence() и pad_packed_sequence()

<table>
  <tr>
    <th>Input[0]</th>
    <td>
      abcd_padded = torch.tensor([1, 2, 3, 4], dtype=torch.float32)<br>
      efg_padded = torch.tensor([5, 6, 7, 0], dtype=torch.float32)<br>
      h_padded = torch.tensor([8, 0, 0, 0], dtype=torch.float32)<br><br>
      padded_tensor = torch.stack([abcd_padded, efg_padded, h_padded])<br><br>
      describe(padded_tensor)
    </td>
  </tr>
</table>

¹ Чтобы убедиться в этом, попробуйте представить в уме или изобразить схематически ход вычислений. В качестве подсказки: рассмотрите очередной шаг — входные данные и последнее скрытое состояние умножаются на веса и складываются с учетом смещения. Какое влияние смещение окажет на выходные данные, если они состояли исключительно из 0?
² Для этого мы воспользуемся функцией describe(), описанной в подразделе «Создание тензоров» на с. 31.