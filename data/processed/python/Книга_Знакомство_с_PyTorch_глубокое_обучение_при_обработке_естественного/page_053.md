---
source_image: page_053.png
page_number: 53
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 28.37
tokens: 7474
characters: 1735
timestamp: 2025-12-24T02:22:33.295892
finish_reason: stop
---

Из этого выражения очевидно, что сигма-функция — гладкая, дифференцируемая функция. Пакет torch реализует сигма-функцию в виде torch.sigmoid(), как показано в примере 3.2.

Пример 3.2. Сигма-функция активации¹

import torch
import matplotlib.pyplot as plt

x = torch.range(-5., 5., 0.1)
y = torch.sigmoid(x)
plt.plot(x.numpy(), y.numpy())
plt.show()

Как вы видите из графика, сигма-функция очень быстро насыщается (то есть выдает экстремальные значения на выходе) для большинства входных значений. Это может стать проблемой, поскольку градиент становится равен нулю или расходится до слишком большого значения с плавающей точкой. Эти феномены известны под названиями проблема «исчезающего» градиента (vanishing gradient problem) и проблема «взрывного» роста градиента (exploding gradient problem) соответственно. В результате сигма-блоки обычно можно увидеть в нейронных сетях только на выходе, где такое размазывание позволяет интерпретировать выходные данные как вероятности.

Гиперболический тангенс

Функция активации th² — лишь незначительно отличающийся вариант сигма-функции. Это очевидно из выражения для th:

\[
f(x) = \text{th}\ x = \frac{e^x - e^{-x}}{e^x + e^{-x}}.
\]

После небольших преобразований (которые мы оставляем читателю в качестве упражнения) можно убедиться, что th — просто линейное преобразование сигма-функции, как показано в примере 3.3. Это будет очевидно, если написать код PyTorch для вызова функции гиперболического тангенса tanh() и построить ее график. Обратите внимание, что гиперболический тангенс, как

¹ Напомним, что для отображения графиков в блокноте iPython необходимо сначала выполнить команду %matplotlib inline. — Примеч. пер.
² В англоязычной литературе обозначается tahn. — Примеч. пер.