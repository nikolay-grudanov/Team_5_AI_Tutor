---
source_image: page_144.png
page_number: 144
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 41.95
tokens: 7755
characters: 2833
timestamp: 2025-12-24T02:25:14.225010
finish_reason: stop
---

Применение одних и тех же весов на всех временных шагах для преобразования входных данных в выходные — еще один пример совместного использования параметров. В главе 4 мы видели совместное использование параметров сверточными нейронными сетями в смысле пространства. CNN задействуют параметры, называемые ядрами, с целью вычисления выходных данных для различных подобластей входных данных. Сверточные ядра смещаются по входным данным, вычисляя выходные данные для всех возможных мест, чтобы усвоить трансляционную инвариантность. Напротив, RNN используют одни и те же параметры для вычисления выходных данных на всех временных шагах, захватывая состояние последовательности с помощью скрытого вектора состояния. Таким образом, задача RNN состоит в усвоении инвариантности благодаря возможности вычисления любых нужных выходных данных по заданным скрытому вектору состояния и входному вектору. Можно считать, что RNN совместно используют параметры во времени, а CNN — параметры в пространстве.

Поскольку длины слов и предложений могут различаться, RNN или любая другая модель последовательности должна уметь обрабатывать последовательности переменной длины (variable-length sequences). Один из методов достижения этого — искусственное ограничение длины последовательностей неким фиксированным значением. В этой книге мы применим для обработки последовательностей переменной длины другой метод — маскирование (masking), воспользовавшись знаниями длин последовательностей. Если вкратце, при маскировании данные могут сигнализировать, что определенные входные векторы не должны учитываться в градиенте или итоговых выходных данных. Фреймворк PyTorch предоставляет примитивы для работы с последовательностями переменной длины в классе PackedSequence — он служит для создания плотных тензоров из более разреженных. Пример этого показан в разделе «Пример: классификация национальной принадлежности фамилий с помощью символьного RNN» на с. 177¹.

Реализация RNN Элмана

Чтобы разобраться в нюансах RNN, пройдемся по простой реализации RNN Элмана. Фреймворк PyTorch предоставляет множество удобных классов и вспомогательных функций для построения RNN. Класс RNN фреймворка PyTorch реализует RNN Элмана. Вместо того чтобы применять непосредственно этот класс, мы воспользуемся RNNCell — абстракцией для отдельного временного шага RNN и сформируем RNN на ее основе. Благодаря этому мы сможем явным образом показать вам все выполняемые RNN вычисления. Приведенный в примере 6.1 класс

¹ В книгах и статьях, посвященных глубокому обучению, часто лишь вскользь затрагивают «подробности реализации» маскирования и упакованных последовательностей. Хотя для общего понимания RNN это не столь важно, углубленное знакомство с данными понятиями, которое вы получите в этой главе, для специалиста-практика жизненно важно. Обратите на это особое внимание!