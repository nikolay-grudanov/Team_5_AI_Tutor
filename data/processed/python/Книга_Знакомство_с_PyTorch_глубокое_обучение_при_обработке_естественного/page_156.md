---
source_image: page_156.png
page_number: 156
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 38.38
tokens: 7771
characters: 2663
timestamp: 2025-12-24T02:25:29.342983
finish_reason: stop
---

Первая проблема с RNN Элмана — трудности хранения долгосрочной информации. Например, в случае RNN из главы 6 на каждом временному шаге мы просто обновляли вектор скрытого состояния независимо от того, имело ли это смысл. В результате RNN никак не контролирует, какие значения сохраняются в скрытом состоянии, а какие отбрасываются — это полностью определяется входными данными. На интуитивном уровне это совершенно бессмысленно. Хотелось бы, чтобы RNN умела определять, требуется ли обновление, и если да, то насколько именно нужно обновить и какие именно части вектора состояния.

Вторая проблема с RNN Элмана — их склонность к выходу градиентов из-под контроля и стремлению в сторону нуля или бесконечности. Такие нестабильные градиенты называются соответственно исчезающими или растущими взрывным образом, в зависимости от направления уменьшения/роста их абсолютных значений. Действительно большое абсолютное значение градиента или действительно маленькое (меньше 1) может привести к нестабильности процедуры оптимизации (см. статьи Хохрайтера и др. [Hochreiter et al., 2001]; Пашкану и др. [Pascanu et al., 2013]).

Существуют методы решения этих проблем с градиентами в «наивных» RNN, например с помощью выпрямленных линейных блоков (rectified linear units, ReLU), отсечения градиентов (gradient clipping) или просто продуманной инициализации. Но ни один из этих методов не работает столь надежно, как методика под названием «шлюзование» (gating).

Шлюзование как решение проблем «наивных» RNN. Чтобы понять на интуитивном уровне, что такое шлюзование, представьте, что вам нужно сложить две величины, \(a\) и \(b\), но при этом хотелось бы контролировать, какая часть величины \(b\) будет включена в сумму. Математически для этого сумму \(a + b\) можно переписать в таком виде:

\[
a + \lambda b,
\]

где \(\lambda\) представляет собой значение от 0 до 1. Если \(\lambda = 0\), то \(b\) не вносит никакого вклада в сумму, а если \(\lambda = 1\), то \(b\) включается в сумму полностью. Таким образом, \(\lambda\) можно рассматривать как своеобразный переключатель или шлюз, определяющий включаемую в сумму долю \(b\). Именно так работает механизм шлюзования. Вернемся теперь к RNN Элмана и посмотрим, как встроить шлюзование в «наивную» RNN, чтобы иметь возможность выполнять условные обновления. Если предыдущее скрытое состояние обозначить \(h_{t-1}\), а текущие входные данные — \(x_t\), то очередное обновление в RNN Элмана будет выглядеть следующим образом:

\[
h_t = h_{t-1} + F(h_{t-1}, x_t),
\]

где \(F\) — очередное вычисление RNN. Конечно, это безусловное суммирование со всеми присущими ему недостатками, описанными выше. Теперь представьте себе,