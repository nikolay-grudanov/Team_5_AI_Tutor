---
source_image: page_207.png
page_number: 207
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 31.09
tokens: 7772
characters: 2958
timestamp: 2025-12-24T02:26:42.943725
finish_reason: stop
---

обучение существует всего несколько лет. Множество новых статей посвящено изучению традиционных методик и задач с точки зрения новой парадигмы глубокого (дифференцируемого) обучения. При чтении классических статей (а мы рекомендуем вам их читать!) по NLP имеет смысл задаться вопросом: чего авторы статьи хотят достичь? Каковы входные/выходные представления? Как можно упростить решение задачи, применяя обсуждавшиеся в предыдущих главах методики?

Композиционность моделей. В этой книге мы обсуждали различные виды архитектур глубокого обучения для NLP: MLP, CNN, модели последовательностей, модели преобразования последовательностей в последовательности и модели на основе механизма внимания. Важно отметить, что хотя мы изучали все эти модели по отдельности, но делали это исключительно в педагогических целях. Одна из прослеживающихся сейчас в литературе тенденций — композиция различных архитектур для решения имеющейся задачи. Например, можно написать сверточную нейронную сеть, работающую с символами слов, с последующей LSTM поверх этого представления, а завершающую классификацию кодировки LSTM выполнить с помощью MLP. Возможность сочетания различных архитектур в зависимости от того, что нужно для решения конкретной задачи, — одна из самых замечательных идей глубокого обучения.

Использование операций свертки для последовательностей. Одна из недавних тенденций в моделировании последовательностей — моделирование последовательности целиком с помощью операций свертки. В качестве примера полностью сверточной модели машинного перевода см. статью Геринга и др. (Gehring et al., 2018). Шаг декодирования включает операцию обращения свертки. Преимущество такого решения состоит в значительном ускорении обучения с помощью полностью сверточной модели.

Вам нужно лишь внимание. Еще одна тенденция — замена операций свертки механизмом внимания (см. статью Васвани и др. [Vaswani et al., 2017]). При использовании механизма внимания, особенно таких его вариантов, как аутовнимание и мультивнимание, можно успешно захватывать «далекие» зависимости, обычно моделируемые с помощью RNN или CNN.

Перенос обучения. Перенос обучения представляет собой обучение представлениям для одной задачи и использование этих представлений для повышения эффективности обучения в другой задаче. Благодаря новой волне популярности нейронных сетей и глубокого обучения в NLP методики переноса обучения с помощью предобученных векторов слов стали применяться повсеместно. Недавние работы Рэдфорда, Петерса и др.(Radford et al., 2018; Peters et al., 2018) демонстрируют полезность представлений без учителя, усвоенных для задач моделирования языков, в различных задачах NLP. Сюда можно отнести составление ответов на вопросы, классификацию, определение сходства предложений и формирование рассуждений на естественном языке.

Кроме того, обучение с подкреплением (reinforcement learning) в последнее время успешно применяется в задачах, связанных с диалогами, а моделирование