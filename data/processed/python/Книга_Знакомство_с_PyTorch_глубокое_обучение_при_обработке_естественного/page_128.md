---
source_image: page_128.png
page_number: 128
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 34.30
tokens: 7672
characters: 2385
timestamp: 2025-12-24T02:24:38.458538
finish_reason: stop
---

Пример 5.6. «Защитные» в векторы слов культурные предубеждения

<table>
  <tr>
    <th>Input[0]</th>
    <td>embeddings.compute_and_print_analogy('man', 'doctor', 'woman')</td>
  </tr>
  <tr>
    <th>Output[0]</th>
    <td>man : doctor :: woman : nurse</td>
  </tr>
</table>

Не следует забывать о возможных систематических ошибках во вложениях с учетом роста их популярности и распространенности в NLP-приложениях. Искоренение систематических ошибок во вложениях слов — новая и очень интересная сфера научных исследований (см. статью Болукбаси и др. [Bolukbasi et al., 2016]). Рекомендуем заглянуть на сайт ethicsinnlp.org, где можно найти актуальную информацию по вопросам интерсекциональности этики и NLP.

Пример: обучение вложениям модели непрерывного мультимножества слов

В этом примере мы рассмотрим одну из самых известных моделей, предназначенных для конструирования универсальных вложений слов и обучения им — модель Word2Vec непрерывного мультимножества слов¹. В этом разделе, говоря о задаче CBOW или задаче классификации CBOW, мы подразумеваем, что конструируем задачу классификации с целью обучения вложениям CBOW. Модель CBOW представляет собой задачу многоклассовой классификации, включающую просмотр текстов, создание контекстных окон слов, исключение из этих окон центральных слов и классификацию контекстных окон по этим отсутствующим словам. Ее можно рассматривать как задачу заполнения пропусков: дано предложение с пропущенным словом и задача модели — выяснить, какое слово там должно стоять.

Цель данного примера — познакомить вас со слоем вложений nn.Embedding — модулем фреймворка PyTorch, инкапсулирующим матрицу вложения. С помощью слоя вложений можно отобразить целочисленный идентификатор токена в вектор, пригодный для применения в вычислениях нейронной сети. При обновлении оптимизатором весов модели с целью минимизации потерь обновляются также значения этого вектора. Во время этого процесса модель обучается создавать вложения слов оптимальным для конкретной задачи образом.

В оставшейся части данного примера мы будем следовать нашему стандартному формату примеров. В первом разделе познакомим вас с набором данных — оцифрованной версией романа Мэри Шелли «Франкенштейн». Далее мы обсудим

¹ См. блокнот /chapters/chapter_5/5_2_CBOW/5_2_Continuous_Bag_of_Words_CBOW.ipynb в репозитории GitHub этой книги (https://nlproc.info/PyTorchNLPBook/repo/).