---
source_image: page_141.png
page_number: 141
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 31.95
tokens: 7643
characters: 2558
timestamp: 2025-12-24T02:24:59.373378
finish_reason: stop
---

Короче говоря, для понимания естественного языка необходимо понимать, как устроены предложения. В предыдущих главах мы познакомились с упреждающими нейронными сетями, такими как многослойный перцептрон и сверточные нейронные сети, а также с возможностями векторных представлений. Хотя с помощью этих методик можно решить множество различных задач обработки текстов на естественных языках, нельзя утверждать, что они правильно моделируют последовательности¹.

Вполне применимы и традиционные подходы к моделированию последовательностей в NLP, включая скрытые марковские модели, условные случайные поля и другие графические модели, хотя в этой книге мы их обсуждать не станем².

В глубоком обучении моделирование последовательностей требует хранения скрытой информации о состоянии (скрытого состояния). По мере обхода элементов последовательности, например просмотра моделью слов в предложении, это скрытое состояние обновляется. Таким образом, скрытое состояние (обычно это вектор) инкапсулирует все просмотренные последовательностью на текущий момент данные³. Этим вектором скрытого состояния, называемым также представлением последовательности (sequence representation), можно затем воспользоваться во множестве задач моделирования последовательностей самыми разнообразными способами, в зависимости от решаемой задачи, начиная от классификации последовательностей до их предсказания. В этой главе мы изучим классификацию данных, а в главе 7 рассмотрим применение моделей последовательностей для генерации последовательностей.

Начнем с простейшей модели последовательности на основе нейронной сети — рекуррентной нейронной сети (recurrent neural network, RNN). После этого мы приведем комплексный пример применения RNN к классификации. А именно, вам предстоит увидеть пример использования символьной RNN для классификации фамилий по национальной принадлежности. Пример с фамилиями демонстрирует, что модели последовательностей могут улавливать орфографические (на уровне частей слов) языковые паттерны. Он реализован таким образом, чтобы читатель мог применить эту модель к другим задачам, включая моделирование последовательностей текста, в котором элементы данных представляют собой слова, а не символы.

1 За исключением сверточных нейронных сетей. Как мы расскажем в главе 9, с помощью CNN можно эффективно собирать информацию о последовательности.
2 Подробности вы можете найти в книге Коллера и Фридмана (Koller, Friedman, 2009).
3 В главе 7 мы рассмотрим различные варианты моделей последовательностей, умеющие «забывать» старую, ненужную информацию.