---
source_image: page_196.png
page_number: 196
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 36.97
tokens: 7638
characters: 2742
timestamp: 2025-12-24T02:26:34.117518
finish_reason: stop
---

context_vectors = torch.sum(weighted_vectors, dim=1)
return context_vectors, vector_probabilities

def terse_attention(encoder_state_vectors, query_vector):
    """
    encoder_state_vectors: трехмерный вектор из bi-GRU в кодировщике
    query_vector: скрытое состояние
    """
    vector_scores = torch.matmul(encoder_state_vectors,
                                 query_vector.unsqueeze(dim=2)).squeeze()
    vector_probabilities = F.softmax(vector_scores, dim=-1)
    context_vectors = torch.matmul(encoder_state_vectors.transpose(-2, -1),
                                 vector_probabilities.unsqueeze(dim=2)).squeeze()
    return context_vectors, vector_probabilities

Обучение поиску и плановая выборка

В текущем виде наша модель предполагает наличие целевой последовательности, которая используется в качестве входных данных на каждом из временных шагов в декодировщике. Во время тестирования это допущение нарушается, поскольку модель не может «жульничать» и знать последовательность, которую пытается сгенерировать. Для решения этой проблемы предусмотрена методика, с помощью которой модель может использовать во время обучения свои собственные предсказания. В литературе данная методика известна под названием «обучение поиску» (learning to search) и «плановая выборка» (scheduled sampling)¹.

Чтобы уяснить себе сущность этой методики, рассмотрим задачу предсказания как задачу поиска. На каждом из временных шагов модель может выбрать один из множества путей (количество вариантов равно размеру целевого словаря), а данные представляют собой наблюдения правильных путей. Во время тестирования модель наконец-то может «выйти из колеи», поскольку правильный путь для вычисления распределений вероятностей ей более не известен. Таким образом, методика, которая разрешает модели производить выборку собственного пути, предоставляет способ оптимизации модели и получения ею лучших распределений вероятностей при отклонении от целевых последовательностей из набора данных.

Чтобы модель могла выполнять выборку собственных предсказаний во время обучения, необходимо внести в код три основных изменения. Во-первых, сделать начальные индексы более явными в виде индексов токена BEGIN-OF-SEQUENCE. Во-вторых, на каждом временнóм шаге цикла генерации будет выбираться случайная выборка, и если эта случайная выборка меньше вероятности собственных предсказаний модели, во время текущей итерации нужно использовать предсказания модели².

¹ Подробности вы можете найти в статьях Дауме, Лэнгфорда и Марку (Daumé, Langford, Marcu, 2009) и Бенжио и др. (Bengio et al., 2015).
² Если вы знакомы с выборкой по методу Монте-Карло для таких способов оптимизации, как методы Монте-Карло по схеме марковских цепей, то наверняка узнаете этот паттерн.