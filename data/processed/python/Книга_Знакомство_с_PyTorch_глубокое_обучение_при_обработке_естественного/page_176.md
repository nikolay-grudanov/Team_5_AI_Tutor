---
source_image: page_176.png
page_number: 176
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 29.09
tokens: 7445
characters: 1627
timestamp: 2025-12-24T02:25:53.357811
finish_reason: stop
---

Рис. 8.3. Преобразование в эмодзи как S2S-задача предсказания: выравнивание токенов в двух фразах указывает на эквивалентность перевода

Традиционно для решения S2S-задач применялись статистические подходы со сложной архитектурой и эвристическими алгоритмами. Хотя обзор этих подходов выходит за рамки данной главы, мы рекомендуем вам прочитать статью Кёна (Koehn, 2009) и взглянуть на перечисленные на сайте statmt.org ресурсы. Из главы 6 мы узнали, что модель последовательности способна кодировать последовательность произвольной длины в вектор. А в главе 7 видели, что одного вектора достаточно, чтобы рекуррентная нейронная сеть генерировала различные фамилии в зависимости от контекста. Естественным расширением этих концепций являются S2S-модели.

На рис. 8.4 показано, как кодировщик «кодирует» все входные данные в представление φ, обусловливающее выдачу декодировщиком правильных выходных данных. В качестве кодировщика можно использовать любую RNN, будь то RNN Элмана, LSTM или GRU. В следующих двух разделах мы покажем два важнейших компонента современных S2S-моделей. Во-первых, рассмотрим двунаправленную рекуррентную модель, в которой для создания лучших представлений объединяются прямой и обратный проходы по последовательности. Во-вторых, в разделе «Захватываем больше информации из последовательности: внимание» на с. 209 мы представим и обсудим механизм внимания, с помощью которого удобно фокусироваться на различных частях входных данных, в зависимости от задачи. Обе темы очень важны для создания сложных решений на основе S2S-моделей.

Рис. 8.4. S2S-модель для преобразования английского текста в эмодзи