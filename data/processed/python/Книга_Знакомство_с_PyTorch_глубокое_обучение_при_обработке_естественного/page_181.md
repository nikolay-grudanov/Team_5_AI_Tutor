---
source_image: page_181.png
page_number: 181
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 42.74
tokens: 7782
characters: 2973
timestamp: 2025-12-24T02:26:15.522603
finish_reason: stop
---

мы опишем его применительно к моделям типа «кодировщик-декодировщик», поскольку именно в них механизмы внимания проявляют себя особенно ярко. Рассмотрим S2S-модель. Напомним, что в обычной S2S-модели на каждом шаге формируется представление скрытого состояния, обозначаемое \( \phi_w \), свое для каждого конкретного временного шага в кодировщике (это видно из рис. 8.6). Для использования механизма внимания мы учтем не только завершающее скрытое состояние кодировщика, но и скрытые состояния для всех промежуточных шагов. Для этих скрытых состояний кодировщика используется, возможно, не слишком информативное название значения, или ценности (values), а в некоторых случаях — ключи (keys). Внимание также зависит от предыдущего скрытого состояния декодировщика, называемого запросом (query)\(^1\).

На рис. 8.9 все это проиллюстрировано для временного шага 0. Вектор запроса для временного шага \( t = 0 \) — фиксированный гиперпараметр. Внимание представлено вектором, размерность которого соответствует количеству значений, на которые обращается внимание. Он называется вектором внимания (attention vector) или весами внимания (attention weights), а иногда выравниванием (alignment). На основе весов внимания в сочетании с состояниями кодировщика (значениями) генерируется вектор контекста (context vector), иногда называемый glimpse («беглый взгляд»). Такой вектор контекста служит входными данными для декодировщика, поэтому не требуется полная кодировка предложения. С помощью функции совместимости (compatibility function) обновляется вектор внимания для следующего временного шага. Конкретный характер функции совместимости зависит от используемого механизма внимания.

Существует несколько способов реализации внимания. Простейший и чаще всего используемый — механизм внимания с учетом содержимого (content-aware). Увидеть его в действии можно в разделе «Пример: нейронный машинный перевод» на с. 215. Еще один распространенный механизм внимания — внимание с учетом местоположения (location-aware), зависит только от вектора запроса и ключа. Веса внимания обычно представляют собой значения с плавающей точкой от 0 до 1. Такой механизм внимания называется мягким (soft). В случае же бинарного (0/1) вектора внимания оно называется жестким (hard).

Приведенный на рис. 8.9 механизм внимания зависит от состояний кодировщика для всех временных шагов входных данных. Такой механизм называется

1 Начинающих термины «ключи», «значения» («ценностями») и «запрос» могут только запутать, но мы все равно будем ими пользоваться, поскольку они стали общепринятыми. Имеет смысл прочитать данный раздел несколько раз, чтобы полностью уяснить эти понятия. Такая терминология возникла потому, что изначально внимание рассматривалось как задача поиска. Расширенное описание этих понятий и механизма внимания в целом можно найти в статье Лилиан Вэн «Внимание? Внимание!» (Weng L. Attention? Attention!) (https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html).