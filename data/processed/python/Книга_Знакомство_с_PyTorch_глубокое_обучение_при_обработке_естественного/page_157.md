---
source_image: page_157.png
page_number: 157
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 41.03
tokens: 7799
characters: 2629
timestamp: 2025-12-24T02:25:34.909330
finish_reason: stop
---

что вместо константы \( \lambda \) в предыдущем примере у нас есть функция от предыдущего вектора скрытого состояния \( h_{t-1} \) и текущих входных данных \( x_t \), тоже производящая шлюзование, то есть ее значение находится в промежутке от 0 до 1. При такой функции шлюзования уравнение обновления RNN будет выглядеть следующим образом:

\[
h_t = h_{t-1} + \lambda(h_{t-1}, x_t) F(h_{t-1}, x_t).
\]

Теперь становится ясно, что функция \( \lambda \) управляет тем, какая доля текущего входного вектора участвует в обновлении состояния \( h_{t-1} \). Более того, \( \lambda \) зависит от контекста. Так работают все шлюзовые нейронные сети. Функция \( \lambda \) обычно представляет собой сигма-функцию, которая, как мы знаем из главы 3, возвращает значение от 0 до 1.

В случае сетей с долгой краткосрочной памятью (long short-term memory network, LSTM (Hochreiter, Schmidhuber, 1997)) этот механизм аккуратно расширяется, включая не только условные обновления, но и умышленное «забывание» значений из предыдущего скрытого состояния \( h_{t-1} \). Подобное «забывание» выполняется путем умножения предыдущего скрытого состояния \( h_{t-1} \) на еще одну функцию, \( \mu \), также возвращающую значения от 0 до 1 и зависящую от текущих входных данных:

\[
h_t = \mu(h_{t-1}, x_t) h_{t-1} + \lambda(h_{t-1}, x_t) F(h_{t-1}, x_t).
\]

Как вы могли догадаться, \( \mu \) — тоже функция шлюзования. Настоящее описание LSTM более запутанное из-за параметризации функций шлюзования, что приводит к довольно сложной (для непосвященных) последовательности операций. Но вооружившись полученным в этом разделе представлением о шлюзовании, вы уже готовы подробнее рассмотреть механизмы обновления LSTM. Рекомендуем вам классическую статью Кристофера Олаха (Christopher Olah) (http://colah.github.io/posts/2015-08-Understanding-LSTMs/). Мы воздержимся от освещения этих вопросов в книге, поскольку такие подробности не важны для применения LSTM в приложениях NLP.

LSTM — лишь один из множества шлюзовых вариантов RNN. Еще один вариант, все более популярный в последние годы, — шлюзовой рекуррентный блок (gated recurrent unit, GRU (Chung et al., 2015)). К счастью, в PyTorch для перехода на LSTM можно просто заменить nn.RNN или nn.RNNCell на nn.LSTM или nn.LSTMCell соответственно без каких-либо других модификаций кода (и аналогично для GRU)!

Механизм шлюзования — эффективное решение проблем, перечисленных выше в этом разделе. Он позволяет не только управлять обновлениями, но и контролировать градиенты, да и обучение при этом упрощается. Без дальнейших проволочек мы теперь покажем на двух примерах шлюзовые архитектуры в действии.