---
source_image: page_022.png
page_number: 22
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 40.18
tokens: 7761
characters: 2700
timestamp: 2025-12-24T02:21:51.796851
finish_reason: stop
---

Хотя математическая формализация не обязательна для успешного моделирования в сфере NLP/глубокого обучения или для написания данной книги, мы формально опишем парадигму обучения с учителем и познакомим новичков в этой области со стандартной терминологией, чтобы им были понятны обозначения и стиль написания научных статей, с которыми они могут встретиться в arXiv.

Рассмотрим набор данных \( D = \{ x_i, y_i \}_{i=1}^n \), в котором количество выборок равно \( n \). Нам нужно на основе этого набора данных обучить функцию (модель) \( f \), параметризованную весами \( w \). Иначе говоря, делается предположение о структуре модели \( f \), а при заданной структуре полученные в результате обучения значения весов \( w \) полностью характеризуют модель. Для входных данных \( X \) модель предсказывает значение \( \hat{y} \) целевой переменной:

\[
\hat{y} = f(X, w).
\]

В обучении с учителем в случае обучающих выборок нам известно истинное значение целевой переменной для наблюдаемой величины. Функция потерь в данном случае будет равна \( L(y, \hat{y}) \). Таким образом, обучение с учителем превращается в поиск оптимальных значений параметров/весов \( w \), при которых достигается минимум совокупных потерь для всех \( n \) выборок.

ОБУЧЕНИЕ С ПОМОЩЬЮ СТОХАСТИЧЕСКОГО ГРАДИЕНТНОГО СПУСКА

Задача машинного обучения с учителем состоит в поиске значений параметров, минимизирующих функцию потерь для данного набора данных. Другими словами, это эквивалентно поиску корней уравнения. Как известно, градиентный спуск (gradient descent) — распространенный метод поиска корней уравнения. Напомним, что в обычном методе градиентного спуска выбираются какие-либо начальные значения для корней (параметров), после чего они обновляются в цикле до тех пор, пока вычисленное значение целевой функции (функции потерь) не окажется ниже заданного порогового значения (критерий сходимости). Для больших наборов данных реализация обычного градиентного спуска — задача почти неразрешимая вследствие ограничений памяти и очень медленно работающая из-за вычислительных издержек. Вместо этого обычно используется аппроксимация градиентного спуска, называемая стохастическим градиентным спуском (stochastic gradient descent, SGD). При этом случайным образом выбирается точка данных (или подмножество точек данных), и для заданного подмножества вычисляется градиент. В случае отдельной точки данных такой подход называется чистым SGD, а в случае подмножества (из более чем одной) точек данных — мини-пакетным SGD. Эпитеты «чистый» и «мини-пакетный» обычно опускают, если используемый вариант метода понятен из контекста. На практике чистый SGD применяется редко из-за его очень медленной сходимости вследствие шума.