---
source_image: page_038.png
page_number: 38
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 17.42
tokens: 7348
characters: 1477
timestamp: 2025-12-24T02:21:56.569428
finish_reason: stop
---

Резюме

В этой главе мы познакомили вас с основными темами книги — обработкой текстов на естественных языках (NLP) и глубоким обучением — и рассмотрели все детали парадигмы обучения с учителем. Теперь вы уже должны хорошо разбираться в таких понятиях, как наблюдаемые величины, целевые переменные, модели, параметры, предсказания, функция потерь, представления, обучение и вывод (или по крайней мере знать об их существовании). Мы также продемонстрировали вам, как кодировать входные данные (наблюдаемые величины и целевые переменные) для задач обучения с помощью унитарного кодирования. Кроме того, вы посмотрели на представления на основе подсчетов, такие как TF и TF-IDF. Мы начали наше знакомство с PyTorch с графов вычислений, затем сравнили статические графы вычислений с динамическими и обсудили операции PyTorch для работы с тензорами. В главе 2 вас ждет обзор традиционного NLP. Если вы новичок в тематике данной книги, то эти две главы станут для вас необходимым фундаментом для оставшихся глав.

Библиография

1. Официальная документация по API PyTorch: http://bit.ly/2RjBxVw.
2. Dougherty J., Kohavi R., Sahami M. Supervised and Unsupervised Discretization of Continuous Features // Proceedings of the 12th International Conference on Machine Learning, 1995.
3. Collobert R., Weston J. A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning // Proceedings of the 25th International Conference on Machine Learning, 2008.