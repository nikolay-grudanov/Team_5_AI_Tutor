---
source_image: page_142.png
page_number: 142
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 43.31
tokens: 7818
characters: 2866
timestamp: 2025-12-24T02:25:10.822781
finish_reason: stop
---

Введение в рекуррентные нейронные сети

Цель рекуррентных нейронных сетей состоит в моделировании последовательностей тензоров1. Рекуррентные нейронные сети, подобно упреждающим, представляют собой целое семейство моделей. В семействе RNN содержится несколько различных моделей, из которых мы будем использовать только простейший вариант, иногда называемый RNN Элмана (Elman RNN)2. Задача рекуррентных сетей — как простейшей сети Элмана, так и более сложных форм, описанных в главе 7, — обучение представлениям последовательности. Для этого предусмотрен вектор скрытого состояния, захватывающий текущее состояние последовательности. Он вычисляется на основе текущего входного вектора и предыдущего вектора скрытого состояния. Эти связи показаны на рис. 6.1, где демонстрируются как функциональная (слева), так и «развернутая» схемы вычислительных зависимостей. На обеих иллюстрациях выходной вектор совпадает со скрытым. Это не всегда так, но в случае RNN Элмана предсказания совпадают со скрытым вектором.

Рассмотрим более подробное описание, чтобы понять, что происходит внутри RNN Элмана. Как показано в «развернутом» представлении на рис. 6.1, известном также под названием «обратное распространение ошибок во времени» (backpropagation through time, BPTT), входной вектор с текущего шага времени и скрытый вектор с предыдущего шага отображаются в скрытый вектор состояния текущего шага. Показанный подробнее на рис. 6.2 новый скрытый вектор вычисляется с помощью матрицы весов «скрытый в скрытый» — для отображения предыдущего скрытого вектора состояния и матрицы весов «входной в скрытый» — для отображения входного вектора.

Критически важно, чтобы веса «скрытый в скрытый» и «входной в скрытый» могли совместно использоваться на различных временных шагах. Из этого факта следует сделать вывод, что при обучении эти веса необходимо подогнать таким образом, чтобы RNN обучалась учитывать поступающую информацию и поддерживать представление состояния, которое бы подытоживало все обработанные до сих пор входные данные. RNN ниоткуда не может узнать, на каком временном шаге она находится. Вместо этого она просто обучается переходу с одного шага на другой с хранением представления состояния, которое бы обеспечивало минимальную функцию потерь.

1 Как вы помните из главы 1, в виде тензора можно выразить все что угодно. В данном случае RNN моделирует последовательность элементов в заданные моменты времени. Каждый из этих элементов можно выразить в виде тензора. В данной главе мы иногда будем использовать вместо слова «тензор» слово «вектор». Размерность будет понятна из контекста.
2 В этой главе под RNN мы будем понимать RNN Элмана. На самом деле все, что мы будем в главе моделировать, можно смоделировать с помощью других рекуррентных нейронных сетей (чему посвящена глава 8), но для простоты ограничимся RNN Элмана. Имейте это в виду при чтении главы.