---
source_image: page_173.png
page_number: 173
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 17.25
tokens: 7364
characters: 1582
timestamp: 2025-12-24T02:25:35.303606
finish_reason: stop
---

8 Продвинутое моделирование последовательностей для обработки текстов на естественных языках

В этой главе мы продолжим обсуждать концепции моделирования последовательностей, о которых говорили в главах 6 и 7, и еще подробнее рассмотрим моделирование преобразования последовательностей в последовательности (sequence-to-sequence modeling), когда модель получает на входе последовательность и возвращает другую последовательность, возможно, другой длины. Примеры задач преобразования последовательностей в последовательности встречаются повсеместно. Например, по сообщению электронной почты предсказать ответ, предсказать английский перевод французской фразы или по статье составить ее краткое изложение. Мы также обсудим отличающиеся структурно варианты моделей последовательностей, в частности двунаправленные модели. Чтобы добиться от представления последовательностей максимальной эффективности, познакомим вас с механизмом внимания и подробнее остановимся на этом вопросе. Наконец, эта глава завершается подробным описанием машинного перевода с помощью нейронных сетей (neural machine translation, NMT) как реализации описанных здесь концепций.

Модели преобразования последовательностей в последовательности, модели типа «кодировщик-декодировщик» и контекстно обусловленная генерация

Модели преобразования последовательностей в последовательности (S2S) — частный случай общего семейства моделей типа «кодировщик-декодировщик». Модель типа «кодировщик-декодировщик» представляет собой композицию двух моделей (рис. 8.1), кодировщика и декодировщика, обычно обучаемых совместно.