---
source_image: page_195.png
page_number: 195
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 31.81
tokens: 7704
characters: 2771
timestamp: 2025-12-24T02:26:25.216731
finish_reason: stop
---

вектора запроса, а набор векторов состояния кодировщика — векторов как ключей, так и значений.

Скалярное произведение скрытого состояния декодировщика с векторами из состояния кодировщика дает в результате по скалярному значению для каждого из элементов закодированной последовательности. После применения многомерной логистической функции эти скалярные значения становятся распределениями вероятностей для векторов состояния кодировщика1. Эти вероятности применяются для умножения векторов состояния кодировщика на веса перед получением в результате их сложения. Используется один вектор для каждого элемента пакета. В завершение скрытое состояние декодировщика может выборочно умножать состояние кодировщика на веса на каждом из временных шагов. Это служит своеобразным «прожектором», благодаря которому модель может научиться «выделять» информацию, необходимую для генерации выходной последовательности.

В примере 8.8 мы продемонстрируем этот вариант механизма внимания. Первая функция пытается подробно описать операции. Кроме того, в ней используется операция view(), добавляющая измерения размером 1 для последующего трансформирования тензора с другим тензором2. В версии terse_attention() операция view() заменяется наиболее популярной операцией unsqueeze(). Кроме того, вместо поэлементного умножения и суммирования используется более эффективная математически операция matmul().

Пример 8.8. Механизм внимания, при котором поэлементное умножение и суммирование выполняются более явным образом

def verbose_attention(encoder_state_vectors, query_vector):
    """
    encoder_state_vectors: трехмерный вектор из bi-GRU в кодировщике
    query_vector: скрытое состояние GRU декодировщика
    """
    batch_size, num_vectors, vector_size = encoder_state_vectors.size()
    vector_scores = \
        torch.sum(encoder_state_vectors * query_vector.view(batch_size, 1,
            vector_size),
            dim=2)
    vector_probabilities = F.softmax(vector_scores, dim=1)
    weighted_vectors = \
        encoder_state_vectors * vector_probabilities.view(batch_size,
            num_vectors, 1)

1 Каждый элемент мини-пакета представляет собой последовательность, а сумма вероятностей для каждой последовательности равна 1.
2 Транслирование происходит, если в тензоре есть измерение размером 1. Пусть этот тензор называется тензор А. При выполнении поэлементной операции (например, сложения или вычитания) над этим и еще одним тензором (назовем его тензор В) их формы должны совпадать, за исключением измерения размерностью 1. Операция над тензором А и тензором В повторяется для каждой из позиций тензора В. Если форма тензора А — (10, 1, 10), а тензора В — (10, 5, 10), то при операции А + В сложение тензора А будет повторено для каждой из пяти позиций тензора В.