---
source_image: page_092.png
page_number: 92
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 39.82
tokens: 7765
characters: 2610
timestamp: 2025-12-24T02:23:46.503903
finish_reason: stop
---

Впрочем, чтобы на основе вектора предсказаний получить вероятности, необходим дополнительный шаг. А именно функция активации на основе многомерной логистической функции для преобразования вектора значений в вероятности. Исторически многомерная логистическая функция применялась во многих сферах. В физике она носит название распределения Больцмана или Гиббса; в статистике это полиномиальная логистическая регрессия; а в сообществе специалистов по NLP она известна в качестве классификатора методом максимальной энтропии (MaxEnt)1. Какое бы название ни использовалось, все равно понятно, что при больших положительных значениях вероятности будут больше, а при меньших отрицательных — меньше. В примере 4.3 этот дополнительный шаг определяется аргументом apply_softmax. В примере 4.4 выводится то же самое, но флаг apply_softmax теперь равен true.

Пример 4.4. Вывод вероятностей с помощью классификатора на основе многослойного перцептрона (обратите внимание на параметр apply_softmax=True)

<table>
  <tr>
    <th>Input[0]</th>
    <td>y_output = mlp(x_input, apply_softmax=True)<br>describe(y_output)</td>
  </tr>
  <tr>
    <th>Output[0]</th>
    <td>Type: torch.FloatTensor<br>Shape/size: torch.Size([2, 4])<br>Values:<br>tensor([[ 0.2087,  0.2868,  0.3127,  0.1919],<br>        [ 0.1832,  0.2824,  0.3649,  0.1696]])</td>
  </tr>
</table>

В заключение упомянем, что многослойные перцептроны представляют собой расположенные ярусами линейные слои, отображающие тензоры в другие тензоры. Между каждой парой линейных слоев располагается нелинейность, прерывающая линейную связь, благодаря которой модель может деформировать окружающее векторное пространство. При классификации подобная деформация должна обеспечивать линейную разделяемость классов. Кроме того, можно воспользоваться многомерной логистической функцией для интерпретации результатов MLP как вероятностей, но не следует применять многомерную логистическую функцию с определенными функциями потерь2, поскольку в основе их реализации могут лежать различные математические/вычислительные упрощения.

1 Это очень существенный вопрос. Более глубокое его изучение выходит за рамки данной книги, но мы рекомендуем вам прочитать посвященное этой теме руководство Френка Ферраро (Frank Ferraro) и Джейсона Айзнера (Jason Eisner) (http://bit.ly/2rPeW8G).

2 Хотя мы и упоминаем эту особенность, но не станем углубляться в подробности взаимодействий между выходными нелинейностями и функциями потерь. Этот вопрос подробно и понятно изложен в документации фреймворка PyTorch (http://bit.ly/2RFOIjM) — именно к ней вам следует обращаться при необходимости.