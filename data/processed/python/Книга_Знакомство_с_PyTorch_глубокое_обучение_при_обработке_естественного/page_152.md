---
source_image: page_152.png
page_number: 152
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 38.81
tokens: 7644
characters: 2572
timestamp: 2025-12-24T02:25:18.003734
finish_reason: stop
---

Процедура обучения и результаты

Процедура обучения следует стандартной схеме. Для каждого пакета данных применяется модель и вычисляются векторы предсказаний. С помощью функции CrossEntropyLoss() и эталонных данных вычисляется функция потерь. На базе значений потерь и оптимизатора вычисляются градиенты, а на их основе обновляются веса модели. Этот процесс повторяется для каждого из пакетов в обучающих данных. Аналогично поступаем с проверочными данными, но модель устанавливается в режим оценки, чтобы предотвратить обратное распространение ошибки. Проверочные данные используются только для получения менее предвзятого представления об эффективности работы модели. Данная процедура повторяется определенное количество эпох. Код вы можете найти в прилагаемых к книге материалах.

Рекомендуем вам поэкспериментировать с гиперпараметрами, чтобы лучше прочувствовать, как они влияют на эффективность и насколько, и составить таблицу полученных результатов. Мы также оставляем вам в качестве упражнения написание подходящей базовой модели для этой задачи1. Реализованная в подразделе «Модель SurnameClassifier» на с. 179 модель носит общий характер, ее применимость не ограничивается символами. Слой вложений в той модели способен отобразить любой дискретный элемент в последовательность дискретных элементов; например, предложение — это последовательность слов. Попробуйте код из примера 6.6 и для других задач классификации, например классификации предложений.

Пример 6.6. Аргументы для SurnameClassifier на основе RNN

args = Namespace(
    # Информация о данных и путях
    surname_csv="data/surnames/surnames_with_splits.csv",
    vectorizer_file="vectorizer.json",
    model_state_file="model.pth",
    save_dir="model_storage/ch6/surname_classification",
    # Гиперпараметры модели
    char_embedding_size=100,
    rnn_hidden_size=64,
    # Гиперпараметры обучения
    num_epochs=100,

1 Для разминки представьте себе MLP, получающий на входе набор символьных униграмм. Затем представьте на его входе набор символьных биграмм. Вполне возможно, что базовые модели окажутся для этой задачи более подходящими, чем простая модель на основе RNN. Это весьма поучительно: для проектирования признаков достаточно сообщить базовой модели, что в символьных биграммах есть сигнал. Подсчитайте количество параметров в вариантах с униграммами и биграммами в качестве входных данных и сравните их с RNN-моделью из этой главы. Больше или меньше параметров оказалось и почему? Наконец, можете ли вы придумать более простую, чем MLP, эффективную базовую модель для данной задачи?