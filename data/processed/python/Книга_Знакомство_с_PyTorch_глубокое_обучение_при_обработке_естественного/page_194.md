---
source_image: page_194.png
page_number: 194
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 34.51
tokens: 7768
characters: 2874
timestamp: 2025-12-24T02:26:25.287290
finish_reason: stop
---

цикл генерации из главы 7, но с несколькими отличиями, очевидно, результатами методологического выбора при используемом Луоном, Фамом и Мэннингом стиле внимания. Во-первых, целевая последовательность представляет собой наблюдения на каждом из временных шагов1. С помощью GRUCell вычисляются скрытые состояния. Начальное скрытое состояние определяется путем применения линейного слоя к сцепленным итоговым скрытым состояниям кодировщика bi-GRU2. Входные данные декодировщика GRU на каждом из временных шагов представляют собой сцепленный вектор вложенного входного токена и вектора контекста с предыдущего шага. Вектор контекста должен захватывать полезную для текущего временного шага информацию и контекстно обусловливать выходные данные модели. На первом временному шаге вектор контекста состоит из нулей, представляя, таким образом, отсутствие контекста, благодаря чему какой-либо вклад в вычисления GRU математически вносят только входные данные.

При новом скрытом состоянии в качестве вектора запроса создается новый набор векторов контекста с помощью механизма внимания для текущего временного шага. В результате конкатенации этих векторов контекста со скрытым состоянием создается вектор, представляющий информацию декодировки для соответствующего временного шага. Этот вектор состояния информации декодировки используется в классификаторе (в данном случае в простом линейном слое) для создания вектора предсказаний, score_for_y_t_index. Из полученных векторов предсказаний с помощью многомерной логистической функции можно сделать распределения вероятностей для выходного словаря или с помощью функции потерь на основе перекрестной энтропии задействовать их для оптимизации по эталонным целевым значениям. Прежде чем говорить о том, как векторы предсказаний используются в процедуре обучения, взглянем на вычисления, связанные с собственно вниманием.

Подробнее изучаем механизм внимания

Важно понимать, как именно работает механизм внимания в данном примере. Как вы помните из пункта «Внимание в глубоких нейронных сетях» на с. 210, механизм внимания можно описать в терминах запросов, ключей и значений. Функция вычисления показателей принимает на входе вектор запроса и векторы ключей, на основе которых вычисляет набор весов для выбора векторов значений. В данном примере ее роль играет скалярное произведение, но возможны и другие3. В этом примере скрытое состояние декодировщика выступает в роли

1 Векторизатор присоединяет к началу последовательности токен BEGIN-OF-SEQUENCE, так что первое наблюдение всегда представляет собой специальный токен, указывающий границу последовательности.
2 Обсуждение вопросов соединения кодировщиков и декодировщиков в нейронном машинном переводе можно найти в разделе 7.3 книги Нойбига (Neubig, 2007).
3 В статье Луона, Фама и Мэннинга (Luong, Pham, Manning, 2015) описываются три различные функции вычисления показателей.