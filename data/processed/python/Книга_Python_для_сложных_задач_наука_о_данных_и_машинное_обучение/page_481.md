---
source_image: page_481.png
page_number: 481
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 23.55
tokens: 7349
characters: 1425
timestamp: 2025-12-24T01:03:38.997681
finish_reason: stop
---

Аналогично тому, как использование информации из двух деревьев позволяет достичь лучшего результата, применение информации из многих обеспечит еще лучший результат.

**Ансамбли оценивателей: случайные леса**

Идея комбинации нескольких переобученных оценивателей для снижения эффекта этого переобучения лежит в основе метода ансамблей под названием «баггинг» (bagging). Баггинг использует ансамбль (например, своеобразную «шляпу фокусника») параллельно работающих переобучаемых оценивателей и усредняет результаты для получения оптимальной классификации. Ансамбль случайных деревьев принятия решений называется случайным лесом (random forest).

Выполнить подобную баггинг-классификацию можно вручную с помощью метаоценивателя BaggingClassifier из библиотеки Scikit-Learn, как показано на рис. 5.74:

In[8]: from sklearn.tree import DecisionTreeClassifier
    from sklearn.ensemble import BaggingClassifier

    tree = DecisionTreeClassifier()
    bag = BaggingClassifier(tree, n_estimators=100, max_samples=0.8,
                            random_state=1)
    bag.fit(X, y)
    visualize_classifier(bag, X, y)

![Границы принятия решений для ансамбля случайных деревьев решений](../images/05_74.png)

Рис. 5.74. Границы принятия решений для ансамбля случайных деревьев решений

В этом примере мы рандомизировали данные путем обучения всех оценивателей на случайном подмножестве, состоящем из 80 % обучающих точек. На практике