---
source_image: page_399.png
page_number: 399
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 29.71
tokens: 7418
characters: 1722
timestamp: 2025-12-24T01:01:36.281212
finish_reason: stop
---

Рис. 5.15. Простая линейная регрессионная аппроксимация наших данных

Пример обучения с учителем: классификация набора данных Iris

Рассмотрим другой пример того же процесса, воспользовавшись обсуждавшимся ранее набором данных Iris. Зададимся вопросом: насколько хорошо мы сможем предсказать метки остальных данных с помощью модели, обученной на некоторой части данных набора Iris?

Для этой задачи мы воспользуемся чрезвычайно простой обобщенной моделью, известной под названием «Гауссов наивный байесовский классификатор», исходящей из допущения, что все классы взяты из выровненного по осям координат Гауссова распределения (см. раздел «Заглянем глубже: наивная байесовская классификация» данной главы). Гауссов наивный байесовский классификатор в силу отсутствия гиперпараметров и высокой производительности — хороший кандидат на роль эталонной классификации. Имеет смысл поэкспериментировать с ним, прежде чем выяснить, можно ли получить лучшие результаты с помощью более сложных моделей.

Мы собираемся проверить работу модели на неизвестных ей данных, так что необходимо разделить данные на обучающую последовательность (training set) и контрольную последовательность (testing set). Это можно сделать вручную, но удобнее воспользоваться вспомогательной функцией train_test_split:

In[15]: from sklearn.cross_validation import train_test_split
    Xtrain, Xtest, ytrain, ytest = train_test_split(X_iris, y_iris,
                                                    random_state=1)

После упорядочения данных последуем нашему рецепту для предсказания меток:

In[16]: from sklearn.naive_bayes import GaussianNB # 1. Выбираем класс модели
        model = GaussianNB()                        # 2. Создаем экземпляр модели