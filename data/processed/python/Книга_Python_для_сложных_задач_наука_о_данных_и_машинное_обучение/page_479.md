---
source_image: page_479.png
page_number: 479
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 30.53
tokens: 7459
characters: 1738
timestamp: 2025-12-24T01:03:42.048996
finish_reason: stop
---

Если вы применяете интерактивный блокнот, то можете воспользоваться вспомогательным сценарием, включенным в онлайн-приложение (https://github.com/jakevdp/PythonDataScienceHandbook) для вызова интерактивной визуализации процесса построения дерева принятия решений (рис. 5.71):

In[6]: # Модуль helpers_05_08 можно найти в онлайн-приложении к книге
    # (https://github.com/jakevdp/PythonDataScienceHandbook)
    import helpers_05_08
    helpers_05_08.plot_tree_interactive(X, y);

![Первый кадр интерактивного виджета дерева принятия решений](../images/chapter5/fig5_71.png)

Рис. 5.71. Первый кадр интерактивного виджета дерева принятия решений. Полную версию см. в онлайн-приложении (https://github.com/jakevdp/PythonDataScienceHandbook)

Обратите внимание, что по мере возрастания глубины мы получаем области классификации очень странной формы. Например, на глубине, равной 5, между желтой и синей областями появляется узкая и вытянутая в высоту фиолетовая область. Очевидно, что такая конфигурация является скорее не результатом собственно распределения данных, а конкретной их дискретизации или свойств шума в них. То есть это дерево принятия решений на глубине всего лишь пяти уровней уже очевидным образом переобучено.

Деревья принятия решений и переобучение

Подобное переобучение присуще всем деревьям принятия решений: нет ничего проще, чем дойти до слишком глубокого уровня дерева, аппроксимируя таким образом нюансы конкретных данных вместо общих характеристик распределений, из которых они получены. Другой способ увидеть это переобучение — обратиться к моделям, обученным на различных подмножествах набора данных, например, на рис. 5.72 показано обучение двух различных деревьев, каждое на половине исходного набора данных.