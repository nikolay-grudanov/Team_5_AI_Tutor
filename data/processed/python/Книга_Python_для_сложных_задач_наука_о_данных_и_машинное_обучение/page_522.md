---
source_image: page_522.png
page_number: 522
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 26.39
tokens: 7455
characters: 1696
timestamp: 2025-12-24T01:04:44.933545
finish_reason: stop
---

Самые проверенные реализации выполняют «под капотом» немного больше действий, но основное представление о методе максимизации математического ожидания предыдущая функция дает.

Предупреждения относительно метода максимизации математического ожидания. При использовании алгоритма максимизации математического ожидания следует иметь в виду несколько нюансов.

□ Глобально оптимальный результат может оказаться недостижимым в принципе. Во-первых, хотя процедура EM гарантированно улучшает результат на каждом шаге, уверенности в том, что она ведет к глобально наилучшему решению, нет. Например, если мы воспользуемся в нашей простой процедуре другим начальным значением для генератора случайных чисел, полученные начальные гипотезы приведут к неудачным результатам (рис. 5.114):

In[6]: centers, labels = find_clusters(X, 4, rseed=0)
    plt.scatter(X[:, 0], X[:, 1], c=labels,
                s=50, cmap='viridis');
![Пример плохой сходимости в методе k-средних](../images/5_114.png)

Рис. 5.114. Пример плохой сходимости в методе k-средних

В этом случае EM-метод сошелся к глобально неоптимальной конфигурации, поэтому его часто выполняют для нескольких начальных гипотез, что и делает по умолчанию библиотека Scikit-Learn (это задается с помощью параметра n_init, по умолчанию имеющего значение 10).

□ Количество кластеров следует выбирать заранее. Еще одна часто встречающаяся проблема с методом k-средних заключается в том, что ему необходимо сообщить, какое количество кластеров вы ожидаете: он не умеет вычислять количество кластеров на основе данных. Например, если предложить алгоритму выделить шесть кластеров, он с радостью это сделает и найдет шесть оптимальных кластеров (рис. 5.115):