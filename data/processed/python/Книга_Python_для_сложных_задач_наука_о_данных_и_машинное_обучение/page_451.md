---
source_image: page_451.png
page_number: 451
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 27.36
tokens: 7403
characters: 1403
timestamp: 2025-12-24T01:02:54.689177
finish_reason: stop
---

ax[1].plot(model.steps[0][1].centers_,
    model.steps[1][1].coef_)
ax[1].set(xlabel='basis location', # Базовое местоположение
    ylabel='coefficient',      # Коэффициент
    xlim=(0, 10))

model = make_pipeline(GaussianFeatures(30), LinearRegression())
basis_plot(model)

![Коэффициенты при Гауссовых базисных функциях в чрезмерно сложной модели](https://i.imgur.com/3Q5z5QG.png)

Рис. 5.48. Коэффициенты при Гауссовых базисных функциях в чрезмерно сложной модели

Нижняя часть рис. 5.48 демонстрирует амплитуду базисной функции в каждой из точек. Это типичное поведение для переобучения с перекрытием областей определения базисных функций: коэффициенты соседних базисных функций усиливают и подавляют друг друга. Мы знаем, что подобное поведение приводит к проблемам и было бы неплохо ограничивать подобные пики в модели явным образом, «накладывая штраф» на большие значения параметров модели. Подобное «штрафование» известно под названием регуляризации и существует в нескольких вариантах.

Гребневая регрессия (L₂-регуляризация)

Вероятно, самый часто встречающийся вид регуляризации — гребневая регрессия (ridge regression), или \( L_2 \)-регуляризация (\( L_2 \)-regularization), также иногда называемая регуляризацией Тихонова (Tikhonov regularization). Она заключается в наложении штрафа на сумму квадратов (евклидовой нормы) коэффициентов модели. В данном случае штраф для модели будет равен: