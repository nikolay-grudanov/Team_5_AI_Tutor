---
source_image: page_384.png
page_number: 384
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 26.49
tokens: 7426
characters: 1508
timestamp: 2025-12-24T01:01:11.600060
finish_reason: stop
---

Самые важные алгоритмы классификации, которые мы обсудим в данной главе, — это Гауссов наивный байесовский классификатор (см. раздел «Заглянем глубже: наивная байесовская классификация» данной главы), метод опорных векторов (см. раздел «Заглянем глубже: метод опорных векторов» далее) и классификация на основе случайных лесов (см. раздел «Заглянем глубже: деревья принятия решений и случайные леса» этой главы).

Регрессия: предсказание непрерывных меток

В отличие от дискретных меток, с которыми мы имели дело в алгоритмах классификации, сейчас мы рассмотрим простую задачу регрессии, где метки представляют собой непрерывные величины.

Изучим показанные на рис. 5.4 данные, состоящие из набора точек с непрерывными метками.

Как и в примере классификации, наши данные двумерны, то есть каждая точка описывается двумя признаками. Непрерывные метки точек представлены их цветом.

![Простой набор данных для регрессии](https://i.imgur.com/3Q5z5QG.png)

Рис. 5.4. Простой набор данных для регрессии

Существует много возможных моделей регрессии, подходящих для таких данных, но мы для предсказания меток для точек воспользуемся простой линейной регрессией. Модель простой линейной регрессии основана на допущении, что, если рассматривать метки как третье пространственное измерение, можно подобрать для этих данных разделяющую плоскость. Это высокоуровневое обобщение хорошо известной задачи подбора разделяющей прямой для данных с двумя координатами. Визуализировать это можно так, как показано на рис. 5.5.