---
source_image: page_536.png
page_number: 536
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 32.15
tokens: 7440
characters: 1418
timestamp: 2025-12-24T01:05:13.954070
finish_reason: stop
---

![Метки смеси Гауссовых распределений для наших данных](https://i.imgur.com/3Q5z5QG.png)

Рис. 5.127. Метки смеси Гауссовых распределений для наших данных

Но в силу того, что GMM содержит «под капотом» вероятностную модель, с ее помощью можно также присваивать метки кластеров на вероятностной основе — в библиотеке Scikit-Learn это можно сделать методом predict_proba. Он возвращает матрицу размера [n_samples, n_clusters], содержащую оценки вероятностей принадлежности точки к конкретному кластеру:

In[8]: probs = gmm.predict_proba(X)
    print(probs[:5].round(3))

[[ 0.   0.   0.475  0.525]
 [ 0.   1.   0.   0. ]
 [ 0.   1.   0.   0. ]
 [ 0.   0.   0.   1. ]
 [ 0.   1.   0.   0. ]]

Для визуализации этой вероятности можно, например, сделать размеры точек пропорциональными степени достоверности их предсказания. Глядя на рис. 5.128, можно увидеть, что как раз точки на границах между кластерами отражают эту неопределенность отнесения точек к кластерам:

In[9]: size = 50 * probs.max(1) ** 2  # Возведение в квадрат усиливает
    plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=size);

«Под капотом» смесь Гауссовых распределений очень напоминает метод \( k \)-средних: она использует подход с максимизацией математического ожидания, который с качественной точки зрения делает следующее.

1. Выбирает первоначальные гипотезы для расположения и формы кластеров.
2. Повторяет до достижения сходимости: