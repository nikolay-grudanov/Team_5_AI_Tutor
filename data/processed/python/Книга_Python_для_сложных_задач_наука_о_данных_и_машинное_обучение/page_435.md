---
source_image: page_435.png
page_number: 435
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 35.15
tokens: 7653
characters: 2214
timestamp: 2025-12-24T01:02:36.945570
finish_reason: stop
---

Заглянем глубже: наивная байесовская классификация

Предыдущие четыре раздела были посвящены общему обзору принципов машинного обучения. В этом и следующем разделах мы подробно рассмотрим несколько специализированных алгоритмов для обучения без и с учителем, начиная с наивной байесовской классификации.

Наивные байесовские модели — группа исключительно быстрых и простых алгоритмов классификации, зачастую подходящих для наборов данных очень высоких размерностей. В силу их быстроты и столь небольшого количества настраиваемых параметров они оказываются очень удобны в качестве грубого эталона для задач классификации. Этот раздел мы посвятим наглядному объяснению работы наивных байесовских классификаторов вместе с двумя примерами их работы на некоторых наборах данных.

Байесовская классификация

Наивные байесовские классификаторы основаны на байесовских методах классификации, в основе которых лежит теорема Байеса — уравнение, описывающее связь условных вероятностей статистических величин. В байесовской классификации нас интересует поиск вероятности метки (категории) при определенных заданных признаках, являющихся результатами наблюдений/экспериментов, обозначенной \( P(L \mid \text{признаков}) \). Теорема Байеса позволяет выразить это в терминах величин, которые мы можем вычислить напрямую:

\[
P(L \mid \text{признаков}) = \frac{P(\text{признаков} \mid L)P(L)}{P(\text{признаков})}
\]

Один из способов выбора между двумя метками (\( L_1 \) и \( L_2 \)) — вычислить отношение апостериорных вероятностей для каждой из них:

\[
\frac{P(L_1 \mid \text{признаков})}{P(L_2 \mid \text{признаков})} = \frac{P(\text{признаков} \mid L_1)P(L_1)}{P(\text{признаков} \mid L_2)P(L_2)}
\]

Все, что нам теперь нужно, — модель, с помощью которой можно было бы вычислить \( P(\text{признаков} \mid L_i) \) для каждой из меток. Подобная модель называется порождающей моделью (generative model), поскольку определяет гипотетический случайный процесс генерации данных. Задание порождающей модели для каждой из меток/категорий — основа обучения подобного байесовского классификатора. Обобщенная версия подобного шага обучения — непростая задача, но мы упростим ее, приняв некоторые упрощающие допущения о виде модели.