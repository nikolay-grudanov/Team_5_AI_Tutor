---
source_image: page_509.png
page_number: 509
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 28.86
tokens: 7407
characters: 1609
timestamp: 2025-12-24T01:04:28.475910
finish_reason: stop
---

яния между всеми парами точек в наборе данных. Справа — модель, используемая алгоритмом обучения на базе многообразий, который называется локально линейным вложением (LLE). Вместо сохранения всех расстояний сохраняются только расстояния между соседними точками: в данном случае ближайшими 100 соседями каждой точки.

![Представление связей между точками в MDS и LLE](https://i.imgur.com/3Q5z5QG.png)

Рис. 5.102. Представление связей между точками в MDS и LLE

При изучении рисунка слева становится понятно, почему метод MDS не работает: способа «уплощения» этих данных с сохранением в достаточной степени длины каждого отрезка между двумя точками просто не существует. На рисунке справа, с другой стороны, ситуация выглядит несколько более оптимистично. Вполне можно представить себе разворачивание данных так, чтобы хотя бы приблизительно сохранить длины отрезков. Именно это и делает метод LLE путем нахождения глобального экстремума отражающей эту логику функции стоимости.

Существует несколько вариантов метода LLE, мы здесь будем использовать для восстановления вложенного двумерного многообразия модифицированный алгоритм LLE. В целом модифицированный алгоритм LLE работает лучше других его вариантов при восстановлении хорошо структурированных многообразий с очень небольшой дисторсией (рис. 5.103):

In[15]:
from sklearn.manifold import LocallyLinearEmbedding
model = LocallyLinearEmbedding(n_neighbors=100, n_components=2,
    method='modified',
    eigen_solver='dense')
out = model.fit_transform(XS)

fig, ax = plt.subplots()
ax.scatter(out[:, 0], out[:, 1], **colorize)
ax.set_ylim(0.15, -0.15);