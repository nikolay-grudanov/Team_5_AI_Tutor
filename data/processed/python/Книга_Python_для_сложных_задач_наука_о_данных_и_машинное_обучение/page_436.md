---
source_image: page_436.png
page_number: 436
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 19.02
tokens: 7380
characters: 1467
timestamp: 2025-12-24T01:02:21.110194
finish_reason: stop
---

Именно на этом этапе возникает слово «наивный» в названии «наивный байесовский классификатор»: сделав очень «наивное» допущение относительно порождающей модели для каждой из меток/категорий, можно будет отыскать грубое приближение порождающей модели для каждого класса, после чего перейти к байесовской классификации. Различные виды наивных байесовских классификаторов основываются на различных «наивных» допущениях относительно данных, мы рассмотрим несколько из них в следующих разделах. Начнем с обычных импортов:

In[1]: %matplotlib inline
    import numpy as np
    import matplotlib.pyplot as plt
    import seaborn as sns; sns.set()

Гауссов наивный байесовский классификатор

Вероятно, самый простой для понимания наивный байесовский классификатор — Гауссов. В этом классификаторе допущение состоит в том, что данные всех категорий взяты из простого нормального распределения. Пускай у нас имеются следующие данные (рис. 5.38):

In[2]: from sklearn.datasets import make_blobs
    X, y = make_blobs(100, 2, centers=2, random_state=2, cluster_std=1.5)
    plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='RdBu');

![Данные для наивной байесовской классификации](../images/chapter_5_38.png)

Рис. 5.38. Данные для наивной байесовской классификации

Один из самых быстрых способов создания простой модели — допущение о том, что данные подчиняются нормальному распределению без ковариации между измерениями. Для обучения этой модели достаточно найти среднее значение