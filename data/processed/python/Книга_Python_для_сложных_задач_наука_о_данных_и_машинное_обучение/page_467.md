---
source_image: page_467.png
page_number: 467
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 29.89
tokens: 7440
characters: 1758
timestamp: 2025-12-24T01:03:22.912513
finish_reason: stop
---

![Диаграмма, показывающая добавление в данные третьего измерения](https://i.imgur.com/3Q5z5QG.png)

Рис. 5.60. Добавление в данные третьего измерения дает возможность линейного разделения

Нам пришлось тщательно выбрать и внимательно настроить нашу проекцию: если бы мы не центрировали радиальную базисную функцию должным образом, то не получили бы столь «чистых», разделяемых линейно результатов. Необходимость подобного выбора — задача, требующая решения: хотелось бы каким-то образом автоматически находить оптимальные базисные функции.

Одна из применяемых с этой целью стратегий состоит в вычислении базисных функций, центрированных по каждой из точек набора данных, с тем чтобы далее алгоритм SVM проанализировал полученные результаты. Эта разновидность преобразования базисных функций, известная под названием преобразования ядра (kernel transformation), основана на отношении подобия (или ядре) между каждой парой точек.

Потенциальная проблема с этой методикой — проекцией N точек на N измерений — состоит в том, что при росте N она может потребовать колоссальных объемов вычислений. Однако благодаря изящной процедуре, известной под названием kernel trick (https://en.wikipedia.org/wiki/Kernel_method), обучение на преобразованных с помощью ядра данных можно произвести неявно, то есть даже без построения полного N-мерного представления ядерной проекции! Этот kernel trick является частью SVM и одной из причин мощи этого метода.

В библиотеке Scikit-Learn, чтобы применить алгоритм SVM с использованием ядерного преобразования, достаточно просто заменить линейное ядро на ядро RBF (radial basis function — «радиальная базисная функция») с помощью гиперпараметра модели kernel (рис. 5.61):

In[14]: clf = SVC(kernel='rbf', C=1E6)
    clf.fit(X, y)