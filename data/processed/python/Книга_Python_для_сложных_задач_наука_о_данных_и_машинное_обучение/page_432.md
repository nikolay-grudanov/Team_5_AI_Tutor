---
source_image: page_432.png
page_number: 432
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 22.65
tokens: 7407
characters: 1485
timestamp: 2025-12-24T01:02:20.598570
finish_reason: stop
---

Очевидно, что для описания зависимости между x и y нам требуется использовать более сложную модель. Сделать это можно путем преобразования данных, добавив дополнительные столбцы признаков для увеличения гибкости модели. Добавить в данные полиномиальные признаки можно следующим образом:

In[12]: from sklearn.preprocessing import PolynomialFeatures
    poly = PolynomialFeatures(degree=3, include_bias=False)
    X2 = poly.fit_transform(X)
    print(X2)

[[ 1.   1.   1.]
 [ 2.   4.   8.]
 [ 3.   9.  27.]
 [ 4.  16.  64.]
 [ 5.  25. 125.]]

В матрице производных признаков один столбец соответствует x, второй — x^2, а третий — x^3. Расчет линейной регрессии для этих расширенных входных данных позволяет получить намного лучшую аппроксимацию (рис. 5.37):

In[13]: model = LinearRegression().fit(X2, y)
    yfit = model.predict(X2)
    plt.scatter(x, y)
    plt.plot(x, yfit);

![Линейная аппроксимация по производным полиномиальным признакам](https://i.imgur.com/3Q5z5QG.png)

Рис. 5.37. Линейная аппроксимация по производным полиномиальным признакам

Идея улучшения модели путем не изменения самой модели, а преобразования входных данных является базовой для многих более продвинутых методов машинного обучения. Мы обсудим эту идею подробнее в разделе «Заглянем глубже: линейная регрессия» данной главы в контексте регрессии по комбинации базисных функций. В общем случае это путь к набору обладающих огромными возможностями методик, известных под названием «ядерные методы» (kernel