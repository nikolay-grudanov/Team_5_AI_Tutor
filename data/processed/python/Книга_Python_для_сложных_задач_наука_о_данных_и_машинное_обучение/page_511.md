---
source_image: page_511.png
page_number: 511
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 42.05
tokens: 7685
characters: 2654
timestamp: 2025-12-24T01:04:41.774694
finish_reason: stop
---

При обучении на базе многообразий смысл вложенных измерений не всегда понятен. В методе РСА смысл главных компонент совершенно ясен.

При обучении на базе многообразий вычислительная сложность методов составляет \( O[N^2] \) или даже \( O[N^3] \). Некоторые рандомизированные варианты метода РСА работают гораздо быстрее (однако в пакете megaman (https://github.com/mmp2/megaman) реализованы методы обучения на базе многообразий, масштабирующиеся гораздо лучше).

С учетом всего этого единственное безусловное преимущество методов обучения на базе многообразий перед РСА состоит в их способности сохранять нелинейные зависимости в данных. Именно поэтому я стараюсь сначала изучать данные с помощью РСА, а затем использовать методы обучения на базе многообразий.

В библиотеке Scikit-Learn реализовано несколько распространенных вариантов обучения на базе многообразий и локально линейного вложения: в документации Scikit-Learn имеется их обсуждение и сравнение (http://scikit-learn.org/stable/modules/manifold.html). Исходя из моего собственного опыта, могу дать вам следующие рекомендации.

В модельных задачах, подобных S-образной кривой, локально линейное вложение (LLE) и его варианты (особенно модифицированный метод LLE) демонстрируют отличные результаты. Они реализованы в классе sklearn.manifold.LocallyLinearEmbedding.

В случае многомерных данных, полученных из реальных источников, метод LLE часто работает плохо, и изометрическое отображение (Isomap), похоже, выдает более осмысленные вложения. Оно реализовано в классе sklearn.manifold.Isomap.

Для сильно кластеризованных данных отличные результаты демонстрирует метод стохастического вложения соседей на основе распределения Стьюдента (t-distributed stochastic neighbor embedding), хотя и работает иногда очень медленно по сравнению с другими методами. Он реализован в классе sklearn.manifold.TSNE.

Если вы хотите посмотреть, как они работают, запустите каждый из них на данных из этого раздела.

Пример: использование Isomap для распознавания лиц

Обучение на базе многообразий часто применяется при исследовании зависимостей между многомерными точками данных. Один из распространенных случаев многомерных данных — изображения. Например, набор изображений, состоящих каждое из 1000 пикселов, можно рассматривать как набор точек в 1000-мерном пространстве — яркость каждого пикселя в каждом изображении соответствует координате в соответствующем измерении.

Применим алгоритм Isomap к данным, содержащим какие-либо лица. Воспользуемся набором данных Labeled Faces in the Wild (LFW), с которым уже сталкивались в разделах «Заглянем глубже: метод опорных векторов» и «Заглянем глубже: метод