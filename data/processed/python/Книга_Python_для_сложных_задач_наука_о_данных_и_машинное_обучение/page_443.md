---
source_image: page_443.png
page_number: 443
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 35.44
tokens: 7612
characters: 2254
timestamp: 2025-12-24T01:02:49.685121
finish_reason: stop
---

Для данных с очень большим числом измерений, когда сложность модели не столь важна.

Два последних случая кажутся отдельными, но на самом деле они взаимосвязаны: по мере роста количества измерений у набора данных вероятность близости любых двух точек падает (в конце концов, чтобы находиться рядом, они должны находиться рядом по каждому из измерений). Это значит, что кластеры в многомерных случаях имеют склонность к более выраженной изоляции, чем кластеры в случаях с меньшим количеством измерений (конечно, если новые измерения действительно вносят дополнительную информацию). Поэтому упрощенные классификаторы, такие как наивный байесовский классификатор, при росте количества измерений начинают работать не хуже, а то и лучше более сложных: когда данных достаточно много, даже простая модель может оказаться весьма эффективной.

Заглянем глубже: линейная регрессия

Аналогично тому, как наивный байесовский классификатор (который мы обсуждали в разделе: «Заглянем глубже: наивная байесовская классификация» данной главы) — отличная отправная точка для задач классификации, так и линейные регрессионные модели — хорошая отправная точка для задач регрессии. Подобные модели популярны в силу быстрой обучаемости и возврата очень удобных для интерпретации результатов. Вероятно, вы уже знакомы с простейшей формой линейной регрессионной модели (то есть подбора для данных разделяющей прямой линии), но такие модели можно распространить на моделирование и более сложного поведения данных.

В этом разделе мы начнем с быстрого и интуитивно понятного математического описания известной задачи, а потом перейдем к вопросам обобщенных линейных моделей, учитывающих более сложные паттерны в данных. Начнем с обычных импортов:

In[1]: %matplotlib inline
    import matplotlib.pyplot as plt
    import seaborn as sns; sns.set()
    import numpy as np

Простая линейная регрессия

Начнем с линейной регрессии, прямолинейной аппроксимации наших данных. Прямолинейная аппроксимация представляет собой модель вида \( y = ax + b \), в которой \( a \) известна как угловой коэффициент, а \( b \) — как точка пересечения с осью координат \( Y \).

Рассмотрим следующие данные, распределенные около прямой с угловым коэффициентом 2 и точкой пересечения −5 (рис. 5.42):