---
source_image: page_562.png
page_number: 562
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 27.26
tokens: 7669
characters: 2618
timestamp: 2025-12-24T01:05:46.945963
finish_reason: stop
---

Одно из преимуществ подобного порождающего классификатора — удобство интерпретации результатов: мы получаем для каждой неизвестной выборки не только вероятностную классификацию, но и полную модель распределения точек, с которыми мы ее сравниваем! При необходимости это позволяет пролить свет на причины того, почему конкретная классификация именно такова, причины, которые такие алгоритмы, как SVM и случайные леса, скрывают.

Чтобы достичь еще большего, можно внести в нашу модель классификатора KDE некоторые усовершенствования:

□ допустить независимое изменение ширины ядра для каждого класса;
□ оптимизировать ширину ядер не на основе оценки точности предсказания, а на основе функции правдоподобия для обучающих данных при порождающей модели для каждого класса, то есть использовать оценки эффективности непосредственно из функции KernelDensity, а не общую оценку точности предсказания.

И наконец, если вы хотите приобрести опыт создания своих собственных оценивателей, можете попробовать создать аналогичный байесовский классификатор с использованием смесей Гауссовых распределений вместо KDE.

Прикладная задача: конвейер распознавания лиц

В этой главе мы рассмотрели несколько основных идей и алгоритмов машинного обучения. Но перейти от теоретических идей к настоящим прикладным задачам может оказаться непростым делом. Реальные наборы данных часто бывают зашумлены и неоднородны, в них могут отсутствовать признаки, они могут содержать данные в таком виде, который сложно преобразовать в аккуратную матрицу [n_samples, n_features]. Вам придется, прежде чем воспользоваться любым из изложенных здесь методов, сначала извлечь эти признаки из данных. Не существует готового единого шаблона, подходящего для всех предметных областей. В этом вопросе вам как исследователю данных придется использовать ваши собственные интуицию и накопленный опыт.

Одно из очень интересных приложений машинного обучения — анализ изображений, и мы уже видели несколько примеров его с использованием пиксельных признаков для классификации. На практике данные редко оказываются настолько однородными, и простых пикселов будет недостаточно. Это привело к появлению обширной литературы, посвященной методам выделения признаков (feature extraction) для изображений (см. раздел «Проектирование признаков» данной главы).

В этом разделе мы рассмотрим одну из подобных методик выделения признаков, гистограмму направленных градиентов (histogram of oriented gradients, HOG, см. https://ru.wikipedia.org/wiki/Гистограмма_направленных_градиентов), которая преобразует пиксели изображения в векторное представление, чувствительное к несущим