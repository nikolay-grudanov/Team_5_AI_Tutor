---
source_image: page_500.png
page_number: 500
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 43.71
tokens: 7735
characters: 2788
timestamp: 2025-12-24T01:04:25.732981
finish_reason: stop
---

В верхнем ряду на этом рисунке показаны входные изображения, а в нижнем — восстановленные на основе лишь 150 из почти 3000 изначальных признаков. Из этой визуализации становится понятно, почему применяющийся в разделе «Заглянем глубже: метод опорных векторов» данной главы выбор признаков с помощью РСА работал настолько успешно: хотя он понизил размерность данных почти в 20 раз, спроектированные изображения содержат достаточно информации для визуального распознавания изображенных на фотографиях персон. А значит, наш алгоритм классификации достаточно обучить на 150-мерных, а не 3000-мерных данных, что в зависимости от конкретного алгоритма может оказаться намного более эффективным.

Резюме метода главных компонент

В этом разделе мы обсудили использование метода главных компонент для понижения размерности, визуализации многомерных данных, фильтрации шума и выбора признаков в многомерных данных. Метод РСА благодаря своей универсальности и легкой интерпретируемости результатов оказался эффективным в множестве контекстов и дисциплин. Я стараюсь при работе с любым многомерным набором данных начинать с использования метода РСА для визуализации зависимостей между точками (аналогично тому, как мы сделали с рукописными цифрами), выяснения дисперсии данных (аналогично тому, как мы поступили с собственными лицами) и выяснения внутренней размерности данных (путем построения графика доли объяснимой дисперсии). РСА не подходит для всех многомерных наборов данных, но с его помощью можно просто и эффективно почерпнуть о наборе многомерных данных полезную информацию.

Основной недостаток метода РСА состоит в том, что на него оказывают сильное влияние аномальные значения в данных. Поэтому было разработано немало ошибкоустойчивых вариантов РСА, многие из них стремятся итеративно отбрасывать те точки данных, которые описываются исходными компонентами недостаточно хорошо. Библиотека Scikit-Learn содержит несколько интересных вариантов метода РСА, включая классы RandomizedPCA и SparsePCA, находящиеся в модуле sklearn.decomposition. RandomizedPCA, который мы уже встречали ранее, использует недетерминированный метод для быстрой аппроксимации нескольких первых из главных компонент данных с очень высокой размерностью, а SparsePCA вводит понятие регуляризации (см. раздел «Заглянем глубже: линейная регрессия» данной главы), служащее для обеспечения разреженности компонент.

В следующих разделах мы рассмотрим другие методы машинного обучения без учителя, основывающиеся на некоторых идеях метода РСА.

Заглянем глубже: обучение на базе многообразий

Мы уже ознакомились с возможностями метода главных компонент для решения задачи понижения размерности — снижения количества признаков набора данных с сохранением существенных зависимостей между точками. Хотя метод РСА гибок,