---
source_image: page_532.png
page_number: 532
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 36.30
tokens: 7607
characters: 2213
timestamp: 2025-12-24T01:05:05.833233
finish_reason: stop
---

Некоторые детали в изображении справа утрачены, но изображение в целом остается вполне узнаваемым. Коэффициент сжатия этого изображения — почти 1 миллион! Существуют лучшие способы сжатия информации в изображениях, но этот пример демонстрирует возможности творческого подхода к методам машинного обучения без учителя, таким как метод k-средних.

Заглянем глубже: смеси Гауссовых распределений

Модель кластеризации методом k-средних, которую мы обсуждали в предыдущем разделе, проста и относительно легка для понимания, но ее простота приводит к сложностям в применении. В частности, не вероятностная природа метода k-средних и использование им простого расстояния от центра кластера для определения принадлежности к кластеру приводит к низкой эффективности во многих встречающихся на практике ситуациях. В этом разделе мы рассмотрим смеси Гауссовских распределений, которые можно рассматривать в качестве развития идей метода k-средних, но которые могут также стать мощным инструментом для статистических оценок, выходящих за пределы простой кластеризации. Начнем с обычных импортов:

In[1]: %matplotlib inline
    import matplotlib.pyplot as plt
    import seaborn as sns; sns.set()
    import numpy as np

Причины появления GMM: недостатки метода k-средних

Рассмотрим некоторые недостатки метода k-средних и задумаемся о том, как можно усовершенствовать кластерную модель. Как мы уже видели в предыдущем разделе, в случае простых, хорошо разделяемых данных метод k-средних обеспечивает удовлетворительные результаты кластеризации.

Например, для случая простых «пятен» данных алгоритм k-средних позволяет быстро маркировать кластеры достаточно близко к тому, как мы бы маркировали их на глаз (рис. 5.124):

In[2]: # Генерируем данные
    from sklearn.datasets.samples_generator import make_blobs
    X, y_true = make_blobs(n_samples=400, centers=4,
        cluster_std=0.60, random_state=0)
    X = X[:, :-1] # Транспонируем для удобства оси координат

In[3]: # Выводим данные на график с полученными методом k-средних метками
    from sklearn.cluster import KMeans
    kmeans = KMeans(4, random_state=0)
    labels = kmeans.fit(X).predict(X)
    plt.scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis');