---
source_image: page_466.png
page_number: 466
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 21.66
tokens: 7373
characters: 1360
timestamp: 2025-12-24T01:03:11.413107
finish_reason: stop
---

Рис. 5.59. В случае нелинейных границ линейный классификатор неэффективен

Очевидно, что эти данные никаким образом линейно не разделимы. Но мы можем извлечь урок из регрессии по комбинации базисных функций, которую рассмотрели в разделе «Заглянем глубже: линейная регрессия» данной главы, и попытаться спроектировать эти данные в пространство более высокой размерности, поэтому линейного разделителя будет достаточно. Например, одна из подходящих простых проекций — вычисление радиальной базисной функции, центрированной по середине совокупности данных:

In[12]: r = np.exp(-(X ** 2).sum(1))

Визуализировать это дополнительное измерение данных можно с помощью трехмерного графика. Если вы используете интерактивный блокнот, то сможете вращать график с помощью слайдеров (рис. 5.60):

In[13]: from mpl_toolkits import mplot3d

    def plot_3D(elev=30, azim=30, X=X, y=y):
        ax = plt.subplot(projection='3d')
        ax.scatter3D(X[:, 0], X[:, 1], r, c=y, s=50, cmap='autumn')
        ax.view_init(elev=elev, azim=azim)
        ax.set_xlabel('x')
        ax.set_ylabel('y')
        ax.set_zlabel('r')

    interact(plot_3D, elev=[-90, 90], azip=(-180, 180),
             X=fixed(X), y=fixed(y));

Как видим, при наличии третьего измерения данные можно элементарно разделить линейно путем проведения разделяющей плоскости на высоте, скажем, \( r = 0.7 \).