---
source_image: page_420.png
page_number: 420
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 22.35
tokens: 7312
characters: 1276
timestamp: 2025-12-24T01:02:01.646198
finish_reason: stop
---

достигает максимума перед резким спадом в точке, где модель становится переобученной.

Как можно понять из приведенной кривой проверки, оптимальный компромисс между систематической ошибкой и дисперсией достигается для многочлена третьей степени. Вычислить и отобразить на графике эту аппроксимацию на исходных данных можно следующим образом (рис. 5.29):

In[14]: plt.scatter(X.ravel(), y)
    lim = plt.axis()
    y_test = PolynomialRegression(3).fit(X, y).predict(X_test)
    plt.plot(X_test.ravel(), y_test);
    plt.axis(lim);

![Перекрестно-проверенная оптимальная модель для приведенных на рис. 5.27 данных](../images/fig_5_29.png)

Рис. 5.29. Перекрестно-проверенная оптимальная модель для приведенных на рис. 5.27 данных

Отмечу, что для нахождения оптимальной модели не требуется вычислять оценку эффективности для обучения, но изучение зависимости между оценками эффективности для обучения и проверки дает нам полезную информацию относительно эффективности модели.

Кривые обучения

Важный нюанс сложности моделей состоит в том, что оптимальность модели обычно зависит от размера обучающей последовательности. Например, сгенерируем новый набор данных с количеством точек в пять раз больше (рис. 5.30):

In[15]: X2, y2 = make_data(200)
    plt.scatter(X2.ravel(), y2);