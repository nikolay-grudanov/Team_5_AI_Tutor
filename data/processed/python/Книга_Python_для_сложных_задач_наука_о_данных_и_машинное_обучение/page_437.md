---
source_image: page_437.png
page_number: 437
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 24.35
tokens: 7427
characters: 1481
timestamp: 2025-12-24T01:02:30.557587
finish_reason: stop
---

и стандартное отклонение точек внутри каждой из категорий — это все, что требуется для описания подобного распределения. Результат этого наивного Гауссова допущения показан на рис. 5.39.

Эллипсы на этом рисунке представляют Гауссову порождающую модель для каждой из меток с ростом вероятности по мере приближении к центру эллипса. С помощью этой порождающей модели для каждого класса мы можем легко вычислить вероятность \( P(\text{признаков} \mid L_i) \) для каждой точки данных, а следовательно, быстро рассчитать соотношение для апостериорной вероятности и определить, какая из меток с большей вероятностью соответствует конкретной точке.

Эта процедура реализована в оценивателе sklearn.naive_bayes.GaussianNB:

In[3]: from sklearn.naive_bayes import GaussianNB
    model = GaussianNB()
    model.fit(X, y);

![Визуализация Гауссовой наивной байесовской модели](https://i.imgur.com/3Q5z5QG.png)

Рис. 5.39. Визуализация Гауссовой наивной байесовской модели

Сгенерируем какие-нибудь новые данные и выполним предсказание метки:

In[4]: rng = np.random.RandomState(0)
    Xnew = [-6, -14] + [14, 18] * rng.rand(2000, 2)
    ynew = model.predict(Xnew)

Теперь у нас есть возможность построить график этих новых данных и понять, где пролегает граница принятия решений (decision boundary) (рис. 5.40):

In[5]: plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='RdBu')
    lim = plt.axis()
    plt.scatter(Xnew[:, 0], Xnew[:, 1], c=ynew, s=20, cmap='RdBu', alpha=0.1)
    plt.axis(lim);