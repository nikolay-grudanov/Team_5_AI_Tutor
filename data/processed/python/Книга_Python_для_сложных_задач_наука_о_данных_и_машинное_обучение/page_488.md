---
source_image: page_488.png
page_number: 488
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 23.63
tokens: 7368
characters: 1411
timestamp: 2025-12-24T01:03:49.675882
finish_reason: stop
---

всего визуализировать его поведение на примере двумерного набора данных. Рассмотрим следующие 200 точек (рис. 5.80):

In[2]: rng = np.random.RandomState(1)
    X = np.dot(rng.rand(2, 2), rng.randn(2, 200)).T
    plt.scatter(X[:, 0], X[:, 1])
    plt.axis('equal');
Визуально очевидно, что зависимость между величинами x и y практически линейна. Это напоминает данные линейной регрессии, которые мы изучали в разделе «Заглянем глубже: линейная регрессия» этой главы, но постановка задачи здесь несколько иная: задача машинного обучения без учителя состоит в выяснении зависимости между величинами x и y, а не в предсказании значений величины y по значениям величины x.

![Данные для демонстрации алгоритма PCA](../images/chapter_5/fig_5_80.png)

Рис. 5.80. Данные для демонстрации алгоритма PCA

В методе главных компонент выполняется количественная оценка этой зависимости путем нахождения списка главных осей координат (principal axes) данных и их использования для описания набора данных. Выполнить это с помощью оценивателя PCA из библиотеки Scikit-Learn можно следующим образом:

In[3]: from sklearn.decomposition import PCA
    pca = PCA(n_components=2)
    pca.fit(X)

Out[3]: PCA(copy=True, n_components=2, whiten=False)

При обучении алгоритм определяет некоторые относящиеся к данным величины, самые важные из них — компоненты и объяснимая дисперсия (explained variance):

In[4]: print(pca.components_)