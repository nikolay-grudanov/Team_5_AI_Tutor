---
source_image: page_414.png
page_number: 414
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 27.28
tokens: 7385
characters: 1575
timestamp: 2025-12-24T01:01:57.887332
finish_reason: stop
---

414 Глава 5 • Машинное обучение

☐ собрать больше выборок для обучения;
☐ собрать больше данных для добавления новых признаков к каждой заданной выборке.

Ответ на этот вопрос зачастую парадоксален. В частности, иногда использование более сложной модели приводит к худшим результатам, а добавление новых выборок для обучения не приводит к их улучшению! Успешных специалистов-практиков в области машинного обучения как раз и отличает умение определять, какие действия улучшают характеристики модели.

Компромисс между систематической ошибкой и дисперсией

По существу, выбор «оптимальной модели» состоит в поиске наилучшего компромисса между систематической ошибкой (bias) и дисперсией (variance). Рассмотрим рис. 5.24, на котором представлены два случая регрессионной аппроксимации одного набора данных.

Модели со значительной систематической ошибкой: недообученные данные
Модели с высокой дисперсией: переобученные данные

Рис. 5.24. Модели регрессии со значительной систематической ошибкой и высокой дисперсией

Очевидно, что обе модели не слишком хорошо аппроксимируют наши данные, но проблемы с ними различны.

Приведенная слева модель пытается найти прямолинейное приближение к данным. Но в силу того, что внутренняя структура данных сложнее прямой линии, с помощью прямолинейной модели невозможно описать этот набор данных достаточно хорошо. О подобной модели говорят, что она недообучена (underfit), то есть гибкость модели недостаточна для удовлетворительного учета всех признаков в данных. Другими словами, у этой модели имеется значительная систематическая ошибка.