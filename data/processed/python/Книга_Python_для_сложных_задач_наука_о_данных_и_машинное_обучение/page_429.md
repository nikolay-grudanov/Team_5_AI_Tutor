---
source_image: page_429.png
page_number: 429
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 26.10
tokens: 7469
characters: 2030
timestamp: 2025-12-24T01:02:20.769080
finish_reason: stop
---

Текстовые признаки

При проектировании признаков часто требуется преобразовывать текст в набор репрезентативных числовых значений. Например, в основе наиболее автоматических процедур извлечения данных социальных медиа лежит определенный вид кодирования текста числовыми значениями. Один из простейших методов кодирования данных — по количеству слов: для каждого фрагмента текста подсчитывается количество вхождений в него каждого из слов, после чего результаты помещаются в таблицу.

Рассмотрим следующий набор из трех фраз:

In[6]: sample = ['problem of evil',
                'evil queen',
                'horizon problem']

Для векторизации этих данных на основе числа слов можно создать столбцы, соответствующие словам problem, evil, horizon и т. д. Хотя это можно сделать вручную, мы избежим нудной работы, воспользовавшись утилитой CountVectorizer библиотеки Scikit-Learn:

In[7]: from sklearn.feature_extraction.text import CountVectorizer
    vec = CountVectorizer()
    X = vec.fit_transform(sample)
    X

Out[7]: <3x5 sparse matrix of type '<class 'numpy.int64'>'
    with 7 stored elements in Compressed Sparse Row format>

Результат представляет собой разреженную матрицу, содержащую количество вхождений каждого из слов. Для удобства мы преобразуем ее в объект DataFrame с маркированными столбцами:

In[8]: import pandas as pd
    pd.DataFrame(X.toarray(), columns=vec.get_feature_names())

Out[8]:   evil  horizon  of  problem  queen
    0      1        0     1       1      0
    1      1        0     0       0      1
    2      0        1     0       1      0

У этого подхода существуют проблемы: использование непосредственно количеств слов ведет к признакам, с которыми встречающимся очень часто словам придается слишком большое значение, а это в некоторых алгоритмах классификации может оказаться субоптимальным. Один из подходов к решению этой проблемы известен под названием «терма-обратная частотность документа» (term frequency-inverse document frequency) или TF-IDF. При нем слова получают вес с учетом