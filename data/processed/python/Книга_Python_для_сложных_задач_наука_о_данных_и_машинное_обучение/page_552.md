---
source_image: page_552.png
page_number: 552
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 34.90
tokens: 7620
characters: 2352
timestamp: 2025-12-24T01:05:40.280906
finish_reason: stop
---

Два последних графика представляют собой примеры одномерной ядерной оценки плотности распределения: в первом используется так называемое ядро типа «цилиндр», а во втором — Гауссово ядро. Рассмотрим ядерную оценку плотности распределения более подробно.

Ядерная оценка плотности распределения на практике

Свободными параметрами ядерной оценки плотности распределения являются ядро (kernel), определяющее форму распределения в каждой точке, и ширина ядра (kernel bandwidth), определяющая размер ядра в каждой точке. На практике для ядерной оценки плотности распределения существует множество различных ядер: в частности, реализация KDE библиотеки Scikit-Learn поддерживает использование одного из шести ядер, о которых вы можете прочитать в посвященной оцениванию плотности документации библиотеки Scikit-Learn (http://scikit-learn.org/stable/modules/density.html).

Хотя в языке Python реализовано несколько вариантов ядерной оценки плотности (особенно в пакетах SciPy и StatsModels), я предпочитаю использовать вариант из Scikit-Learn по причине гибкости и эффективности. Он реализован в оценивателе sklearn.neighbors.KernelDensity, умеющем работать с KDE в многомерном пространстве с одним из шести ядер и одной из нескольких дюжин метрик. В силу того что метод KDE может потребовать значительных вычислительных затрат, этот оцениватель использует «под капотом» алгоритм на основе деревьев и умеет достигать компромисса между временем вычислений и точностью с помощью параметров atol (absolute tolerance, допустимая абсолютная погрешность) и rtol (relative tolerance, допустимая относительная погрешность). Определить ширину ядра — свободный параметр — можно стандартными инструментами перекрестной проверки библиотеки Scikit-Learn.

Рассмотрим простой пример воспроизведения предыдущего графика с помощью оценивателя KernelDensity библиотеки Scikit-Learn (рис. 5.145):

In[10]: from sklearn.neighbors import KernelDensity

    # Создание экземпляра модели KDE и ее обучение
    kde = KernelDensity(bandwidth=1.0, kernel='gaussian')
    kde.fit(x[:, None])

    # score_samples возвращает логарифм плотности распределения вероятности
    logprob = kde.score_samples(x_d[:, None])

    plt.fill_between(x_d, np.exp(logprob), alpha=0.5)
    plt.plot(x, np.full_like(x, -0.01), '|k', markeredgewidth=1)
    plt.ylim(-0.02, 0.22)

Out[10]: (-0.02, 0.22)