---
source_image: page_482.png
page_number: 482
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 26.64
tokens: 7444
characters: 1789
timestamp: 2025-12-24T01:03:42.488971
finish_reason: stop
---

для более эффективной рандомизации деревьев принятия решений обеспечивается определенная стохастичность процесса выбора разбиений. При этом всякий раз в обучении участвуют все данные, но результаты обучения все равно сохраняют требуемую случайность. Например, при выборе, по какому признаку выполнять разбиение, случайное дерево может выбирать из нескольких верхних признаков. Узнать больше технических подробностей о стратегиях рандомизации можно в документации библиотеки Scikit-Learn (http://scikit-learn.org/stable/modules/ensemble.html#forest) и упомянутых в ней справочных руководствах.

В библиотеке Scikit-Learn подобный оптимизированный ансамбль случайных деревьев принятия решений, автоматически выполняющий всю рандомизацию, реализован в оценивателе RandomForestClassifier. Все, что остается сделать, — выбрать количество оценивателей и он очень быстро (при необходимости параллельно) обучит ансамбль деревьев (рис. 5.75):

In[9]: from sklearn.ensemble import RandomForestClassifier

    model = RandomForestClassifier(n_estimators=100, random_state=0)
    visualize_classifier(model, X, y);

![Ансамбль случайных деревьев принятия решений](../images/chapter_5/fig_5_75.png)

Рис. 5.75. Ансамбль случайных деревьев принятия решений

Как видим, путем усреднения более чем 100 случайно возмущенных моделей мы получаем общую модель, намного более близкую к нашим интуитивным представлениям о правильном разбиении параметрического пространства.

Регрессия с помощью случайных лесов

В предыдущем разделе мы рассмотрели случайные леса в контексте классификации. Случайные леса могут также оказаться полезными для регрессии (то есть непрерывных, а не категориальных величин). В этом случае используется оцениватель RandomForestRegressor, синтаксис которого напоминает показанный выше.