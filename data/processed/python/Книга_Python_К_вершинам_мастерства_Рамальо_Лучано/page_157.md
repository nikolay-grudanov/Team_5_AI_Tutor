---
source_image: page_157.png
page_number: 157
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 55.68
tokens: 12053
characters: 3008
timestamp: 2025-12-24T01:42:10.990509
finish_reason: stop
---

Резюме

чально известные ошибки UnicodeEncodeError, UnicodeDecodeError и SyntaxError, вызванные неправильным кодированием исходного файла Python.

Продолжая тему исходного кода, я изложил свою точку зрения на использование идентификаторов, содержащих не-ASCII символы: если программисты, сопровождающие программу, хотят, чтобы ее код был написан в манере, близкой к естественному языку, в котором встречаются не-ASCII символы, то идентификаторы не должны выглядеть белыми воронами — если только не требуется запускать программу и в среде Python 2. Но если проект нацелен на привлечение соавторов со всего мира, то идентификаторы должны быть английскими словами, и тогда набора символов ASCII вполне достаточно.

Далее мы рассмотрели теорию и практику распознавания кодировки в отсутствие метаданных; теоретически это невозможно, но на практике пакет Chardet неплохо справляется с этой задачей для многих популярных кодировок. Мы сказали о том, что маркеры порядка байтов — единственная информация о кодировке, присутствующая в файлах с кодировкой UTF-16 и UTF-32, иногда также UTF-8.

В следующем разделе мы продемонстрировали открытие текстовых файлов. В этой несложной задаче есть один подвох: именованный аргумент encoding= необязателен, хотя должен быть таковым. Если кодировка не задана, то получается программа, которая генерирует «простой текст», не совместимый с разными платформами из-за несовпадения кодировок по умолчанию. Затем мы рассказали о различных параметрах, которые интерпретатор Python использует в качестве источников умолчаний: locale.getpreferredencoding(), sys.getfilesystemencoding(), sys.getdefaultencoding(), а также о кодировках стандартных потоков ввода-вывода (например, sys.stdout.encoding). Печальным фактом для пользователей Windows является то, что эти параметры зачастую имеют разные значения на одной и той же машине, причем эти значения несовместимы между собой. Напротив, пользователи GNU/Linux и OS X обитают в счастливом мире, где практически повсюду по умолчанию используется кодировка UTF-8.

Сравнение текстов оказывается на удивление сложным делом, потому что в Unicode некоторые символы можно представить несколькими способами, поэтому перед сравнением необходимо выполнить нормализацию. Мы не только объяснили, что такое нормализация и сворачивание регистра, но и привели несколько служебных функций, которые вы можете приспособить к своим нуждам, и среди них функцию, которая полностью удаляет все акценты. Далее мы видели, как правильно сортировать текст Unicode с применением стандартного модуля locale (у которого есть некоторые недостатки) или альтернативного ему внешнего пакета PyUCA, не зависящего от головоломных настроек локали.

Наконец, мы познакомились с базой данных Unicode (источником метаданных о каждом символе) и завершили обсуждение рассмотрением двухрежимных API (реализованных, в частности, в модулях re и os, некоторые функции которых можно вызывать с аргументами типа str или bytes, что приводит к различным, но осмысленным результатам).