---
source_image: page_121.png
page_number: 121
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 22.41
tokens: 6615
characters: 1475
timestamp: 2025-12-24T02:12:19.923190
finish_reason: stop
---

от знака равенства используются значения из следующего слоя (узел k), а во второй — из предыдущего слоя (узел j).

Возможно, глядя на приведенную выше формулу, вы заметили, что горизонтальная матрица, представленная одной строкой, — это транспонированная матрица сигналов o_j на выходе предыдущего слоя. Цветовое выделение элементов матриц поможет вам понять, что скалярное произведение матриц отлично работает и в этом случае.

Используя символическую запись матриц, мы можем привести эту формулу к следующему виду, хорошо приспособленному для реализации в программном коде на языке, обеспечивающем эффективную работу с матрицами.

\[
\Delta W_{jk} = \alpha \cdot E_k \cdot O_k (1 - O_k) \cdot O_j^T
\]

Фактически это выражение совсем не сложное. Сигмоиды исчезли из поля зрения, поскольку они скрыты в матрицах выходных сигналов o_k узлов.

Вот и все! Работа сделана.

Резюме

• Ошибка нейронной сети является функцией весов внутренних связей.
• Улучшение нейронной сети означает уменьшение этой ошибки посредством изменения указанных весов.
• Непосредственный подбор подходящих весов наталкивается на значительные трудности. Альтернативный подход заключается в итеративном улучшении весовых коэффициентов путем уменьшения функции ошибки небольшими шагами. Каждый шаг совершается в направлении скорейшего спуска из текущей позиции. Этот подход называется градиентным спуском.
• Градиент ошибки можно без особых трудностей рассчитать, используя дифференциальное исчисление.