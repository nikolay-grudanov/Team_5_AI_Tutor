---
source_image: page_173.png
page_number: 173
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 25.65
tokens: 6647
characters: 1591
timestamp: 2025-12-24T02:13:34.142848
finish_reason: stop
---

Для весов связей между входным и скрытым слоями мы используем только что рассчитанную переменную hidden_errors.

Ранее нами было получено выражение для обновления веса связи между узлом j и узлом k следующего слоя в матричной форме.

\[
\Delta W_{jk} = \alpha * E_k * \text{сигмоида}(O_k) * (1 - \text{сигмоида}(O_k)) \cdot O_j^T
\]

Величина \( \alpha \) — это коэффициент обучения, а сигмоида — это функция активации, с которой вы уже знакомы. Вспомните, что символ "*" означает обычное поэлементное умножение, а символ "." — скалярное произведение матриц. Последний член выражения — это транспонированная (\( ^T \)) матрица исходящих сигналов предыдущего слоя. В данном случае транспонирование означает преобразование столбца выходных сигналов в строку.

Это выражение легко транслируется в код на языке Python. Сначала запишем код для обновления весов связей между скрытым и выходным слоями.

# обновить весовые коэффициенты связей между скрытым и выходным слоями
self.who += self.lr * numpy.dot((output_errors * final_outputs * (1.0 - final_outputs)), numpy.transpose(hidden_outputs))

Это довольно длинная строка кода, но цветовое выделение поможет вам разобраться в том, как она связана с приведенным выше математическим выражением. Коэффициент обучения self.lr просто умножается на остальную часть выражения. Есть еще матричное умножение, выполняемое с помощью функции numpy.dot(), и два элемента, выделенных синим и красным цветами, которые отображают части, относящиеся к ошибке и сигмоидам из следующего слоя, а также транспонированная матрица исходящих сигналов предыдущего слоя.