---
source_image: page_042.png
page_number: 42
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 15.24
tokens: 6455
characters: 1019
timestamp: 2025-12-24T02:10:28.450361
finish_reason: stop
---

Но погодите! Что произошло? Глядя на график, мы видим, что нам не удалось добиться того наклона прямой, которого мы хотели. Она не обеспечивает достаточно надежное разделение областей диаграммы, занимаемых точками данных божьих коровок и гусениц.

Ну что тут сказать? Мы получили то, что просили. Линия обновляется, подстраиваясь под то целевое значение y, которое мы задаем.

Что-то здесь не так? А ведь действительно, если мы будем продолжать так и далее, т.е. просто обновлять наклон для очередного примера тренировочных данных, то все, что мы будем каждый раз получать в конечном счете, — это линию, проходящую вблизи точки данных последнего тренировочного примера. В результате этого мы отбрасываем весь предыдущий опыт обучения, который могли бы использовать, и учимся лишь на самом последнем примере.

Как исправить эту ситуацию?

Легко! И эта идея играет ключевую роль в машинном обучении. Мы сглаживаем обновления, т.е. немного уменьшаем величину поправок. Вместо того чтобы каждый раз с энтузиазмом заменять A