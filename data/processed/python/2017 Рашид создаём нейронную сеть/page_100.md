---
source_image: page_100.png
page_number: 100
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 22.30
tokens: 6608
characters: 1686
timestamp: 2025-12-24T02:11:50.937404
finish_reason: stop
---

ошибка_{скрытый} = W^T_{скрытый_выходной} \cdot ошибка_{выходной}

Конечно, это просто замечательно, но правильно ли мы поступили, отбросив нормирующий множитель? Оказывается, что эта упрощенная модель обратного распространения сигналов ошибок работает ничуть не хуже, чем более сложная, которую мы разработали перед этим. В блоге, посвященном данной книге, вы найдете публикацию, в которой представлены результаты расчетов обратного распространения ошибки, выполненных несколькими различными способами:

http://makeyourownneuralnetwork.blogspot.co.uk/2016/07/error-backpropagation-revisted.html

Если наш простой подход действительно хорошо работает, мы оставим его!

Немного подумав, можно прийти к выводу, что даже в тех случаях, когда в обратном направлении распространяются слишком большие или слишком малые ошибки, сеть сама все исправит при выполнении последующих итераций обучения. Важно то, что при обратном распространении ошибок учитываются весовые коэффициенты связей, и это наилучший показатель того, что мы пытаемся справедливо распределить ответственность за возникающие ошибки.

Мы проделали большую работу, очень большую!

Резюме

• Обратное распространение ошибок можно описать с помощью матричного умножения.
• Это позволяет нам записывать выражения в более компактной форме, независимо от размеров нейронной сети, и обеспечивает более эффективное и быстрое выполнение вычислений компьютерами, если в языке программирования предусмотрен синтаксис матричных операций.
• Отсюда следует, что использование матриц обеспечивает повышение эффективности расчетов как для распространения сигналов в прямом направлении, так и для распространения ошибок в обратном направлении.