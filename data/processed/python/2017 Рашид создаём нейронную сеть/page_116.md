---
source_image: page_116.png
page_number: 116
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 24.53
tokens: 6632
characters: 1531
timestamp: 2025-12-24T02:12:15.522644
finish_reason: stop
---

Теперь мы можем разделаться с каждой из этих частей по отдельности. С первой частью мы справимся легко, поскольку для этого нужно всего лишь взять простую производную от квадратичной функции. В результате получаем:

\[
\frac{\partial E}{\partial w_{jk}} = -2(t_k - o_k) \cdot \frac{\partial o_k}{\partial w_{jk}}
\]

Со второй частью придется немного повозиться, но и это не вызовет больших затруднений. Здесь \( o_k \) — это выходной сигнал узла \( k \), который, как вы помните, получается в результате применения сигмоиды к сигналам, поступающим на данный узел. Для большей ясности запишем это в явном виде:

\[
\frac{\partial E}{\partial w_{jk}} = -2(t_k - o_k) \cdot \frac{\partial \text{сигмоида} (\sum_j w_{jk} \cdot o_j)}{\partial w_{jk}}
\]

Здесь \( o_j \) — выходной сигнал узла предыдущего скрытого слоя, а не выходной сигнал узла последнего слоя.

Как продифференцировать сигмоиду? Мы могли бы это сделать самостоятельно, проведя сложные и трудоемкие вычисления в соответствии с изложенными в приложении А фундаментальными идеями, однако эта работа уже проделана другими людьми. Поэтому мы просто воспользуемся уже известным ответом, как это ежедневно делают математики по всему миру.

\[
\frac{\partial \text{сигмоида} (x)}{\partial x} = \text{сигмоида} (x) (1 - \text{сигмоида} (x))
\]

Дифференцирование некоторых функций приводит к выражениям устрашающего вида. В случае же сигмоиды результат получается очень простым. Это одна из причин широкого применения сигмоиды в качестве функции активации в нейронных сетях.