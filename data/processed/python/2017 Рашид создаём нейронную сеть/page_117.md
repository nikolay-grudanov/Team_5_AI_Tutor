---
source_image: page_117.png
page_number: 117
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 32.61
tokens: 6755
characters: 1793
timestamp: 2025-12-24T02:12:23.987321
finish_reason: stop
---

Используя этот впечатляющий результат, получаем следующее выражение:

\[
\frac{\partial E}{\partial w_{jk}} = -2(t_k - o_k) \cdot \text{сигмоида} \left( \sum_j w_{jk} \cdot o_j \right) (1 - \text{сигмоида} \left( \sum_j w_{jk} \cdot o_j \right)) \cdot \frac{\partial}{\partial w_{jk}} \left( \sum_j w_{jk} \cdot o_j \right)
\]

\[
= -2(t_k - o_k) \cdot \text{сигмоида} \left( \sum_j w_{jk} \cdot o_j \right) (1 - \text{сигмоида} \left( \sum_j w_{jk} \cdot o_j \right)) \cdot o_j
\]

А откуда взялся последний сомножитель? Это результат применения цепного правила к производной сигмоиды, поскольку выражение под знаком функции сигмоида () также должно быть проинтегрировано по переменной \( w_{jk} \). Это делается очень просто и дает в результате \( o_j \).

Прежде чем записать окончательный ответ, избавимся от множителя 2 в начале выражения. Мы вправе это сделать, поскольку нас интересует только направление градиента функции ошибки, так что этот множитель можно безболезненно отбросить. Нам совершенно безразлично, какой множитель будет стоять в начале этого выражения, 2, 3 или даже 100, коль скоро мы всегда будем его игнорировать. Поэтому для простоты избавимся от него.

Окончательное выражение, которое мы будем использовать для изменения веса \( w_{jk} \), выглядит так.

\[
\frac{\partial E}{\partial w_{jk}} = - (t_k - o_k) \cdot \text{сигмоида} \left( \sum_j w_{jk} \cdot o_j \right) (1 - \text{сигмоида} \left( \sum_j w_{jk} \cdot o_j \right)) \cdot o_j
\]

Ух ты! У нас все получилось!
Это и есть то магическое выражение, которое мы искали. Оно является ключом к тренировке нейронных сетей.
Проанализируем вкратце это выражение, отдельные части которого выделены цветом. Первая часть, с которой вы уже хорошо знакомы, — это ошибка (целевое значение минус фактическое значение).