---
source_image: page_092.png
page_number: 92
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 19.91
tokens: 6508
characters: 1138
timestamp: 2025-12-24T02:11:39.311855
finish_reason: stop
---

Обратное распространение ошибок при большом количестве слоев

На следующей диаграмме представлен пример простой нейронной сети с тремя слоями: входным, скрытым и выходным.

![Диаграмма нейронной сети с тремя слоями](https://i.imgur.com/3Q5z5QG.png)

Продвигаясь в обратном направлении от последнего, выходного слоя (краиного справа), мы видим, как информация об ошибке в выходном слое используется для определения величины поправок к весовым коэффициентам связей, со стороны которых к нему поступают сигналы. Здесь использованы более общие обозначения \( e_{выходной} \) для выходных ошибок и \( w_{св} \) для весов связей между скрытым и выходным слоями. Мы вычисляем конкретные ошибки, ассоциируемые с каждой связью, путем распределения ошибки пропорционально соответствующим весам.

Графическое представление позволяет лучше понять, какие вычисления следует выполнить для дополнительного слоя. Мы просто берем ошибки \( e_{скрытый} \) на выходе скрытого слоя и вновь распределяем их по предшествующим связям между входным и скрытым слоями пропорционально весовым коэффициентам \( w_{bc} \). Следующая диаграмма иллюстрирует эту логику.