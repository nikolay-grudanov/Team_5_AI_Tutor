---
source_image: page_120.png
page_number: 120
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 32.28
tokens: 6774
characters: 1782
timestamp: 2025-12-24T02:12:29.746360
finish_reason: stop
---

Обновленный вес \( w_{jk} \) — это старый вес с учетом отрицательной поправки, величина которой пропорциональна производной функции ошибки. Поправка записана со знаком "минус", поскольку мы хотим, чтобы вес увеличивался при отрицательной производной и уменьшался при положительной, о чем ранее уже говорилось. Символ \( \alpha \) (альфа) — это множитель, сглаживающий величину изменений во избежание перескоков через минимум. Этот коэффициент часто называют коэффициентом обучения.

Данное выражение применяется к весовым коэффициентам связей не только между скрытым и выходным, но и между входным и скрытым слоями. Эти два случая различаются градиентами функции ошибки, выражения для которых приводились выше.

Прежде чем закончить с этим примером, посмотрим, как будут выглядеть те же вычисления в матричной записи. Для этого сделаем то, что уже делали раньше, — запишем, что собой представляет каждый элемент матрицы изменений весов.

\[
\begin{pmatrix}
\Delta w_{1,1} & \Delta w_{2,1} & \Delta w_{3,1} & \ldots \\
\Delta w_{1,2} & \Delta w_{2,2} & \Delta w_{3,2} & \ldots \\
\Delta w_{1,3} & \Delta w_{2,3} & \Delta w_{j,k} & \ldots \\
\ldots & \ldots & \ldots & \ldots
\end{pmatrix}
=
\begin{pmatrix}
E_1 * S_1 (1 - S_1) \\
E_2 * S_2 (1 - S_2) \\
E_k * S_k (1 - S_k) \\
\ldots
\end{pmatrix}
\cdot
\begin{pmatrix}
o_1 & o_2 & o_j & \ldots
\end{pmatrix}
\]

значения из следующего слоя

значения из предыдущего слоя

Я опустил коэффициент обучения \( \alpha \), поскольку это всего лишь константа, которая никак не влияет на то, как мы организуем матричное умножение.

Матрица изменений весов содержит значения поправок к весовым коэффициентам \( w_{jk} \) для связей между узлом \( j \) одного слоя и узлом \( k \) следующего слоя. Вы видите, что в первой части выражения справа