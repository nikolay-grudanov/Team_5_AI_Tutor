---
source_image: page_095.png
page_number: 95
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 22.28
tokens: 6550
characters: 1132
timestamp: 2025-12-24T02:11:41.955074
finish_reason: stop
---

этого узла в прямом направлении. На приведенной выше диаграмме показано, что имеется некоторая доля выходной ошибки \( e_{выходной,1} \), приписываемая связи с весом \( w_{11} \), и некоторая доля выходной ошибки \( e_{выходной,2} \), приписываемая связи с весом \( w_{12} \).

Вышесказанное можно записать в виде следующего выражения:

\[
e_{скрытый,1} = сумма\ ошибок,\ распределенных\ по\ связям\ W_{11}\ и\ W_{12}
\]
\[
= e_{выходной,1} * \frac{W_{11}}{W_{11} + W_{21}} + e_{выходной,2} * \frac{W_{12}}{W_{12} + W_{22}}
\]

Чтобы проиллюстрировать, как эта теория выглядит на практике, приведем диаграмму, демонстрирующую обратное распространение ошибок в простой трехслойной сети на примере конкретных данных.

![Диаграмма обратного распространения ошибок в трехслойной нейронной сети](https://i.imgur.com/3Q5z5QG.png)

Проследим за обратным распространением одной из ошибок. Вы видите, что после распределения ошибки 0,5 на втором узле выходного слоя между двумя связями с весами 1,0 и 4,0 мы получаем доли, равные 0,1 и 0,4 соответственно. Также можно видеть, что объединенная ошибка на втором узле скрытого слоя представляет