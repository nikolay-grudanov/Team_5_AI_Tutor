---
source_image: page_119.png
page_number: 119
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 28.99
tokens: 6705
characters: 1783
timestamp: 2025-12-24T02:12:24.396580
finish_reason: stop
---

• Последняя часть выражения приобретает смысл выходных сигналов \( o_i \) узлов первого слоя, и в данном случае эти сигналы являются входными.

Тем самым нам удалось изящно избежать излишних трудоемких вычислений, в полной мере воспользовавшись всеми преимуществами симметрии задачи для конструирования нового выражения. Несмотря на всю ее простоту, это очень мощная методика, взятая на вооружение выдающимися математиками и учеными. Овладев ею, вы, несомненно, произведете на коллег большое впечатление!

Итак, вторая часть окончательного ответа, к получению которого мы стремимся (градиент функции ошибки по весовым коэффициентам связей между входным и скрытым слоями), приобретает следующий вид:

\[
\frac{\partial E}{\partial w_{ij}} = - (e_j) \cdot \text{sigmoída} \left( \sum_i w_{ij} \cdot o_i \right) (1 - \text{sigmoída} \left( \sum_i w_{ij} \cdot o_i \right)) \cdot o_i
\]

На данном этапе нами получены все ключевые магические выражения, необходимые для вычисления искомого градиента, который мы используем для обновления весовых коэффициентов по результатам обучения на каждом тренировочном примере, чем мы сейчас и займемся.

Не забывайте о том, что направление изменения коэффициентов противоположно направлению градиента, что неоднократно демонстрировалось на предыдущих диаграммах. Кроме того, мы сглаживаем интересующие нас изменения параметров посредством коэффициента обучения, который можно настраивать с учетом особенностей конкретной задачи. С этим подходом вы также уже сталкивались, когда при разработке линейных классификаторов мы использовали его для уменьшения негативного влияния неудачных примеров на эффективность обучения, а при минимизации функции ошибки — для того, чтобы избежать постоянных перескоков через минимум. Выразим это на языке математики: