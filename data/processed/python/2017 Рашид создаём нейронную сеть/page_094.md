---
source_image: page_094.png
page_number: 94
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 26.21
tokens: 6627
characters: 1562
timestamp: 2025-12-24T02:11:45.844191
finish_reason: stop
---

Это означает, что с каждой из двух связей, исходящих из узла промежуточного слоя, ассоциируется некоторая ошибка. Мы могли бы воссоединить ошибки этих двух связей, чтобы получить ошибку для этого узла в качестве второго наилучшего подхода, поскольку мы не располагаем фактическим целевым значением для узла промежуточного слоя. Следующая диаграмма иллюстрирует эту идею:

![Диаграмма нейронной сети с двумя входными узлами, двумя скрытыми узлами и двумя выходными узлами, с обозначениями ошибок и весов](../images/chapter_5/figure_5_1.png)

На диаграмме отчетливо видно, что именно происходит, однако для уверенности давайте разберем ее более детально. Нам необходимы величины ошибок для узлов скрытого слоя, чтобы использовать их для обновления весовых коэффициентов связей с предыдущим слоем. Обозначим эти ошибки как \( e_{скрытый} \). Но у нас нет очевидного ответа на вопрос о том, какова их величина. Мы не можем сказать, что ошибка — это разность между желаемым или целевым выходным значением этого узла и его фактическим выходным значением, поскольку данные тренировочного примера предоставляют лишь целевые значения для узлов последнего, выходного слоя. Они не говорят абсолютно ничего о том, какими должны быть выходные сигналы узлов любого другого слоя. В этом и заключается суть сложившейся головоломки.

Мы можем воссоединить ошибки, распределенные по связям, используя обратное распространение ошибок, с которым вы уже познакомились. Поэтому ошибка на первом скрытом узле представляет собой сумму ошибок, распределенных по всем связям, исходящим из