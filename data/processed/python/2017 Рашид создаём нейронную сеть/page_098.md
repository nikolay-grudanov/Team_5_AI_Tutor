---
source_image: page_098.png
page_number: 98
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 29.57
tokens: 6749
characters: 1701
timestamp: 2025-12-24T02:11:54.857910
finish_reason: stop
---

e_1 * w_{21} / (w_{21} + w_{11}) и e_2 * w_{22} / (w_{22} + w_{12}). Ранее мы уже видели, как работают эти выражения.

Итак, для скрытого слоя мы имеем следующую матрицу, которая выглядит немного сложнее, чем мне хотелось бы.

\[
\text{ошибка}_{\text{скрытый}} = \begin{pmatrix}
\frac{w_{11}}{w_{11} + w_{21}} & \frac{w_{12}}{w_{12} + w_{22}} \\
\frac{w_{21}}{w_{21} + w_{11}} & \frac{w_{22}}{w_{22} + w_{12}}
\end{pmatrix} \cdot \begin{pmatrix} e_1 \\ e_2 \end{pmatrix}
\]

Было бы здорово, если бы это выражение можно было переписать в виде простого перемножения матриц, которыми мы уже располагаем. Это матрицы весовых коэффициентов, прямого сигнала и выходных ошибок. Преимущества, которые можем при этом получить, огромны.

К сожалению, легкого способа превратить это выражение в сверхпростое перемножение матриц, как в случае распространения сигналов в прямом направлении, не существует. Распутать все эти доли, из которых образованы элементы большой матрицы, непросто. Было бы замечательно, если бы мы смогли представить эту матрицу в виде комбинации имеющихся матриц.

Что можно сделать? Нам позарез нужен способ, обеспечивающий возможность использования матричного умножения, чтобы повысить эффективность вычислений.

Ну что ж, дерзнем!

Взгляните еще раз на приведенное выше выражение. Вы видите, что наиболее важная для нас вещь — это умножение выходных ошибок \( e_n \) на связанные с ними веса \( w_{ij} \). Чем больше вес, тем большая доля ошибки передается обратно в скрытый слой. Это важный момент. В дробях, являющихся элементами матрицы, нижняя часть играет роль нормирующего множителя. Если пренебречь этим фактором, можно потерять лишь масштабирование ошибок, передаваемых по меха-