---
source_image: page_172.png
page_number: 172
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 25.00
tokens: 6676
characters: 1726
timestamp: 2025-12-24T02:13:30.919200
finish_reason: stop
---

Список targets_list преобразуется в массив точно так же, как список input_list:

targets = numpy.array(targets_list, ndmin=2).T

Теперь мы очень близки к решению основной задачи тренировки сети — уточнению весов на основе расхождения между расчетными и целевыми значениями.

Будем решать эту задачу поэтапно.
Прежде всего, мы должны вычислить ошибку, являющуюся разностью между желаемым целевым выходным значением, предоставленным тренировочным примером, и фактическим выходным значением. Она представляет собой разность между матрицами (targets - final_outputs), рассчитываемую поэлементно. Соответствующий код выглядит очень просто, что еще раз подтверждает мощь и красоту матричного подхода.

# ошибка = целевое значение - фактическое значение
output_errors = targets - final_outputs

Далее мы должны рассчитать обратное распространение ошибок для узлов скрытого слоя. Вспомните, как мы распределяли ошибки между узлами пропорционально весовым коэффициентам связей, а затем рекомбинировали их на каждом узле скрытого слоя. Эти вычисления можно представить в следующей матричной форме:

\[
\text{ошибки}_{\text{скрытый}} = \text{веса}^T_{\text{скрытый_выходной}} \cdot \text{ошибки}_{\text{выходной}}
\]

Код, реализующий эту формулу, также прост в силу способности Python вычислять скалярные произведения матриц с помощью модуля numpy.

# ошибки скрытого слоя - это ошибки output_errors,
# распределенные пропорционально весовым коэффициентам связей
# и рекомбинированные на скрытых узлах
hidden_errors = numpy.dot(self.who.T, output_errors)

Итак, мы получили то, что нам необходимо для уточнения весовых коэффициентов в каждом слое. Для весов связей между скрытым и выходным слоями мы используем переменную output_errors.