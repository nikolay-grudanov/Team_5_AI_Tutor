---
source_image: page_252.png
page_number: 252
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 27.30
tokens: 6558
characters: 1170
timestamp: 2025-12-24T02:15:18.303575
finish_reason: stop
---

Поэтому 5 в \(2t^5\) используется в качестве дополнительного коэффициента перед уменьшением степени на единицу: \(5*2t^4 = 10t^4\).

Приведенная ниже формула суммирует все, что было сказано о дифференцировании степеней, в виде следующего правила:

\[
y = ax^n \implies \frac{\delta y}{\delta n} = nax^{n-1}
\]

Испытаем эту формулу на дополнительных примерах только ради того, чтобы набить руку в использовании этого нового приема:

\[
s = t^5 \implies \frac{\delta s}{\delta t} = 5t^4
\]

\[
s = 6t^6 + 9t + 4 \implies \frac{\delta s}{\delta t} = 36t^5 + 9
\]

\[
s = t^3 + c \implies \frac{\delta s}{\delta t} = 3t^2
\]

Это правило пригодится вам во многих случаях, а зачастую ничего другого вам и не потребуется. Верно и то, что правило применимо только к полиномам, т.е. к выражениям, состоящим из переменных в различных степенях, как, например, выражение \(y = ax^3 + bx^2 + cx + d\), но не к функциям вида \(sin(x)\) или \(cos(x)\). Это не является существенным недостатком, поскольку в огромном количестве случаев вам вполне хватит правила дифференцирования степеней.

Однако для нейронных сетей нам понадобится еще один инструмент, о котором сейчас пойдет речь.