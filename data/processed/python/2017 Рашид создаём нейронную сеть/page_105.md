---
source_image: page_105.png
page_number: 105
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 21.14
tokens: 6590
characters: 1431
timestamp: 2025-12-24T02:11:57.475257
finish_reason: stop
---

находить минимум, даже не располагая знаниями свойств этой функции, достаточными для нахождения минимума математическими методами. Если функция настолько сложна, что простого способа нахождения минимума алгебраическими методами не существует, то мы можем вместо этого применить метод градиентного спуска. Ясное дело, он может не дать нам точный ответ, поскольку мы приближаемся к ответу шаг за шагом, постепенно улучшая нашу позицию. Но это лучше, чем вообще не иметь никакого ответа. Во всяком случае, мы можем продолжить уточнение ответа еще более мелкими шагами по направлению к минимуму, пока не достигнем желаемой точности.

А какое отношение имеет этот действительно эффективный метод градиентного спуска к нейронным сетям? Если упомянутой сложной функцией является ошибка сети, то спуск по склону для нахождения минимума означает, что мы минимизируем ошибку. Мы улучшаем выходной сигнал сети. Это именно то, чего мы хотим!

Чтобы вы все наглядно усвоили, рассмотрим использование метода градиентного спуска на простейшем примере.

Ниже приведен график простой функции \( y = (x - 1)^2 + 1 \). Если бы это была функция, описывающая ошибку, то мы должны были бы найти значение \( x \), которое минимизирует эту функцию. Представим на минуту, что мы имеем дело не со столь простой функцией, а с гораздо более сложной.

![График функции y = (x-1)^2+1 с отмеченными точками и направлением спуска](https://i.imgur.com/3Q5z5QG.png)