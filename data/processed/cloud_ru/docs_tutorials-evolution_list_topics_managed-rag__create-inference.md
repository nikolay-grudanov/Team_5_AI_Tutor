---
source_image: docs_tutorials-evolution_list_topics_managed-rag__create-inference.jpg
page_number: 0
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 143.66
tokens: 15574
characters: 10941
timestamp: 2025-12-24T06:00:13.574324
finish_reason: stop
---

<h2>Создание инференса для использования в Managed RAG</h2>
<p>С помощью этого руководства вы последовательно создадите три типа инференса в ML Inference для использования их в базе знаний Managed RAG, затем проверите работоспособность базы знаний.</p>
<p>Вы будете использовать следующие сервисы:</p>
<ul>
<li><b>Evolution Managed RAG</b> — сервис для создания и управления базами знаний, используемыми при генерации ответов языковыми моделями.</li>
<li><b>Evolution Object Storage</b> — объективное хранилище для размещения документов, из которых будет формироваться база знаний.</li>
<li><b>Evolution ML Inference</b> — сервис для запуска ML-моделей в облаке.</li>
<li><b>Huggingface</b> — платформа для публикации и использования моделей машинного обучения.</li>
</ul>
<p>Шаги:</p>
<ol>
<li><b>Создайте бакет и загрузите файл.</b></li>
<li><b>Получите токен Huggingface.</b></li>
<li><b>Создайте инференс для модели-эмбеддера.</b></li>
<li><b>Создайте инференс для модели-реранкера.</b></li>
<li><b>Создайте инференс для LLM.</b></li>
<li><b>Создайте базу знаний.</b></li>
<li><b>Проверьте работу базу знаний.</b></li>
</ol>
<h3>Перед началом работы</h3>
<ol>
<li>Зарегистрируйтесь в личном кабинете Cloud.ru.<br>Если вы уже зарегистрированы, войдите под своей учетной записью.</li>
<li>Убедитесь, что в личном кабинете Cloud.ru подключены сервисы <b>Managed RAG, ML Inference, Object Storage</b>.</li>
<li>Скачайте текстовый файл <code>faq_products.txt</code>.</li>
</ol>
<h3>1. Создайте бакет и загрузите файл</h3>
<ol>
<li><b>Создайте бакет в Object Storage:</b>
<ol type="a">
<li>Укажите название бакета, например <code>rag-inference-bucket</code>. Остальные параметры оставьте по умолчанию.</li>
<li>Нажмите Создать.</li>
</ol>
</li>
<li><b>Создайте папку в бакете</b> со следующими параметрами:
<ol type="a">
<li>Перейдите в бакет <code>rag-inference-bucket</code>.</li>
<li>Нажмите Создать папку.</li>
<li>Укажите название <code>rag-inference-kb/</code> и нажмите Создать.</li>
</ol>
</li>
<li><b>Загрузите папку</b> текстовый файл <code>faq_products.txt</code>.</li>
</ol>
<h3>2. Получите токен Huggingface</h3>
<ol>
<li>Войдите или зарегистрируйтесь на <a href="https://huggingface.co">https://huggingface.co</a>.</li>
<li>Перейдите в раздел <b>Access Tokens</b>.</li>
</ol>
<p><img src="https://cloud.ru/docs/images/access_tokens.png" alt="Access Tokens"></p>
<ol start="3">
<li>Нажмите <b>Create new token</b>.</li>
<li>Выберите тип <b>Write</b>.</li>
<li>Введите название токена, например <code>rag_with_mlinference</code>.</li>
<li>Нажмите <b>Create token</b>.</li>
<li>Скопируйте токен и сохраните его, например в блокнот. После закрытия страницы он будет недоступен.</li>
</ol>
<h3>3. Создайте инференс для модели-эмбеддера</h3>
<p>Инференс создается на примере модели с Huggingface <b>Qwen/Qwen3-Embedding-0.6B</b>.</p>
<ol>
<li>Перейдите в личный кабинет Cloud.ru, AI Factory → ML Inference.</li>
<li>На вкладке Model RUN нажмите Создать.</li>
<li>Укажите название <code>embedder-for-rag</code>.</li>
<li>Выберите для Runtime значение <code>vLLM</code>.</li>
<li>Добывьте модель.
<ol type="a">
<li>Нажмите Добавить из Hugging Face.</li>
<li>В поле Репозиторий с моделью Hugging Face вставьте скопированное название модели <code>Qwen/Qwen3-Embedding-0.6B</code>.</li>
<li>Нажмите Добавить токен в Secret Management, если токен еще не добавлен.</li>
<li>Укажите путь, например <code>rag_with_mlinference</code>.</li>
<li>Введите описание, например <code>Huggingface access token</code>.</li>
<li>В поле Значение секрета выберите Стандартный режим и вставьте токен Huggingface, полученный на шаге 2.</li>
<li>Нажмите Создать.</li>
</ol>
<p>Токен сохранен в Secret Management. Вернитесь к созданию инференса.</p>
</li>
<li>В поле Токен доступа в Hugging Face выберите созданный токен <code>rag_with_mlinference→ версия 1</code>.</li>
<li>Нажмите Добавить.</li>
<li>Дождитесь расчета ресурсов.</li>
<li>В поле Задача ML модели выберите Embedding — отличительная черта инференса такого типа.</li>
<li>Остальные параметры оставьте по умолчанию и нажмите Продолжить.</li>
<li>Включите опцию Невыключать модель.</li>
<li>(Опционально) Настройте масштабирование.</li>
<li>(Опционально) В настройке Аутентификация выберите сервисный аккаунт.</li>
<li>(Опционально) В настройке Логирование укажите лог-группу.</li>
<li>Нажмите Создать.</li>
<li>Дождитесь, когда инференс перейдет в статус «Запущен».</li>
<li>Перейдите на вкладку Информация и скопируйте идентификатор инференса — часть публичного URL между <code>https://</code> и <code>.modelrun</code>.</li>
</ol>
<p>Например, в публичном URL <code>https://12345c60-xxx-4527-xxxx-f789f789fb11.modelrun.inference.cloud.ru</code> нужный идентификатор — <code>12345c60-xxx-4527-xxxx-f789f789fb11</code>.</p>
<h3>4. Создайте инференс для модели-реранкера</h3>
<p>Инференс создается на примере модели с Huggingface <b>Qwen/Qwen3-Reranker-0.6B</b>.</p>
<ol>
<li>Перейдите в личный кабинет Cloud.ru, AI Factory → ML Inference.</li>
<li>На вкладке Model RUN нажмите Создать.</li>
<li>Укажите название <code>reranker-for-rag</code>.</li>
<li>Выберите для Runtime значение <code>vLLM</code>.</li>
<li>Добывьте модель.
<ol type="a">
<li>Нажмите Добавить из Hugging Face.</li>
<li>В поле Репозиторий с моделью Hugging Face вставьте скопированное название модели <code>Qwen/Qwen3-Reranker-0.6B</code>.</li>
<li>Нажмите Добавить токен в Secret management, если токен еще не добавлен.</li>
<li>Укажите путь, например <code>rag_with_mlinference</code>.</li>
<li>Введите описание.</li>
<li>В поле Значение секрета выберите Стандартный режим и вставьте токен Huggingface, полученный на шаге 2.</li>
<li>Нажмите Создать.</li>
</ol>
<p>Токен сохранен в Secret Management. Вернитесь к созданию инференса.</p>
</li>
<li>В поле Токен доступа в Hugging Face выберите созданный токен <code>rag_with_mlinference→ версия 1</code>.</li>
<li>Нажмите Добавить.</li>
<li>Дождитесь расчета ресурсов.</li>
<li>В поле Задача ML модели выберите Score — отличительная черта инференса такого типа.</li>
<li>Остальные параметры оставьте по умолчанию и нажмите Продолжить.</li>
<li>Включите опцию Невыключать модель.</li>
<li>(Опционально) Настройте масштабирование.</li>
<li>(Опционально) В настройке Аутентификация выберите сервисный аккаунт.</li>
<li>(Опционально) В настройке Логирование укажите лог-группу.</li>
<li>Нажмите Создать.</li>
<li>Дождитесь, когда инференс перейдет в статус «Запущен».</li>
<li>Перейдите на вкладку Информация и скопируйте идентификатор инференса — часть публичного URL между <code>https://</code> и <code>.modelrun</code>.</li>
</ol>
<p>Например, в публичном URL <code>https://12345c60-xxx-4527-xxxx-f789f789fb11.modelrun.inference.cloud.ru</code> нужный идентификатор — <code>12345c60-xxx-4527-xxxx-f789f789fb11</code>.</p>
<h3>5. Создайте инференс для LLM</h3>
<p>Инференс создается на примере модели с Huggingface <b>t-tech/T-lite-it-1.0</b>.</p>
<ol>
<li>Перейдите в личный кабинет Cloud.ru, AI Factory → ML Inference.</li>
<li>На вкладке Model RUN нажмите Создать.</li>
<li>Укажите название <code>llm-for-rag</code>.</li>
<li>Выберите для Runtime значение <code>vLLM</code>.</li>
<li>Добывьте модель.
<ol type="a">
<li>Нажмите Добавить из Hugging Face.</li>
<li>В поле Репозиторий с моделью Hugging Face вставьте скопированное название модели <code>t-tech/T-lite-it-1.0</code>.</li>
<li>Нажмите Добавить токен в Secret Management, если токен еще не добавлен.</li>
<li>Укажите путь, например <code>rag_with_mlinference</code>.</li>
<li>Введите описание.</li>
<li>В поле Значение секрета выберите Стандартный режим и вставьте токен Huggingface, полученный на шаге 2.</li>
<li>Нажмите Создать.</li>
</ol>
<p>Токен сохранен в Secret Management. Вернитесь к созданию инференса.</p>
</li>
<li>В поле Токен доступа в Hugging Face выберите созданный токен <code>rag_with_mlinference→ версия 1</code>.</li>
<li>Нажмите Добавить.</li>
<li>Дождитесь расчета ресурсов.</li>
<li>В поле Задача ML модели выберите Generate — отличительная черта инференса такого типа.</li>
<li>Остальные параметры оставьте по умолчанию и нажмите Продолжить.</li>
<li>Включите опцию Невыключать модель.</li>
<li>(Опционально) Настройте масштабирование.</li>
<li>(Опционально) В настройке Аутентификация выберите сервисный аккаунт.</li>
<li>(Опционально) В настройке Логирование укажите лог-группу.</li>
<li>Нажмите Создать.</li>
<li>Дождитесь завершения индексации базы знаний и ее версии — это займет несколько минут.</li>
<li>Перейдите в созданную версию базы знаний.</li>
<li>Скопируйте значения полей ID версии и ID базы знаний.</li>
</ol>
<h3>6. Создайте базу знаний с использованием инференса</h3>
<p>На этом шаге вы создадите базу знаний на основе загруженных документов и проиндексируете ее для использования с языкными моделями.</p>
<ol>
<li>В личном кабинете перейдите в AI Factory → Managed RAG.</li>
<li>Нажмите Создать базу знаний.</li>
<li>В поле Название укажите имя базы знаний, например <code>kb-rag-with-inference</code>.</li>
<li>При необходимости введите описание.</li>
<li>В поле Путь к папке в бакете выберите папку <code>rag-inference-kb</code>, в бакете Object Storage, куда вы загрузили файл <code>faq_products.txt</code>.</li>
<li>В поле Расширение файлов введите <code>.txt</code> и выберите его.</li>
<li>Включите опцию Вручную настроить обработку документов и модель.</li>
<li>(Опционально) В настройке Аутентификация выберите сервисный аккаунт.</li>
<li>(Опционально) В настройке Логирование укажите лог-группу.</li>
<li>Нажмите Продолжить.</li>
<li>Пропустите настройку экстрактора и нажмите Продолжить.</li>
<li>Выберите источник модели ML Inference.</li>
<li>В списке выберите созданный инференс <code>embedder-for-rag</code>.</li>
<li>Нажмите Создать.</li>
<li>Дождитесь завершения индексации базы знаний и ее версии — это займет несколько минут.</li>
<li>Перейдите в созданную версию базы знаний.</li>
<li>Скопируйте значения полей ID версии и ID базы знаний.</li>
</ol>
<h3>7. Проверьте работу базы знаний</h3>
<p>Вы можете дополнительно проверить работу с базой знаний с помощью личного кабинета или API. Рекомендуется использовать оба способа.</p>
<table>
<tr>
<th>Личный кабинет</th>
<th>API</th>
</tr>
<tr>
<td>1. Перейдите в созданную версию базы знаний.<br>2. Перейдите на вкладку Чат.<br>3. Включите опцию Использовать модель-реранкер.<br>4. В качестве источника модели-реранкера выберите ML Inference.<br>5. Выберите созданный инференс <code>reranker-for-rag</code>.<br>6. В качестве Модель-LLM выберите ML Inference и из списка выберите инференс <code>llm-for-rag</code>.<br>7. Отправьте сообщение в чате и получите ответ.</td>
<td></td>
</tr>
</table>
<h3>Что дальше</h3>
<p>С этим руководством вы создали базу знаний на основе нескольких инференсов моделей.</p>
<p>Теперь можно отправлять запросы к инференсу.</p>
<p>Узнавайте больше о прикладных сценариях и примерах решения бизнес-задач, получайте навыки управления облачком, выполняя практические руководства.</p>