---
source_image: page_475.png
page_number: 475
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 21.44
tokens: 7371
characters: 1580
timestamp: 2025-12-24T03:13:12.695945
finish_reason: stop
---

Вывод:

['Pod\n', 'Service\n', 'Volume\n', 'Namespace\n']

На практике это дает возможность обрабатывать большие файлы журналов с помощью выражений-генераторов, не боясь израсходовать всю оперативную память машины.

Конвейер с генератором для чтения и обработки строк

Далее приведен код функции-генератора, которая открывает файл и возвращает генератор:

def process_file_lazily():
    """Отложенная обработка файла с помощью генератора"""

    with open("containers.txt") as file_to_read:
        for line in file_to_read.readlines():
            yield line

Далее мы создаем на основе этого генератора конвейер для построчного выполнения операций. В примере строка переводится в нижний регистр, но здесь можно соединить в цепочку множество других операций, и эффективность будет очень высокой, поскольку достаточно объема памяти, необходимой для обработки отдельной строки:

# Создание объекта-генератора
pipeline = process_file_lazily()
# Преобразование в нижний регистр
lowercase = (line.lower() for line in pipeline)
# Выводим первую обработанную строку
print(next(lowercase))

Результат работы конвейера:

pod

На практике это значит, что размер файлов может быть, по существу, неограниченным, если работа кода завершается при удовлетворении определенного условия. Например, представьте, что нужно найти идентификатор покупателя в терабайтном массиве данных. Конвейер с генератором может работать следующим образом: найти этот идентификатор покупателя и выйти из цикла обработки при первом же найденном вхождении. В мире больших данных это отнюдь не теоретическая проблема.