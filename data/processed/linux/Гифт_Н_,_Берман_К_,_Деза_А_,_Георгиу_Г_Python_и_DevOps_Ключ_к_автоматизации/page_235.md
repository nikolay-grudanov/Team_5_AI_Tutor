---
source_image: page_235.png
page_number: 235
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 25.51
tokens: 7411
characters: 1739
timestamp: 2025-12-24T03:06:59.826262
finish_reason: stop
---

синтаксического разбора. Для такого разбора мы воспользуемся плагином grok. Добавьте в конец того же файла следующую конфигурацию:

filter {
    grok {
        match => { "message" => "%{COMBINEDAPACHELOG}"}
    }
}

В разделе filter теперь описано, как должен использоваться плагин grok, который получает входную строку и применяет обладающий большими возможностями набор регулярных выражений COMBINEDAPACHELOG, с помощью которых можно найти и привязать все компоненты журналов веб-сервера, поступающих из Nginx.

Наконец, в разделе output необходимо указать, куда должны отправляться теперь уже структурированные данные:

output {
    elasticsearch {
        hosts => ["localhost:9200"]
    }
}

Это значит, что все структурированные данные пересылаются в локальный экземпляр Elasticsearch. Как видите, для этого достаточно минимальной конфигурации Logstash (и сервиса Filebeat). Для дальнейшей более точной настройки сбора и синтаксического разбора журналов можно добавить еще несколько плагинов и опций конфигурации. Подобный подход с готовыми инструментами идеален для того, чтобы сразу приступить к работе, не задумываясь о том, какие нужны расширения или плагины. Если интересно, можете заглянуть в исходный код Logstash и найти содержащий COMBINEDAPACHELOG файл grok-patterns — этот набор регулярных выражений поистине впечатляет.

Elasticsearch и Kibana

Чтобы подготовить и запустить систему для получения структурированных данных из Logstash на локальной машине, практически ничего, кроме установки пакета elasticsearch, не требуется. Убедитесь, что сервис запущен и работает без проблем:

$ systemctl start elasticsearch

Аналогичным образом установите пакет kibana и запустите соответствующий сервис:

$ systemctl start kibana