---
source_image: page_479.png
page_number: 479
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 27.63
tokens: 7420
characters: 1647
timestamp: 2025-12-24T03:13:26.510707
finish_reason: stop
---

Файл такого размера лучше подходит для обработки Pandas на ноутбуке, а сам процесс можно в дальнейшем преобразовать в автоматический технологический процесс создания рандомизированных файлов выборок для источников, представляющих собой большие данные. Подобный процесс — лишь один из множества технологических процессов, необходимых для работы с большими данными.

Еще одно определение больших данных было дано Маккинси (McKinsey): «Наборы данных, размер которых слишком велик для ввода, хранения, обработки и анализа стандартными утилитами баз данных». Это определение также вполне логично, с небольшим уточнением: не только утилитами баз данных, а вообще любыми утилитами, работающими с данными. Если прекрасно работающая на ноутбуке утилита, например Pandas, Python, MySQL, утилита глубокого обучения/машинального обучения, Bash и т. д., не может нормально работать из-за большого размера или чрезмерной скорости изменения данных, значит, речь идет о задаче больших данных. Для решения задач больших данных необходимы специальные инструменты, и в следующем разделе мы их и рассмотрим.

Утилиты, компоненты и платформы для работы с большими данными

Обсуждение больших данных можно также разбить на обсуждение отдельных утилит и платформ. На рис. 15.1 приведен типичный жизненный цикл архитектуры больших данных.

Обсудим некоторые важнейшие компоненты этой архитектуры.

Координация

Источники данных
Хранилище данных
Ввод сообщений в режиме реального времени
Файловые системы
Пакетная обработка
Объединенная обработка
Потоковая обработка
Хранилище данных аналитики
Утилита ML/ИИ для аналитических отчетов

Рис. 15.1. Архитектура больших данных