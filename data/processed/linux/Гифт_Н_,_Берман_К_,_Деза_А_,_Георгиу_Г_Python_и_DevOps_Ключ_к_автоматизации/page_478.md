---
source_image: page_478.png
page_number: 478
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 39.52
tokens: 7635
characters: 2383
timestamp: 2025-12-24T03:13:36.120623
finish_reason: stop
---

У термина «большие данные» точно такие же проблемы с контекстом: в зависимости от того, с кем вы говорите, он может значить разное. Задумайтесь над одним из определений. Необходимы ли для обработки данных на ноутбуке те же пакеты программ, что и в среде промышленной эксплуатации?

Прекрасный пример утилиты для малых данных — пакет Pandas. По информации создателя этого пакета, он может использовать в 5–10 раз больше оперативной памяти, чем размер обрабатываемого файла. На практике при 16 Гбайт оперативной памяти в ноутбуке и CSV-файле размером 2 Гбайт задача уже относится к большим данным, поскольку оперативной памяти ноутбука может не хватить для работы с этим файлом. Так что необходимо пересмотреть способ решения задачи. Например, можно открыть лишь фрагмент данных или изначально урезать массив данных.

Рассмотрим конкретный пример этой проблемы и ее обхода. Представьте себе, что вы исследователь данных, который снова и снова получает ошибки нехватки памяти, поскольку берет слишком большие для Pandas файлы. Один из примеров таких файлов — набор данных Open Food Facts (https://oreil.ly/w-tmA) из Kaggle. В разархивированном виде размер этого набора данных составляет более 1 Гбайт. Эта задача как раз входит в число проблемных для Pandas. Один из вариантов ее решения — воспользоваться командой shuf Unix для получения перетасованной выборки из набора данных:

time shuf -n 100000 en.openfoodfacts.org.products.tsv\
    > 10k.sample.en.openfoodfacts.org.products.tsv
1.89s user 0.80s system 97% cpu 2.748 total

Менее чем за 2 секунды нам удалось уменьшить файл до вполне приемлемого размера. Лучше использовать этот подход, так как в нем происходит случайный выбор примеров данных, а не брать просто начало или конец набора данных. Эта проблема играет важную роль в технологическом процессе науки о данных. Кроме того, можно сначала посмотреть на строки файла, чтобы понять, с чем приходится иметь дело:

wc -l en.openfoodfacts.org.products.tsv
356002 en.openfoodfacts.org.products.tsv

Исходный файл включает примерно 350 000 строк, так что 100 000 перетасованных строк составляет примерно треть всего массива данных. В этом можно убедиться, взглянув на преобразованный файл. Его размер составляет 272 Мбайт — около трети от размера исходного файла в 1 Гбайт:

du -sh 10k.sample.en.openfoodfacts.org.products.tsv
272M   10k.sample.en.openfoodfacts.org.products.tsv