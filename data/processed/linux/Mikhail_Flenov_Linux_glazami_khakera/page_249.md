---
source_image: page_249.png
page_number: 249
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 39.67
tokens: 8018
characters: 2713
timestamp: 2025-12-24T04:26:04.991579
finish_reason: stop
---

в запрос различные параметры. Одной из самых мощных является поисковая система Google (www.google.com). В ней реализовано много возможностей, позволяющих сделать поиск более точным. Жаль, что большинство пользователей не освоили их, а вот взломщики изучили все функции и используют в своих целях.

Один из самых простых способов взлома — найти с помощью поисковой системы закрытую веб-страницу. Некоторые сайты имеют засекреченные области, к которым доступ осуществляет по паролю. Сюда же относятся платные ресурсы, где защита основана на проверке пароля при входе, а не на защите каждой страницы и использовании SSL. При этом часто случается, что Google индексирует запрещенные страницы, и их можно просмотреть через поиск. Для этого всего лишь надо четко знать, какая информация хранится в файле, и как можно точнее составить строку поиска.

С помощью Google можно найти достаточно важные данные, которые скрыты от пользователя, но по ошибке администратора стали доступными для индексирующей машины Google — надо только во время поиска правильно задать параметры. Например, можно ввести в строку поиска следующий запрос:

Годовой отчет filetype:doc

Или

Годовой отчет filetype:xls

И вы найдете все документы в формате Word и Excel, содержащие слова "Годовой отчет". Возможно, документов будет слишком много, поэтому запрос придется ужесточить, но кто ищет, тот всегда найдет. Существуют реальные примеры из жизни, когда таким простым способом были найдены секретные данные, в том числе действующие номера кредитных карт и финансовые отчеты фирм.

Мое мнение — очень глупо держать какие-то отчеты на открытых серверах. Уж если Google может добраться до них, то и другие смогут. Но почему-то такие файлы продолжают держать в открытую.

Давайте рассмотрим, как можно запретить индексацию каталогов веб-страниц, которые не должны стать доступными для всеобщего просмотра. Для этого необходимо понимать, что именно индексируют поисковые системы. На этот вопрос ответить легко — все, что попадается под руку: текст, описания, названия картинок, документы поддерживаемых форматов (PDF, XLSX, DOCX и т. д.).

Наша задача — ограничить настойчивость индексирующих роботов поисковых машин, чтобы они не трогали то, что запрещено. Для этого робот должен получить определенный сигнал. Как это сделать? Было найдено достаточно простое, но элегантное решение — в корень сайта помещается файл с именем robots.txt, который содержит правила для поисковых машин.

Допустим, что у вас есть сайт www.your_name.com. Робот, прежде чем начать свою работу, пробует загрузить файл www.your_name.com/robots.txt. Если он будет найден, то индексация пойдет в соответствии с описанными в файле правилами, иначе процесс затронет все подряд.