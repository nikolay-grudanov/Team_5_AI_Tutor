---
source_image: page_250.png
page_number: 250
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 31.09
tokens: 7939
characters: 2444
timestamp: 2025-12-24T04:25:56.918489
finish_reason: stop
---

Формат файла очень прост и состоит всего лишь из двух директив:

□ User-Agent: параметр — в качестве параметра передается имя поисковой системы, к которой относятся запреты. Таких записей в файле может быть несколько, и каждая будет описывать свою поисковую систему. Если запреты должны действовать на все поисковики, то достаточно указать в начале файла директиву User-Agent с параметром звездочка (*);

□ Disallow: адрес — запрещает индексировать определенный адрес, который указывается относительно URL. Например, если вы хотите отказаться от индексации страниц с URL www.your_name.com/admin, то в качестве параметра нужно указать /admin. Как видите, этот адрес берется именно из URL, а не из вашей реальной файловой системы, потому что поисковая система не может знать истинное положение файлов на диске сервера и оперирует только адресами URL.

Вот пример файла robots.txt, который запрещает индексацию страниц, находящихся по адресам www.your_name.com/admin и www.your_name.com/cgi_bin, для любых индексирующих роботов поисковых систем:

User-Agent: *
Disallow: /cgi-bin/
Disallow: /admin/

Данные правила запрещают индексацию с учетом подкаталогов. Например, файлы по адресу www.your_name.com/cgi_bin/forum тоже не будут индексироваться.

Следующий пример запрещает индексацию сайта вовсе:

User-Agent: *
Disallow: /

Если на вашем сайте есть каталог с секретными данными, то следует запретить их индексацию. Но это не является эффективным методом защиты, потому что файл robots.txt может скачать любой желающий, а, значит, это даже лишний раз укажет, где искать важные файлы и документы. Так что лучший метод защиты — не хранить ничего важного на открытых серверах. Лучше для этого создавать отдельные сайты, которые будут доступны только с определенных IP-адресов и только с доступом по паролю.

7.9. Безопасность подключения

Различные технологии прослушивания сетевого трафика в основном эффективны в локальных сетях, но хакеры больше любят интернет-соединения, потому что здесь можно найти больше интересного, и есть лазейка, чтобы удаленно проводить атаку.

Что опасного может увидеть хакер, когда клиент просматривает страницы на веб-сервере? Мы каждый день вводим на веб-страницах какие-либо данные, пароли, номера кредитных карт, и именно это является основной целью хакера.

Как можно, например, находясь в Европе, перехватить трафик, который проходит между двумя городами в США? Скорее всего, пакеты будут следовать по каналам