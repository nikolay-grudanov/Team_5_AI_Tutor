---
source_image: page_258.png
page_number: 258
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 54.27
tokens: 11819
characters: 1663
timestamp: 2025-12-24T06:17:40.347247
finish_reason: stop
---

> errors$type <- rep(c('default train', 'penalty train',
'default test', 'penalty test'), rep(250, 4))
> ggplot(errors, aes(x=iter, y=train_error, group=type)) +
geom_line(aes(linetype=type, color=type))

Результат, изображенный на рис. 6.10, показывает, как заданная по умолчанию модель неуклонно улучшает точность для тренировочного набора, но фактически становится хуже для проверочного набора. Оштрафованная модель такое поведение не показывает.

![График ошибок модели XGBoost](../images/6.10.png)

Рис. 6.10. Коэффициент ошибок модели XGBoost, заданной по умолчанию, против оштрафованной версии XGBoost

**Гребневая регрессия и лассо-регрессия**

Метод наложения штрафа на сложность модели, чтобы помочь предотвратить переподгонку, берет начало с 1970-х гг. Регрессия наименьших квадратов минимизирует остаточную сумму квадратов (RSS) (см. разд. "Наименьшие квадраты" главы 4). Гребневая регрессия минимизирует сумму квадратов остатков плюс штраф на число и размер коэффициентов:

\[
\sum_{i=1}^n \left( Y_i - \hat{b}_0 - \hat{b}_1 x_1 - ... - \hat{b}_p X_p \right)^2 + \lambda \left( \hat{b}_1^2 + ... + \hat{b}_p^2 \right).
\]

Значение \( X \) определяет, насколько много коэффициенты штрафуются; более крупные значения порождают модели, которые с меньшей вероятностью будут переподогнаны к данным. Лассо-регуляризация аналогична за исключением того, что в качестве штрафного члена она использует манхэттенское расстояние вместо евклидова:

\[
\sum_{i=1}^n \left( Y_i - \hat{b}_0 - \hat{b}_1 x_1 - ... - \hat{b}_p X_p \right)^2 + \alpha \left( |\hat{b}_1| + ... + |\hat{b}_p| \right).
\]

Параметры lambda и alpha в xgboost действуют аналогичным образом.