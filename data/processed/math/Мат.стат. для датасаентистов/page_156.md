---
source_image: page_156.png
page_number: 156
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 65.41
tokens: 11891
characters: 2280
timestamp: 2025-12-24T06:12:48.801153
finish_reason: stop
---

Как отыскать модель, которая минимизирует AIC? Один из подходов состоит в переборе всех возможных моделей, который называется регрессией всех подмножеств (all subset regression). Данный подход вычислительно затратен и не выполняется для задач с большими данными и многими переменными. Привлекательной альтернативой является использование шаговой регрессии, которая последовательно добавляет и отбрасывает предикторы для нахождения модели, которая понижает AIC. Программный пакет MASS, созданный Венеблз (Venables) и Рипли (Ripley), предлагает функцию шаговой регрессии, которая называется stepAIC:

library(MASS)
step <- stepAIC(house_full, direction="both")
step

Call:
lm(formula = AdjSalePrice ~ SqFtTotLiving + Bathrooms + Bedrooms +
    BldgGrade + PropertyType + SqFtFinBasement + YrBuilt, data = house0,
    na.action = na.omit)

Coefficients:
(Intercept)                6227632.22
Bathrooms                  44721.72
BldgGrade                  139179.23
PropertyTypeTownhouse      92216.25
YrBuilt                    -3592.47
SqFtTotLiving              186.50
Bedrooms                   -49807.18
PropertyTypeSingle Family   23328.69
SqFtFinBasement            9.04

Функция выбрала модель, в которой несколько переменных были отброшены из house_full: SqFtLot, NbrLivingUnits, YrRenovated и NewConstruction.

Более простыми являются прямой отбор и обратный отбор. В прямом отборе вы начинаете без предикторов и на каждом шаге добавляете по одному предиктору, который имеет самый большой вклад в \( R^2 \), останавливаясь, когда вклад перестает быть статистически значимым. В обратном отборе, или обратном исключении (backward elimination), вы начинаете с полной модели и устраняете предикторы, которые не являются статистически значимыми, пока у вас не останется модель, в которой все предикторы являются статистически значимыми.

Штрафная регрессия аналогична по духу критерию AIC. Вместо того чтобы явным образом перебирать дискретный набор моделей, уравнение подгонки моделей содержит ограничение, которое штрафует модель за слишком большое число переменных (параметров). Взамен полного устранения предикторных переменных — как в шаговой регрессии, прямом и обратном отборе — штрафная регрессия применяет штраф путем понижения коэффициентов в некоторых случаях почти до