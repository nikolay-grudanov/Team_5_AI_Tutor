---
source_image: page_287.png
page_number: 287
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 51.93
tokens: 11716
characters: 1837
timestamp: 2025-12-24T06:18:59.466920
finish_reason: stop
---

Ключевые идеи для модельно-ориентированной кластеризации

• Предполагаются, что кластеры происходят из различных процессов, порождающих данные, с разными вероятностными распределениями.

• Далее выполняются подгонки разных моделей, принимая разные количества (обычно нормальных) распределений.

• Данный метод выбирает модель (и связанное с ней количество кластеров), которая хорошо подходит к данным без использования слишком большого числа параметров (т. е. переподгонки).

Дополнительные материалы для чтения

Подробнее о модельно-ориентированной кластеризации см. документацию по mclust (http://www.stat.washington.edu/research/reports/2012/tr597.pdf).

Шкалирование и категориальные переменные

Приемы обучения без учителя обычно требуют, чтобы данные были соответствующим образом прошколированы. В этом состоит отличие от многих приемов регрессии и классификации, в которых шкалирование не имеет значения (исключением является метод K ближайших соседей; см. разд. "K ближайших соседей" главы 6).

Ключевые термины

Шкалирование (scaling)
Сплющивание или расширение данных обычно для приведения многочисленных переменных к одинаковой шкале измерения.

Нормализация (normalization)
Один из методов шкалирования — вычитание среднего и деление на стандартное отклонение.
Синоним: стандартизация.

Расстояние Говера (Gower’s distance)
Алгоритм шкалирования, применяемый к смешанным числовым и категориальным данным для приведения всех переменных к диапазону 0–1.

Например, если говорить о данных персональной ссуды, переменные имеют самые разные единицы измерения и величины. У некоторых переменных относительно малые значения (например, число используемых лет), в то время как у других — очень большие (например, сумма кредита в долларах). Если данные не прошколировать, то переменные с большими значениями будут доминировать над PCA,