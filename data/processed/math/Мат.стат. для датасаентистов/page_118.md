---
source_image: page_118.png
page_number: 118
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 64.66
tokens: 11945
characters: 2321
timestamp: 2025-12-24T06:10:54.701696
finish_reason: stop
---

Ключевые термины

Ошибка 1-го рода (type 1 error)
Ошибочный вывод, что эффект является статистически значимым.

Коэффициент ложных открытий (false discovery rate)
Доля совершения ошибки 1-го рода в результате множественного тестирования.

Корректировка p-значений (adjustment of p-values)
Уточнение значения при выполнении множественного тестирования на одинаковых данных.

Переподгонка (overfitting)
Подгонка к шуму.

Например, если имеется 20 предикторных переменных и одна результирующая переменная, и все они сгенерированы случайным образом, то существуют достаточно хорошие шансы, что по крайней мере один предиктор (ложным образом) окажется статистически значимым, если выполнить серию из 20 проверок значимости при \( \alpha \)-уровне, равном 0,05. Как уже обсуждалось ранее, эта ситуация называется ошибкой 1-го рода. Данную вероятность можно рассчитать, сперва найдя вероятность, что все переменные пройдут проверку, правильно показав незначимость на уровне 0,05. Вероятность, что одна из переменных пройдет проверку, правильно показав незначимость, равна 0,95, поэтому вероятность, что все 20 предикторов пройдут проверку, правильно показав незначимость, будет равна \( 0,95 \times 0,95 \times 0,95... \) или \( 0,95^{20} = 0,36^1 \). Вероятность, что по крайней мере один предиктор (ложным образом) покажет значимость, обратна этой вероятности, или 1 – (вероятность, что все будут незначимыми) = 0,64.

Этот вопрос связан с проблемой переподгонки в глубинном анализе данных, или "подгонки модели к шуму". Чем больше переменных вы добавляете или больше моделей вы выполняете, тем больше вероятность, что нечто проявится как "значимое" просто по чистой случайности.

В задачах обучения с учителем контрольный набор с отложенными данными, где модели диагностируются на данных, которые модель не видела раньше, снижает этот риск. В задачах статистического и машинного обучения, не сопряженных с помеченным контрольным набором, сохраняется риск прихода к заключениям, основанном на статистическом шуме.

1 Правило умножения вероятностей утверждает, что вероятность одновременного наступления n независимых событий является произведением индивидуальных вероятностей. Например, если Вы и я каждый подбросим монету один раз, то вероятность, что Ваша и моя монеты обе повернутся орлом, равна \( 0,5 \cdot 0,5 = 0,25 \).