---
source_image: page_259.png
page_number: 259
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 73.44
tokens: 12019
characters: 2613
timestamp: 2025-12-24T06:18:06.747569
finish_reason: stop
---

Гиперпараметры и перекрестная проверка

Пакет xgboost имеет просто пугающий массив гиперпараметров (см. врезку "Гиперпараметры XGBoost" далее в этой главе, в которой обсуждается этот вопрос). Как явствует из разд. "Регуляризация: предотвращение переподгонки" ранее в этой главе, конкретный выбор может существенно изменить подгонку модели. Чем следует руководствоваться при выборе с учетом огромного количества сочетаний гиперпараметров на выбор? Стандартное решение этой проблемы состоит в том, чтобы использовать перекрестную проверку (см. разд. "Перекрестная проверка" главы 4). Перекрестная проверка в произвольном порядке разбивает данные на K разных групп, так называемые блоки. Для каждого блока выполняется тренировка модели на данных, которые не находятся в блоке, и затем модель оценивается на данных в блоке. Это приводит к мере точности модели на внебыворочных данных. Лучшим набором гиперпараметров является тот, который получен моделью с самой низкой общей ошибкой согласно расчетам по усреднению ошибок из каждого блока.

Для того чтобы проиллюстрировать данный прием, применим его к набору параметров для xgboost. Мы обследуем два параметра: параметр сжатия eta (см. разд. "XGBoost" ранее в этой главе) и max_depth. Параметр max_depth — это максимальная глубина от листового узла до корня дерева, чье значение по умолчанию равно 6. Он дает нам еще один способ управлять переподгонкой: глубокие деревья имеют тенденцию к большей сложности и могут чрезмерно подстраиваться под данные. Сначала мы задаем блоки и список параметров:

> N <- nrow(loan_data)
> fold_number <- sample(1:5, N, replace = TRUE)
> params <- data.frame(eta = rep(c(.1, .5, .9), 3),
    max_depth = rep(c(3, 6, 12), rep(3,3)))

Теперь мы применим приведенный выше алгоритм вычисления ошибки для каждой модели и каждого блока, используя пять блоков:

> error <- matrix(0, nrow=9, ncol=5)
> for(i in 1:nrow(params)){
>   for(k in 1:5){
>     fold_idx <- (1:N)[fold_number == k]
>     xgb <- xgboost(data=predictors[-fold_idx,], label=label[-fold_idx],
>         params = list(eta = params[i, 'eta'],
>             max_depth = params[i, 'max_depth']),
>         objective = "binary:logistic", nrounds=100, verbose=0)
>     pred <- predict(xgb, predictors[fold_idx,])
>     error[i, k] <- mean(abs(label[fold_idx] - pred) >= 0.5)
>   }
> }

Поскольку мы выполняем подгонку в общей сложности 45 моделей, этот процесс займет немного времени. Ошибки хранятся в виде матрицы, при этом модели расположены вдоль строк, блоки — вдоль столбцов. При помощи функции rowMeans мы можем сравнить коэффициент ошибок для разных наборов параметров: