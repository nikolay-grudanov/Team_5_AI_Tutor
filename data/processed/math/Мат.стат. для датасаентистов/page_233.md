---
source_image: page_233.png
page_number: 233
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 81.82
tokens: 12090
characters: 2939
timestamp: 2025-12-24T06:16:53.825248
finish_reason: stop
---

Выбор K

Выбор K имеет чрезвычайно важное значение для результативности KNN. Самый простой выбор состоит в том, чтобы установить \( K = 1 \), что соответствует классификатору 1-го ближайшего соседа. Предсказание интуитивно понятно: оно основывается на нахождении в тренировочном наборе записи, наиболее схожей с новой предсказываемой записью. Принятие за основу \( K = 1 \) редко является лучшим выбором; вы почти всегда будете получать превосходную результативность, используя \( K > 1 \) ближайших соседей.

Вообще говоря, если значение K слишком низкое, то мы можем вызвать переподгонку: включив в модель шум в данных. Более высокие значения K обеспечивают сглаживание, которое снижает риск переподгонки в тренировочных данных. С другой стороны, если K слишком высокое, то мы можем вызвать излишнее сглаживание данных и упустить способность KNN захватывать локальную структуру в данных — одно из его главных преимуществ.

Значение K, которое лучше балансирует между переподгонкой и сверхсглаживанием, как правило, определяется точностными метрическими показателями и, в частности, точностью на основе контрольной выборки с отложенными данными или перекрестной проверки. Нет никакого общего правила относительно лучшего значения K — все зависит главным образом от природы данных. Для высоко структурированных данных с небольшим шумом меньше значения K работают лучше всего. Заимствуя термин из области обработки сигналов, этот тип данных иногда называют данными с высоким соотношением "сигнал/помеха" (SNR, signal-to-noise ratio). Примерами данных с высоким SNR, как правило, являются данные для распознавания почерка и речи. Для шумных данных с меньшей структурированностью (данных с низким SNR), таких как данные о ссудах, уместными являются более крупные значения K. Как правило, значения K попадают в диапазон от 1 до 20. Нередко выбирается нечетное число, чтобы избежать равенства голосов при голосовании.

Компромисс между смещением и дисперсией

Разность потенциалов между сверхсглаживанием и переподгонкой является вариантом компромисса между смещением и дисперсией, повсеместно распространенной проблемы в подгонке статистических моделей. Дисперсия обозначает ошибку моделирования, которая происходит из-за выбора тренировочных данных; т. е. если бы вы решите выбрать другой набор тренировочных данных, то результирующая модель будет иной. Смещение обозначает ошибку моделирования, которая происходит, потому что вы должным образом не идентифицировали реальный базовый сценарий; эта ошибка не исчезнет, если просто добавить больше тренировочных данных. Когда гибкая модель переподогнана, дисперсия увеличивается. Вы можете уменьшить гибкость при помощи более простой модели, но смещение может увеличиться из-за потери гибкости в моделировании реальной базовой ситуации. Общий подход к решению этого компромисса лежит через перекрестную проверку. Для получения дополнительной информации см. разд. "Перекрестная проверка" главы 4.