---
source_image: page_251.png
page_number: 251
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 72.67
tokens: 12018
characters: 2764
timestamp: 2025-12-24T06:17:38.214923
finish_reason: stop
---

Существует два способа оценить важность переменных.

✦ Путем уменьшения точности модели, если значения переменной в произвольном порядке перестановлены (type=1). Произвольная перестановка значений имеет эффект удаления всей предсказательной силы для этой переменной. Точность вычисляется из внепакетных данных (поэтому данная мера является практически перекрестно-проверочной оценкой).

✦ Путем среднего уменьшения в оценке разнородности Джини (см. разд. "Измерение однородности или разнородности" ранее в этой главе) для всех узлов, которые были разбиты по переменной (type=2). Это демонстрирует, какой вклад переменная вносит в улучшение чистоты узлов. Мера основывается на тренировочном наборе, и поэтому менее надежная, чем мера, вычисленная на внепакетных данных.

Верхние и нижние части рис. 6.8 показывают важность переменных согласно уменьшению точности и разнородности Джини в указанном порядке. Переменные в обеих частях ранжированы по уменьшению точности. Оценки важности переменных, произведенные этими двумя мерами, очень различаются.

Поскольку уменьшение точности является более надежным метрическим показателем, зачем нужно использовать меру уменьшения разнородности Джини? По умолчанию randomForest вычисляет только оценку Джини: мера разнородности Джини является побочным продуктом алгоритма, тогда как точность модели в зависимости от переменной требует дополнительных вычислений (произвольная перестановка данных и предсказание этих данных). В случаях, где вычислительная сложность имеет значение, например в эксплуатационной среде, где выполняется подгонка тысяч моделей, она (сложность) не будет стоить дополнительных вычислительных усилий. Кроме того, уменьшение меры Джини проливает свет на то, какие переменные используются случайным лесом для создания своих правил разбиения (вспомним, что данная информация, легко видимая в простом дереве, практически теряется в случайном лесе). Исследование разницы между уменьшением разнородности Джини и важностью переменных за счет точности модели может подсказать способы улучшения модели.

Гиперпараметры

Случайный лес, как и в случае многих статистических алгоритмов машинного обучения, может рассматриваться, как алгоритм "черного ящика" с кнопками для настройки характера работы ящика. Эти кнопки называются гиперпараметрами, т. е. параметрами, которые необходимо настроить прежде, чем приступить к подгонке модели; они не оптимизируются как составная часть тренировочного процесса. В то время как традиционные статистические модели требуют выбора (например, выбора предикторов для использования в модели регрессии), гиперпараметры случайного леса имеют большее значение, в особенности, чтобы предотвратить пере-подгонку. В частности, два самых важных гиперпараметра случайного леса следующие: