---
source_image: page_148.png
page_number: 148
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 66.20
tokens: 12033
characters: 2476
timestamp: 2025-12-24T06:12:24.280067
finish_reason: stop
---

Наименьшие квадраты

Каким образом выполняется подгонка модели к данным? Когда существует четкая связь, вы можете мысленно представить подгонку прямой вручную. На практике прямая регрессии является оценкой, которая минимизирует сумму квадратических значений остатков, также именуемых остаточной суммой квадратов, или RSS (residual sum of squares):

\[
RSS = \sum_{i=1}^{n} \left( Y_i - \hat{Y}_i \right)^2 = \sum_{i=1}^{n} \left( Y_i - \hat{b}_0 - \hat{b}_1 X_i \right)^2.
\]

Оценки \( \hat{b}_0 \) и \( \hat{b}_1 \) — это значения, которые минимизирует RSS.

Метод минимизации суммы квадратических остатков называется регрессией наименьших квадратов, или обычным методом наименьших квадратов (обычным МНК). Данный метод часто приписывается Карлу Фридриху Гауссу, немецкому математику, он был в 1805 г. впервые опубликован французским математиком Андре-Мари Лежандром (Adrien-Marie Legendre). Регрессия наименьших квадратов приводит к простой формуле вычисления коэффициентов:

\[
\hat{b}_1 = \frac{\sum_{i=1}^{n} (Y_i - \bar{Y})(X_i - \bar{X})}{\sum_{i=1}^{n} (X_i - \bar{X})^2};
\]
\[
\hat{b}_0 = \bar{Y} - \hat{b}_1 \bar{X}.
\]

Исторически, вычислительное удобство является одной из причин широкого применения метода наименьших квадратов в регрессии. С появлением больших данных его вычислительная скорость по-прежнему остается важным фактором. Метод наименьших квадратов, как и среднее значение (см. разд. "Медиана и робастные оценки" главы 1), чувствителен к выбросам, хотя этот факт имеет тенденцию быть значимой проблемой только в небольших или умеренных по размеру задачах. См. разд. "Выбросы" далее в этой главе, где рассматриваются выбросы в регрессии.

Терминология регрессионного анализа

Когда аналитики и исследователи используют термин "регрессия" отдельно, они обычно ссылаются на линейную регрессию; в центре внимания, как правило, находится разработка линейной модели для объяснения связи между предикторными переменными и числовой переменной исхода (результатирующей переменной). В своем формальном статистическом смысле регрессия также охватывает нелинейные модели, которые выдают функциональную связь между предикторными переменными и переменной исхода. В сообществе машинного обучения данный термин также временами используется в свободном толковании для ссылки на использование любой предсказательной модели, которая производит предсказанный числовой исход (в отличие от методов классификации, которые предсказывают бинарный или категориальный исход).