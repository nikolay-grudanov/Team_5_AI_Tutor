---
source_image: page_254.png
page_number: 254
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 87.40
tokens: 12158
characters: 2889
timestamp: 2025-12-24T06:18:07.393264
finish_reason: stop
---

1. Инициализировать \( M \) — максимальное число моделей, подлежащих подгонке, и установить счетчик итераций в \( m = 1 \). Инициализировать веса наблюдений \( w_i = 1 / N \) для \( i = 1, 2, ..., N \). Инициализировать ансамблевую модель \( \hat{F}_0 = 0 \).

2. Натренировать модель, используя \( \hat{f}_m \) с применением весов наблюдений \( w_1, w_2, ..., w_N \), которые минимизируют взвешенную ошибку \( e_m \), определяемую путем суммирования весов неправильно классифицированных наблюдений.

3. Добавить в ансамбль модель: \( \hat{F}_m = \hat{F}_{m-1} + \alpha_m \hat{f}_m \), где \( \alpha_m = \frac{\log 1 - e_m}{e_m} \).

4. Обновить веса \( w_1, w_2, ..., w_N \) таким образом, чтобы веса наблюдений, которые были классифицированы неправильно, были увеличены. Размер увеличения зависит от \( \alpha_m \), при этом более крупные значения \( \alpha_m \) приводят к большим весам.

5. Нарастить счетчик моделей \( m = m + 1 \). Если \( m \leq M \), то перейти к шагу 1.

Бустированная оценка задается следующей формулой:

\[
\hat{F} = \alpha_1 \hat{f}_1 + \alpha_2 \hat{f}_2 + ... + \alpha_M \hat{f}_M.
\]

Путем увеличения весов наблюдений, которые были классифицированы неправильно, алгоритм побуждает модели более активно тренироваться на данных, для которых он показывал плохую результативность. Фактор \( \alpha_m \) гарантирует, что модели с более низкой ошибкой будут иметь больший вес.

Градиентный бустинг похож на Adaboost, но представляет задачу как оптимизацию функции стоимости. Вместо того чтобы корректировать веса, градиентный бустинг выполняет подгонку моделей к псевдоостатку, что имеет эффект более активной тренировки на более крупных остатках. В духе случайного леса стохастический градиентный бустинг добавляет в алгоритм произвольность, отбирая наблюдения и предикторные переменные на каждом этапе.

XGBoost

XGBoost — это наиболее широко используемый бесплатный пакет программ с реализацией стохастического градиентного бустинга, который первоначально был разработан Тьянси Ченом (Tianqi Chen) и Карлосом Гестрином (Carlos Guestrin) в Вашингтонском университете. Его вычислительно эффективная реализация со многими опциями доступна в качестве программной библиотеки для большинства основных языков программирования, используемых в науке о данных. В R XGBoost доступен в виде программного пакета xgboost.

Функция xgboost имеет много параметров, которые могут и должны корректироваться (см. разд. "Гиперпараметры и перекрестная проверка" далее в этой главе). Двумя очень важными параметрами являются subsample, управляющий долей наблюдений, которые должны отбираться во время каждой итерации, и eta — фактор сжатия, применяемый к \( \alpha_m \) в алгоритме бустинга (см. разд. "Алгоритм бустинга" ранее в этой главе). Использование subsample заставляет бустинг действовать как случайный лес за исключением того, что отбор выполняется без возврата. Параметр