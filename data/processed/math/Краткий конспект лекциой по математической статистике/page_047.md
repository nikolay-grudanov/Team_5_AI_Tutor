---
source_image: page_047.png
page_number: 47
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 67.09
tokens: 12475
characters: 3364
timestamp: 2025-12-24T07:04:58.786955
finish_reason: stop
---

Определение 24. Критерий \( \delta \) называют минимаксным критерием, если он лучше (не хуже) всех других критериев в смысле минимаксного подхода.

Иначе говоря, минимаксный критерий имеет самую маленькую «наибольшую ошибку» \( \max\{\alpha(\delta), \beta(\delta)\} \) среди всех прочих критериев.
Упражнение. Убедиться, что в примере 28 критерий \( \delta \) является минимаксным, если \( c = 1/2 \).

2. Байесовский подход.

Этот подход применяют в двух случаях:
а) если известно априори, что с вероятностью \( p \) справедлива гипотеза \( H_1 \), а с вероятностью \( q = 1 - p \) — гипотеза \( H_2 \),
б) если задана линейная «функция потерь»: потери от ошибочного решения равны \( p \), если происходит ошибка 1-го рода, и равны \( q \), если второго. Здесь \( p + q \) уже не обязательно равно 1, но потери можно свести к единице нормировкой \( p/(p+q) \) и \( q/(p+q) \).

Говорят, что критерий \( \delta_1 \) не хуже, чем \( \delta_2 \) (в смысле байесовского подхода), если \( p\alpha(\delta_1) + q\beta(\delta_1) \leq p\alpha(\delta_2) + q\beta(\delta_2) \).

Определение 25. Критерий \( \delta \) называют байесовским критерием, если он лучше (не хуже) всех других критериев в смысле байесовского подхода.

Иначе говоря, байесовский критерий имеет самую маленькую «средневзвешенную ошибку» \( p\alpha(\delta) + q\beta(\delta) \) среди всех прочих критериев. По формуле полной вероятности это есть вероятность ошибки критерия в случае (а), или математическое ожидание потерь в случае (б).
Упражнение. Убедиться, что в примере 28 критерий \( \delta \) является байесовским для \( p = q \), если \( c = 1/2 \).

3. Выбор наиболее мощного критерия.

Ошибки 1-го и 2-го рода обычно неравноправны (см. пример 26, и пусть «изделие» = самолет или ядерный реактор). Поэтому возникает желание контролировать одну из ошибок (скажем, 1-го рода). Например, зафиксировать ее на достаточно низком (безопасном) уровне, и рассматривать только критерии с такой или еще меньшей вероятностью ошибки 1-го рода. Среди них наилучшим, очевидно, следует признать критерий, обладающий наименьшей вероятностью ошибки 2-го рода.
Введем при \( \varepsilon \in [0, 1] \) класс критериев \( K_\varepsilon = \{ \delta(\vec{X}) : \alpha(\delta) \leq \varepsilon \} \)

Определение 26. Критерий \( \delta \in K_\varepsilon \) называют наиболее мощным критерием (НМК) уровня \( \varepsilon \) (в классе \( K_\varepsilon \)), если \( \beta(\delta) \leq \beta(\delta_0) \) для любого критерия \( \delta_0 \in K_\varepsilon \).

Если имеется более двух гипотез, то сравнивать критерии в смысле определения 26 можно, если зафиксировать все ошибки, кроме одной.

7.4 Построение НМК. Лемма Неймана - Пирсона

Мы рассмотрим подробно третий подход к сравнению критериев, и научимся строить наиболее мощный критерий заданного уровня.
Пусть \( \vec{X} = (X_1, \ldots, X_n) \) — выборка (набор независимых, одинаково распределенных величин), и имеются две гипотезы о распределении \( X_i \):

\[
H_1 : X_i \in \mathcal{F}_1 \text{ с плотностью } f_1(y), \quad \Psi_1(\vec{X}) = \prod_{i=1}^n f_1(X_i) \text{ — функция правдоподобия}
\]

\[
H_2 : X_i \in \mathcal{F}_2 \text{ с плотностью } f_2(y), \quad \Psi_2(\vec{X}) = \prod_{i=1}^n f_2(X_i) \text{ — функция правдоподобия}
\]

Предполагается, что распределения \( \mathcal{F}_1, \mathcal{F}_2 \) либо оба дискретны, либо оба абсолютно непрерывны.