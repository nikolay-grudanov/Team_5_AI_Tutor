---
source_image: page_062.png
page_number: 62
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 35.95
tokens: 12640
characters: 4003
timestamp: 2025-12-24T07:05:10.077641
finish_reason: stop
---

9 Исследование статистической зависимости: линейная регрессия

Часто требуется определить зависимость наблюдаемой случайной величины от одной или нескольких других величин. Самый общий случай такой зависимости — зависимость статистическая: например, наблюдаемая с.в. \( X \) есть функция от двух с.в. \( \xi \) и \( \eta \), а с.в. \( Z \) — от \( \xi \) и \( \phi \). Зависимость между \( X \) и \( Z \) есть, но она не является, вообще говоря, функциональной зависимостью. Наличие такой зависимости может быть проверено, скажем, по критерию \( \chi^2 \), если имеется выборка из значений пары с.в. \((X, Z)\).

Мы рассмотрим один из случаев этой задачи, когда имеет смысл предполагать наличие «почти функциональной» зависимости между двумя величинами. Часто эту зависимость можно воображать как вход и выход некоторой машины («ящика с шуршавчиком»). Входные данные («факторы»), как правило, известны. На выходе мы наблюдаем результат преобразования входных данных в ящике по каким-либо правилам. Мы можем получать даже значения случайных величин, которые (в среднем) функционально зависят от входных данных. При этом строгая функциональная зависимость входных и выходных данных редко имеет место, чаще на нее накладываются случайные «помехи»: ошибки наблюдения, воздействие неучтенных внешних факторов (случайность, наконец) и т.д.

9.1 Модель регрессии

Рассматривается модель, в которой наблюдаемая случайная величина \( X \) зависит от другой случайной величины \( Y \) (значения которой мы либо задаем, либо знаем). Пусть зависимость математического ожидания \( X \) от значений \( Y \) определяется формулой \( \mathrm{E}(X|Y = t) = f(t) \), где \( f \) — неизвестная функция. После \( n \) экспериментов с входными данными \( Y = t_1, \ldots, Y = t_n \) (какие-то заданные числа или вектора, природа которых чаще всего не имеет значения) получены значения \( X_1, \ldots, X_n \).

Пусть \( X_1 = f(t_1) + \varepsilon_1, \ldots, X_n = f(t_n) + \varepsilon_n \), где \( \varepsilon_i \) — ошибки наблюдения, равные в точности разнице между реальным и усредненным значением случайной величины \( X \) при значении \( Y = t_i \): \( \varepsilon_i = X_i - f(t_i) = X_i - \mathrm{E}(X|Y = t_i) \).

Требуется по значениям \( t_1, \ldots, t_n \) и \( X_1, \ldots, X_n \) оценить как можно точнее функцию \( f \).

9.2 Метод наименьших квадратов: примеры

Даже в отсутствие ошибок наблюдений функцию \( f \) можно восстановить лишь приближенно, в виде полинома. Поэтому обычно предполагают, что \( f \) есть полином (редко больше третьей — четвертой степени) с неизвестными коэффициентами. Метод наименьших квадратов состоит в выборе этих коэффициентов так, чтобы минимизировать сумму квадратов ошибок \( \sum_{i=1}^n (X_i - f(t_i))^2 \).

Пример 31. Пусть \( X_i = \theta + \varepsilon_i, i = 1, \ldots, n \), где \( \theta \) — неизвестный параметр. Здесь \( f \) — полином нулевой степени. Найдем оценку \( \hat{\theta} \) для параметра \( \theta \), на которой достигается минимум величины \( \sum_{i=1}^n \varepsilon_i^2 = \sum_{i=1}^n (X_i - \theta))^2 \).

\[
\frac{\partial}{\partial \theta} \sum_{i=1}^n (X_i - \theta))^2 = -2 \sum_{i=1}^n (X_i - \theta)) \Bigg|_{\theta = \hat{\theta}} = 0 \implies \hat{\theta} = \overline{X}.
\]

Определение 31. Оценка \( \hat{\theta} \) параметра \( \theta \), на которой достигается \( \min_{\theta} \sum_{i=1}^n \varepsilon_i^2 \), называется оценкой метода наименьших квадратов (ОМНК).

Пример 32. Линейная регрессия Рассмотрим линейную регрессию \( X_i = \theta_1 + t_i \theta_2 + \varepsilon_i, i = 1, \ldots, n \), где \( \theta = (\theta_1, \theta_2) \) — неизвестный параметр. Здесь \( f \) — полином первой степени (прямая). Найдем оценку МНК \( (\hat{\theta}_1, \hat{\theta}_1) \) для параметра \( \theta \), на которой достигается минимум величины \( \sum_{i=1}^n \varepsilon_i^2 = \sum_{i=1}^n (X_i - \theta_1 - t_i \theta_2))^2 \).

Обозначим \( \bar{t} = \frac{1}{n} \sum t_i \). Приравняв к нулю частные производные, найдем точку экстремума.