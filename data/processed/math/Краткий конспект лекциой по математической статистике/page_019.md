---
source_image: page_019.png
page_number: 19
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 105.13
tokens: 12619
characters: 3687
timestamp: 2025-12-24T07:03:22.709877
finish_reason: stop
---

Но это возможно только если \( \theta^* \equiv \theta \) (оценка в точности отгадывает неизвестный параметр). То есть \( \theta^* \) даже не является статистикой. Такого типа примеры привести можно (например, по выборке из \( U_{\theta,\theta+1}, \theta \in \mathbb{Z} \) можно точно указать \( \theta \)), но математической статистике здесь делать нечего.

Упражнение. Объяснить словесно доказательство теоремы 6.

Если в очень широком классе всех оценок наилучшей не существует, то, возможно, следует сузить класс рассматриваемых оценок (или разбить класс всех оценок на отдельные подклассы и в каждом искать наилучшую).

Обычно рассматривают оценки, имеющие одинаковое смещение \( b(\theta) = E_\theta \theta^* - \theta \). Обозначим через \( K_b \) класс оценок, имеющих смещение \( b(\theta) \):

\[
K_b = \{ \theta^* : E_\theta \theta^* = \theta + b(\theta) \}, \quad K_0 = \{ \theta^* : E_\theta \theta^* = \theta \}.
\]

Здесь \( K_0 \) — класс несмешенных оценок.

Определение 9. Оценка \( \theta^* \in K_b \) называется эффективной оценкой в классе \( K_b \), если она лучше (не хуже) всех других оценок класса \( K_b \) в смысле среднеквадратического подхода. То есть для любой \( \theta_1^* \in K_b \), для любого \( \theta \in \Theta \)

\[
E_\theta (\theta^* - \theta)^2 \leq E_\theta (\theta_1^* - \theta)^2.
\]

Определение 10. Эффективная оценка в классе \( K_0 \) (несмешенных оценок) называется просто эффективной.

Замечание 10. Для \( \theta^* \in K_0 \), по определению дисперсии, \( E_\theta (\theta^* - \theta)^2 = E_\theta (\theta^* - E_\theta \theta^*)^2 = D_\theta \theta^* \), так что сравнение в среднеквадратичном несмешенных оценок — это сравнение их дисперсий. Поэтому эффективную оценку (в классе \( K_0 \)) часто называют «несмещенной оценкой с равномерно минимальной дисперсией». Равномерность подразумевается по всем \( \theta \in \Theta \). Для \( \theta^* \in K_b \)

\[
E_\theta (\theta^* - \theta)^2 = D_\theta (\theta^* - \theta^*) + (E_\theta \theta^* - \theta)^2 = D_\theta \theta^* + b^2(\theta),
\]

так что сравнение в среднеквадратичном оценок с одинаковым смещением — это также сравнение их дисперсий.

Упражнение. Мы собираемся искать наилучшую оценку в классе \( K_b \). Объясните, почему доказательство теоремы 4 не пройдет в классе \( K_b \).

3.3 Единственность эффективной оценки в классе с фиксированным смещением

Теорема 7. Если \( \theta_1^* \in K_b \) и \( \theta_2^* \in K_b \) — две эффективные оценки в классе \( K_b \), то с вероятностью 1 они совпадают: \( P_\theta (\theta_1^* = \theta_2^*) = 1 \).

Доказательство теоремы 7. Заметим сначала, что \( E_\theta (\theta_1^* - \theta)^2 = E_\theta (\theta_2^* - \theta)^2 \). Действительно, так как \( \theta_1^* \) эффективна в классе \( K_b \), то она не хуже оценки \( \theta_2^* \), то есть

\[
E_\theta (\theta_1^* - \theta)^2 \leq E_\theta (\theta_2^* - \theta)^2
\]

и наоборот. Поэтому \( E_\theta (\theta_1^* - \theta)^2 = E_\theta (\theta_2^* - \theta)^2 \).

Рассмотрим оценку \( \theta^* = \frac{\theta_1^* + \theta_2^*}{2} \in K_b \) (доказать!). Вычислим ее среднеквадратическое отклонение. Заметим, что

\[
\left( \frac{a+b}{2} \right)^2 + \left( \frac{a-b}{2} \right)^2 = \frac{a^2 + b^2}{2}.
\] (4)

Положим \( a = \theta_1^* - \theta, b = \theta_2^* - \theta \). Тогда \( (a+b)/2 = \theta^* - \theta, a-b = \theta_1^* - \theta_2^* \). Подставим эти выражения в (4) и возьмем математические ожидания обеих частей:

\[
E_\theta (\theta^* - \theta)^2 + E_\theta \left( \frac{\theta_1^* - \theta_2^*}{2} \right)^2 = E_\theta \frac{(\theta_1^* - \theta)^2 + (\theta_2^* - \theta)^2}{2} = E_\theta (\theta_1^* - \theta)^2 = E_\theta (\theta_2^* - \theta)^2.
\] (5)