---
source_image: page_131.png
page_number: 131
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 28.87
tokens: 5471
characters: 1766
timestamp: 2025-12-24T07:29:08.516782
finish_reason: stop
---

§ 17 НЕРАВЕНСТВО ЧЕБЫШЕВА

Теорема 1.2 (неравенство Чебышева). Если случайная величина \( \xi \) имеет дисперсию, то при любом \( \varepsilon > 0 \)

\[
P(|\xi - M\xi| \geq \varepsilon) \leq \frac{D\xi}{\varepsilon^2}.
\]

Доказательство. Случайная величина \( \eta = (\xi - M\xi)^2 \geq 0 \) при всех \( \omega \in \Omega \) и \( M\eta = M(\xi - M\xi)^2 = D\xi \) конечно. Следовательно, можно воспользоваться неравенством (1.1). Таким образом,

\[
P(|\xi - M\xi| \geq \varepsilon) = P(\eta \geq \varepsilon^2) \leq \frac{M\eta}{\varepsilon^2} = \frac{D\xi}{\varepsilon^2}.
\]

Неравенство Чебышева позволяет оценивать вероятности отклонений значений случайной величины от своего математического ожидания. Пусть проводится \( n \) независимых измерений некоторой неизвестной величины \( a \). Ошибки измерения \( \delta_1, \delta_2, \ldots, \delta_n \) будем считать случайными величинами. Предположим, что \( M\delta_k = 0,\ k = 1, 2, \ldots, n \). Это условие можно рассматривать как отсутствие систематической ошибки. Пусть еще \( D\delta_k = b^2 \). За значение неизвестной величины \( a \) принимают обычно среднее арифметическое результатов измерений. Тогда ошибка в определении числа \( a \) будет равна

\[
\eta_n = \frac{\delta_1 + \delta_2 + \ldots + \delta_n}{n}
\]

и

\[
D\eta_n = \frac{1}{n^2}(D\delta_1 + \ldots + D\delta_n) = \frac{b^2}{n}, \quad M\eta_n = 0.
\]

Предположим, что нам нужно, чтобы ошибка \( \eta_n \) не превосходила \( \Delta \) с достаточно большой вероятностью. Например,

\[
P(|\eta_n| < \Delta) > 0,99.
\]

Это неравенство можно записать в эквивалентном виде

\[
P(|\eta_n| \geq \Delta) \leq 0,01. \tag{1.2}
\]

По неравенству Чебышева имеем

\[
P(|\eta_n| \geq \Delta) \leq \frac{D\eta_n}{\Delta^2} = \frac{b^2}{n\Delta^2},
\]