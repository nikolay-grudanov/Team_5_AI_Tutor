---
source_image: page_289.png
page_number: 289
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 35.30
tokens: 6796
characters: 3040
timestamp: 2025-12-24T07:59:17.475780
finish_reason: stop
---

Если вас больше интересует не проверка конкретной теоретической модели, а исследование отношений переменных внутри этого набора данных, то вы, вероятно, решите использовать автоматизированный метод для построения вашей модели. Вы решаете построить две модели, используя два метода (методы с включением и метод с исключением), и далее сравнить эти две модели. Для метода с включением вы устанавливаете критерий добавления \( p \leq 0,05 \) (коэффициент для любого предиктора должен удовлетворить этому стандарту, для того чтобы быть включенным в модель); для метода с исключением вы устанавливаеете критерий удаления \( F \geq 0,100 \) (переменная будет удалена, если уровень изменения вероятности \( F \)-статистики не ниже 0,100).

Метод с включением

В методе с включением первым в модель вводится предиктор с наиболее сильной парной корреляцией с IQ (\( r = 0,978 \)) — вычислительные навыки. В этой модели \( R^2 = 0,956 \), и такая модель в целом значимая, с \( F(1, 10) = 217,36, p = 0,000 \). Ни один другой предиктор не дает значимого улучшения качества модели, так что это и есть наша окончательная модель, её коэффициенты показаны в табл. 10.12. Такой результат одновременно и удивляет (так как другие исследователи обнаруживали близкие отношения IQ и других переменных, например мыслительной способности), и не удивляет (так как большинство наших предикторов настолько сильно коррелируют, что мы могли бы ожидать большого перекрытия между любыми объясняемыми дисперсиями IQ).

Таблица 10.12. Окончательная регрессионная модель, построенная с использованием автоматизированного метода с включением

<table>
  <tr>
    <th rowspan="2"> </th>
    <th colspan="2">Нестандартизованные коэффициенты</th>
    <th colspan="3">Стандартизованные коэффициенты</th>
  </tr>
  <tr>
    <th>В</th>
    <th>Станд. ошибка</th>
    <th>Бета</th>
    <th>t</th>
    <th>Значимость</th>
  </tr>
  <tr>
    <td>Константа</td>
    <td>74.318</td>
    <td>2.043</td>
    <td></td>
    <td>36.374</td>
    <td>0.000</td>
  </tr>
  <tr>
    <td>Вычисление</td>
    <td>5.122</td>
    <td>0.347</td>
    <td>0.978</td>
    <td>14.743</td>
    <td>0.000</td>
  </tr>
</table>

Таблица 10.13 отражает информацию о переменных, исключенных из финальной модели. Можно заметить, рассматривая t-статистики и значимость колонок, что некоторые из них очень близки к порогу включения, особенно «Чтение» (\( t = 2,239, p = 0,052 \)). Легко представить, что, если бы вы составили выборку из других наблюдений, «Чтение» могло бы быть включено в модель, а «Счет» — исключено. Регрессионная модель, полученная при помощи метода с включением:

\[
IQ = 74,318 + 5,122(\text{Счет}) + e.
\]

Одно огромное преимущество использования метода с включением состоит в быстром нахождении минимальной модели, объясняющей наибольшее количество дисперсии в вашей выборке данных. Это особенно полезно, если у вас имеется большое число предикторов, но никаких определенных теоретических соображений по поводу того, как они соотносятся друг с другом и с результатом, и вы хотите только