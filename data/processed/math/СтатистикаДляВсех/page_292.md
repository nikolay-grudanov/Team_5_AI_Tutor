---
source_image: page_292.png
page_number: 292
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 43.57
tokens: 7061
characters: 3155
timestamp: 2025-12-24T07:59:32.104521
finish_reason: stop
---

<table>
  <tr>
    <th>Модель</th>
    <th colspan="2">Нестандартизованные коэффициенты</th>
    <th colspan="3">Стандартизованные коэффициенты</th>
  </tr>
  <tr>
    <th></th>
    <th></th>
    <th></th>
    <th></th>
    <th></th>
    <th></th>
  </tr>
  <tr>
    <td rowspan="5">4</td>
    <td>Музыкальность</td>
    <td>-1.179</td>
    <td>0.775</td>
    <td>-0.161</td>
    <td>-1.522</td>
    <td>0.172</td>
  </tr>
  <tr>
    <td>Мышление</td>
    <td>-0.785</td>
    <td>0.399</td>
    <td>-0.179</td>
    <td>-1.968</td>
    <td>0.090</td>
  </tr>
  <tr>
    <td>Константа</td>
    <td>68.274</td>
    <td>5.524</td>
    <td>12.360</td>
    <td>0.000</td>
    <td></td>
  </tr>
  <tr>
    <td>Вычисление</td>
    <td>3.149</td>
    <td>1.122</td>
    <td>0.601</td>
    <td>2.806</td>
    <td>0.023</td>
  </tr>
  <tr>
    <td>Чтение</td>
    <td>2.476</td>
    <td>1.352</td>
    <td>0.393</td>
    <td>1.831</td>
    <td>0.105</td>
  </tr>
  <tr>
    <td>Мышление</td>
    <td>-0.294</td>
    <td>0.253</td>
    <td>-0.067</td>
    <td>-1.161</td>
    <td>0.279</td>
  </tr>
  <tr>
    <td rowspan="3">5</td>
    <td>Константа</td>
    <td>64.655</td>
    <td>4.649</td>
    <td>13.908</td>
    <td>0.000</td>
    <td></td>
  </tr>
  <tr>
    <td>Вычисление</td>
    <td>2.765</td>
    <td>1.093</td>
    <td>0.528</td>
    <td>2.529</td>
    <td>0.030</td>
  </tr>
  <tr>
    <td>Чтение</td>
    <td>2.945</td>
    <td>1.316</td>
    <td>0.467</td>
    <td>2.239</td>
    <td>0.050</td>
  </tr>
</table>

Окончательная модель регрессии, полученная с помощью метода с исключением (модель № 5 в табл. 10.14), следующая:

\[
IQ = 64.655 + 2.765(\text{Вычисление}) + 2.945(\text{Чтение}) + e.
\]

Эта модель объясняет 97,2% дисперсии IQ, что немного больше, чем модель метода с включением (95,6%). Хотя обе модели объясняют почти одинаковое количество дисперсии, интересно отметить отличие коэффициентов. Модель, полученная методом включения, имеет большую константу и больший коэффициент для навыка вычислений. Эти различия, скорее всего, объясняются тем, что некоторое количество дисперсии, объясненной навыком «Счет» в первой модели, объясняется навыком «Чтение» во второй и что включение второго предиктора естественно уменьшает константу, так как каждый результат IQ теперь объясняется двумя оценками умственных способностей, а не одной.

Упражнения

Множественная линейная регрессия может быть использована для изучения разных типов исследовательских задач, как показано в нижеследующих примерах.

Пример 1

Как специалист по кадрам вы заинтересованы в мотивационных факторах, связанных с продуктивностью (результатирующая переменная) ИТ-групп (групп, занимающихся разработкой информационных технологий), основанной на метрике KLOC (kilolines of code) — тысячи строк кода, написанного за неделю. Считается, что на продуктивность влияют четыре мотивационных фактора: они могут быть основаны либо на внутренней, либо на внешней мотивации и быть либо самооценочными, либо оцениваемыми со стороны. Для измерения этих факторов разработаны четыре шкалы, которые используются как предикторные переменные в модели (в скобках оригинальные названия):