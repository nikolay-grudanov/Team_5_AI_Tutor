---
source_image: page_236.png
page_number: 236
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 36.07
tokens: 6656
characters: 2719
timestamp: 2025-12-24T07:57:59.905287
finish_reason: stop
---

Расчет простой регрессии вручную

сравнение группы 1 с группой 2 (приняты обозначения «группа I» и «группа J»). Разница в средних между этим группами составляет 7,80, и разница достоверна (\( p = 0,029 \)). 95%-ный доверительный интервал для этой разницы средних составляет (0,067, 14,92); заметим, что он не включает нуль. Вторая строка табл. 8.7 представляет из себя сравнение групп 1 и 3; средняя разница составляет 4,17, и она не достоверна (\( p = 0,341 \)). Заметим для сравнения, что доверительный интервал включает нуль (−2,95, 11,30). В третьей строке сравниваются группы 2 и 1; результат ровно тот же, что и в первой строке, с точностью до знака (поскольку в третьей строке среднее группы 1 вычитается из среднего группы 2, тогда как в первой строке среднее группы 2 вычиталось из среднего группы 1). В строке 4 показано сравнение средних групп 2 и 3; разница в средних составляет −3,62, и она не достоверна (\( p = 0,442 \)). Строки 5 и 6 совпадают со строками 2 и 4.

Столбцы табл. 8.8 соответствуют гомогенным наборам групп; в гомогенном наборе средние включенных групп не отличаются достоверно друг от друга. В данном случае группы 2 и 3 формируют гомогенную группу (столбец 1); группы 1 и 3 также гомогенны (столбец 2).

Расчет простой регрессии вручную

Коэффициенты регрессии можно рассчитать вручную, используя суммы квадратов, дисперсии \( X \) и \( Y \) и несколько других величин, которые можно вычислить без помощи компьютера. Проблема с ручным расчетом регрессии не в том, что он включает какие-то особенно сложные этапы, а в том, что с набором данных любого размера работа становится очень утомительной и способствующей ошибкам. Тем не менее пройти через модифицированную версию этого процесса может быть полезным для понимания смысла коэффициентов регрессии, и именно для этого приведен следующий раздел.

Мы заметили ранее, что при работе с реальными данными мы не ожидаем получить идеальное предсказание по уравнению регрессии. На самом деле мы предполагаем, что будут некоторые различия между наблюдаемыми и предсказываемыми по модели значениями. Мы также обсуждали квадраты отклонений, которые являются квадратами разностей каждого наблюдаемого и предсказанного по уравнению значений. Сумма квадратов отклонений — это сумма квадратов ошибок, и она рассчитывается, как показано на рис. 8.10.

\[
SSE = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
\]

Рис. 8.10. Сумма квадратов ошибок

В этой формуле \( y_i \) — это наблюдаемое значение, а \( \hat{y}_i \) — это предсказанное значение (в соответствии с уравнением регрессии) для него. Поскольку значение \( \hat{y}_i \) определяется по уравнению регрессии (\( ax_i + b \)), сумма квадратов ошибок также может быть записана, как показано на рис. 8.11.