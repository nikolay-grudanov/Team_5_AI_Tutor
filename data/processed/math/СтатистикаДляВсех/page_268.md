---
source_image: page_268.png
page_number: 268
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 39.99
tokens: 6824
characters: 3429
timestamp: 2025-12-24T07:58:50.108661
finish_reason: stop
---

Модели множественной регрессии

Очень часто применяется такое правило, что каждая переменная должна объяснять статистически значимое количество дисперсии. На самом деле регрессионную модель нельзя сделать хуже (уменьшить объясненную дисперсию) добавлением новой переменной, но даже модели, построенные по принципу максимизации объясненной дисперсии в целом, имеют некоторые правила определения, достаточно ли данная переменная улучшает модель и может ли она быть сохранена в модели. Во-вторых, при работе с множественными предикторами нужно ожидать, что некоторые из них обычно коррелируют как с зависимой переменной, так и друг с другом; из этого следует, что добавление или удаление предиктора, скорее всего, изменяет коэффициенты при всех переменных в модели. Это очень важно при интерпретации результатов: не достаточно утверждать, что переменная \( A \) — незначимый предиктор результирующей \( E \), придется сказать, что переменная \( A \) — незначимый предиктор \( E \) в модели, включающей переменные \( B, C \) и \( D \).

Формально модели множественной линейной регрессии имеют вид:

\[
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_n X_n + e,
\]

где \( Y \) — зависимая переменная, \( \beta_0 \) — константа, \( X_1, X_2, ... X_n \) — независимые переменные, \( \beta_0, \beta_1, ... \beta_n \) — коэффициенты, а \( e \) — остаточный член или ошибка модели. То же самое было описано в главе 8, тем не менее, основные моменты стоит рассмотреть сейчас. Зависимая переменная (\( Y \)) и независимые переменные (\( X_1, X_2, ... X_n \)) — данные наблюдений, а константа (\( \beta_0 \)) и коэффициенты (\( \beta_0, \beta_1, ... \beta_n \)) — значения, вычисляемые алгоритмом линейной регрессии так, чтобы минимизировать остаток или ошибку (\( e \)) в модели. Для данного случая (\( i \)) предсказание величины \( Y_i \) вычисляется с помощью умножения данных наблюдаемых величин (\( X_1, X_2 \) и т. д.) на соответствующие коэффициенты (\( \beta_1, \beta_2 \) и т. д.) и добавлением \( \beta_0 \). Разность между наблюдаемой величиной \( Y_i \) и предсказанной величиной \( \hat{Y}_i \) — ошибка предсказания, или остаток \( e_i \) для данного случая. Коэффициенты определяются так, чтобы сумма квадратов остатков была минимальна. (Остатки должны быть возведены в квадрат, потому что некоторые из них положительны, некоторые — отрицательны и в сумме дают 0, если их не возводить в квадрат.)

Допущения простой регрессии (обсужденные в главе 8) также имеют место и для множественной регрессии. Кроме того, при использовании более одного предиктора приходится волноваться о мультиколлинеарности. Это означает, что ни один из предикторов не должен сильно коррелировать с каким-либо другим. В частности, ни одна из предикторных переменных не должна быть линейной комбинацией других; иными словами, нельзя включать в качестве предикторов переменные \( A, B \) и \( A+B \) в одну модель. Можете смеяться, но это очень легко — составить новую переменную и забыть убрать её компоненты из списка предикторов. Сильно скоррелированные предикторные переменные обычно объясняют одинаковую дисперсию результирующей переменной, что скрывает от нас отношения отдельных переменных с результирующей. К тому же модели, содержащие сильно скоррелированные предикторы, обычно нестабильны, то есть добавление или удаление одной переменной из модели может кардинально поменять коэффициенты и значимость остальных предикторов. (Мы ожидаем