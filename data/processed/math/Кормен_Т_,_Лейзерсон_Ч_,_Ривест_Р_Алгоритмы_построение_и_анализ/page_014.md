---
source_image: page_014.png
page_number: 14
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 81.89
tokens: 11501
characters: 2380
timestamp: 2025-12-24T06:20:42.825719
finish_reason: stop
---

Время работы в худшем случае и в среднем

Итак, мы видим, что время работы в худшем случае и в лучшем случае могут сильно различаться. Большей частью нас будет интересовать время работы в худшем случае (worst-case running time), которое определяется как максимальное время работы для входов данного размера. Почему? Вот несколько причин.

• Зная время работы в худшем случае, мы можем гарантировать, что выполнение алгоритма закончится за некоторое время, даже не зная, какой именно вход (данного размера) попадётся.
• На практике "плохие" входы (для которых время работы близко к максимуму) могут часто попадаться. Например, для базы данных плохим запросом может быть поиск отсутствующего элемента (довольно частая ситуация).
• Время работы в среднем может быть довольно близко к времени работы в худшем случае. Пусть, например, мы сортируем случайно расположенные n чисел в помощью процедуры INSERTION-SORT. Сколько раз придётся выполнить цикл в строках 5–8? В среднем около половины элементов массива A[1..j−1] больше A[j], так что t_j в среднем можно считать равным j/2, и время T(n) квадратично зависит от n.

В некоторых случаях нас будет интересовать также среднее время работы (average-case running time, expected running time) алгоритма на входах данной длины. Конечно, эта величина зависит от выбранного распределения вероятностей (обычно рассматривается равномерное распределение), и на практике реальное распределение входов может оказаться совсем другим. (Иногда его можно преобразовать в равномерное, используя датчик случайных чисел.)

Порядок роста

Наш анализ времени работы процедуры INSERTION-SORT был основан на нескольких упрощающих предположениях. Сначала мы предположили, что время выполнения i-й строки постоянно и равно c_i. Затем мы огрубили оценку до an^2 + bn + c. Сейчас мы пойдём ещё дальше и скажем, что время работы в худшем случае имеет порядок роста (rate of growth, order of growth) n^2, отбрасывая члены меньших порядков (линейные) и не интересуясь коэффициентом при n^2. Это записывают так: T(n) = Θ(n^2) (подробное объяснение обозначений мы отложим до следующей главы).

Алгоритм с меньшим порядком роста времени работы обычно предпочтителен: если, скажем, один алгоритм имеет время работы Θ(n^2), а другой — Θ(n^3), то первый более эффективен (по крайней мере для достаточно длинных входов; будут ли реальные входы таковыми — другой вопрос).