---
source_image: page_048.png
page_number: 48
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 49.36
tokens: 7893
characters: 2558
timestamp: 2025-12-24T09:11:33.452219
finish_reason: stop
---

существует \( N \) возможных исходов, то распределение содержит \( N \) вероятностей \( p_1, p_2, ..., p_N \), причем \( p_1 + p_2 + ... + p_N = 1 \). В случае подбрасывания монеты распределение содержит два исхода: выпадение орла и выпадение решки, вероятность каждого из которых равна \( 1/2 \). Сумма обеих вероятностей \( 1/2 + 1/2 = 1 \), поскольку монета обязательно ляжет на одну сторону.

При равномерном распределении вероятности всех исходов одинаковы. Если имеется \( N \) событий, то вероятность каждого равна \( 1/N \). Например, если 128-битовый ключ случайно выбирается из равномерного распределения, то каждый из \( 2^{128} \) возможных ключей будет иметь вероятность \( 1/2^{128} \).

С другой стороны, если распределение неравномерно, то вероятности не равны. Если результаты подбрасывания монеты имеют неравномерное распределение, то монета называется несимметричной; например, орел может выпадать с вероятностью \( 1/4 \), а решка — с вероятностью \( 3/4 \).

**Энтропия: мера неопределенности**

Энтропия измеряет неопределенность, или беспорядочность, системы. Можно считать, что энтропия описывает удивление, которое мы испытываем, глядя на результат случайного процесса: чем выше энтропия, тем меньше уверенность в результате.

Энтропию распределения вероятностей можно вычислить. Если распределение состоит из вероятностей \( p_1, p_2, ..., p_N \), то его энтропия равна сумме произведений вероятностей на их логарифмы, взятой с обратным знаком:

\[
-p_1 \times \log(p_1) - p_2 \times \log(p_2) ... - p_N \times \log(p_N).
\]

Здесь под \( \log \) понимается двоичный логарифм, т. е. логарифм по основанию 2. В отличие от натурального, двоичный логарифм выражает величину информации в битах и дает целые значения для вероятностей, являющихся степенями двойки. Например, \( \log(1/2) = -1 \), \( \log(1/4) = -2 \) и вообще \( \log(1/2^n) = -n \). (Именно поэтому сумма берется с обратным знаком, чтобы в результате получилось положительное число.) Таким образом, для случайных равномерно распределенных 128-битовых ключей энтропия равна:

\[
2^{128} \times (-2^{-128} \times \log(2^{-128})) = -\log(2^{-128}) = 128 \text{ бит}.
\]

Если заменить 128 произвольным целым числом \( n \), то окажется, что энтропия равномерно распределенных \( n \)-битовых строк равна \( n \) бит.

Энтропия максимальна, когда распределение равномерно, потому что при таком распределении максимальна неопределенность: ни один исход не является более вероятным, чем любой другой. Поэтому \( n \)-битовые значения не могут иметь более \( n \) бит энтропии.