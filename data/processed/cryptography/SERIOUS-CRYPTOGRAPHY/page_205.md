---
source_image: page_205.png
page_number: 205
model: model-run-olm-ocr
prompt_type: olmocr_technical
processing_time: 41.94
tokens: 7898
characters: 2916
timestamp: 2025-12-24T09:15:59.068867
finish_reason: stop
---

В этом алгоритме для поиска значения x применяется цикл for, в котором перебираются все элементы массива. На каждой итерации переменной i присваивается число, начиная с 1. Затем проверяется, равно ли x значение в i-й позиции массива аггау. Если да, то возвращается i. В противном случае мы увеличиваем i на единицу и проверяем следующую позицию — и так далее. Если, проверив все n элементов массива, мы так и не найдем x, то вернем значение 0.

Для алгоритмов такого типа сложность определяется как количество итераций цикла for: 1 в лучшем случае (если x равно аггау[1]), n в худшем случае (если x равно аггау[n] или x вообще не найдено в массиве аггау) и n/2 в среднем, если x случайно распределено среди n позиций массива. Если взять массив, в 10 раз больший, то алгоритм будет работать в 10 раз медленнее. Поэтому сложность пропорциональна n, или «линейно зависит» от n. Алгоритм со сложностью, линейно зависящей от n, считается быстрым, в отличие от алгоритма, сложность которого зависит от n экспоненциально. Хотя обработка большого объема данных замедляется, на практике разница обычно составляет всего несколько секунд.

Но большинство полезных алгоритмов работает медленнее, и их сложность больше линейной. Классический пример — алгоритм сортировки: если дан список n значений в случайном порядке, то в среднем для его сортировки понадобится n × log n простых операций, такую сложность иногда называют линейно-логарифмической. Поскольку n × log n растет быстрее n, скорость сортировки падает не пропорционально n, а быстрее. И все же такие алгоритмы сортировки остаются в области практических вычислений, или вычислений, которые можно выполнить за разумное время.

В какой-то момент мы достигаем потолка, практически осуществимого даже при сравнительно малой длине входных данных. Возьмем простейший пример из области криптоанализа: поиск секретного ключа полным перебором. Напомним (см. главу 1), что для заданного открытого текста P и шифртекста C = E(K, P) требуется примерно 2^n попыток для определения n-битового симметричного ключа, потому что всего существует 2^n возможных ключей. Это пример сложности, растущей экспоненциально. С точки зрения теории сложности, экспоненциальная сложность означает, что задачу практически невозможно решить, потому что при возрастании n объем вычислений очень быстро становится неподъемным.

Можно возразить, что мы здесь сравниваем апельсины с яблоками: в функции search() в листинге 9.1 подсчитывалось количество операций if (аггау[i] == x), а при восстановлении ключа подсчитывается количество шифрований, каждое из которых в тысячи раз медленнее одного сравнения на равенство. Такая несогласованность может быть существенной при сравнении алгоритмов с очень похожей сложностью, но в большинстве случаев она не играет роли, потому что количество операций важнее стоимости одной операции. Кроме того, при оценке сложности постоянные множители игнорируются: говоря,