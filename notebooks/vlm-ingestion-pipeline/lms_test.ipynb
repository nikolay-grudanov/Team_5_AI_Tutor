{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70e7dbee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –Ø—á–µ–π–∫–∞ 1: –ò–º–ø–æ—Ä—Ç—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã\n",
      "   üìÇ –ö–æ—Ä–Ω–µ–≤–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: /home/gna/workspase/education/MEPHI/Team_5_AI_Tutor\n",
      "   üìÇ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏: /home/gna/workspase/education/MEPHI/Team_5_AI_Tutor/notebooks/vlm-ingestion-pipeline/test_images\n",
      "   üìÇ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: /home/gna/workspase/education/MEPHI/Team_5_AI_Tutor/data/test_results\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "============================================================================\n",
    "–Ø–ß–ï–ô–ö–ê 1: –ò–ú–ü–û–†–¢–´ –ò –ù–ê–°–¢–†–û–ô–ö–ò\n",
    "============================================================================\n",
    "–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ OCR –º–æ–¥–µ–ª–µ–π —Å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π —á–µ—Ä–µ–∑ ThreadPoolExecutor\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# –°–¢–ê–ù–î–ê–†–¢–ù–´–ï –ë–ò–ë–õ–ò–û–¢–ï–ö–ò\n",
    "# ============================================================================\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import base64\n",
    "import logging\n",
    "import time\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from io import BytesIO\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Optional, Tuple, Any\n",
    "from dataclasses import dataclass, field, asdict\n",
    "\n",
    "# ============================================================================\n",
    "# –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê\n",
    "# ============================================================================\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import threading\n",
    "\n",
    "# ============================================================================\n",
    "# –í–ù–ï–®–ù–ò–ï –ë–ò–ë–õ–ò–û–¢–ï–ö–ò\n",
    "# ============================================================================\n",
    "from PIL import Image\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "\n",
    "# ============================================================================\n",
    "# –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –ò –ê–ù–ê–õ–ò–ó\n",
    "# ============================================================================\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ============================================================================\n",
    "# –ù–ê–°–¢–†–û–ô–ö–ê –õ–û–ì–ò–†–û–í–ê–ù–ò–Ø\n",
    "# ============================================================================\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(sys.stdout),\n",
    "        logging.FileHandler('ocr_processing.log', encoding='utf-8')\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ============================================================================\n",
    "# –ì–õ–û–ë–ê–õ–¨–ù–´–ï –ü–ï–†–ï–ú–ï–ù–ù–´–ï\n",
    "# ============================================================================\n",
    "# –î–æ–±–∞–≤–∏—Ç—å –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ –≤ PYTHONPATH\n",
    "PROJECT_ROOT = Path.cwd().resolve()\n",
    "if PROJECT_ROOT.name != \"Team_5_AI_Tutor\":\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parents[1]\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "IMAGES_DIR = PROJECT_ROOT / \"notebooks\" / \"vlm-ingestion-pipeline\" / \"test_images\"\n",
    "OUTPUT_DIR = DATA_DIR / \"test_results\"\n",
    "\n",
    "# –°–æ–∑–¥–∞—ë–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "\n",
    "print(\"‚úÖ –Ø—á–µ–π–∫–∞ 1: –ò–º–ø–æ—Ä—Ç—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã\")\n",
    "print(f\"   üìÇ –ö–æ—Ä–Ω–µ–≤–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {PROJECT_ROOT}\")\n",
    "print(f\"   üìÇ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏: {IMAGES_DIR}\")\n",
    "print(f\"   üìÇ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9163ab12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –Ø—á–µ–π–∫–∞ 2: –°—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã\n",
      "   - OCRResult: —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
      "   - ModelStats: —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –º–æ–¥–µ–ª–∏\n",
      "   - FolderStats: —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø–∞–ø–∫–µ\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "============================================================================\n",
    "–Ø–ß–ï–ô–ö–ê 2: –°–¢–†–£–ö–¢–£–†–´ –î–ê–ù–ù–´–•\n",
    "============================================================================\n",
    "\"\"\"\n",
    "\n",
    "@dataclass\n",
    "class OCRResult:\n",
    "    \"\"\"\n",
    "    –†–µ–∑—É–ª—å—Ç–∞—Ç OCR –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.\n",
    "    \n",
    "    Thread-safety: ‚úÖ Immutable –ø–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è\n",
    "    \"\"\"\n",
    "    model_name: str           # –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ (HunyuanOCR, Qwen3-VL, –∏ —Ç.–¥.)\n",
    "    folder_name: str          # –ù–∞–∑–≤–∞–Ω–∏–µ –ø–∞–ø–∫–∏/–∫–Ω–∏–≥–∏\n",
    "    filename: str             # –ò–º—è —Ñ–∞–π–ª–∞ (page_001.png)\n",
    "    page_number: int          # –ù–æ–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã\n",
    "    text: str                 # –†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç\n",
    "    tokens: int               # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤\n",
    "    processing_time: float    # –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (—Å–µ–∫—É–Ω–¥—ã)\n",
    "    text_length: int          # –î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞ (—Å–∏–º–≤–æ–ª–æ–≤)\n",
    "    timestamp: str            # ISO timestamp\n",
    "    error: Optional[str] = None  # –¢–µ–∫—Å—Ç –æ—à–∏–±–∫–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å)\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"–ê–≤—Ç–æ–∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø–æ–ª–µ–π\"\"\"\n",
    "        if not self.timestamp:\n",
    "            self.timestamp = datetime.now().isoformat()\n",
    "        if self.text_length == 0:\n",
    "            self.text_length = len(self.text)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelStats:\n",
    "    \"\"\"\n",
    "    –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ä–∞–±–æ—Ç—ã –º–æ–¥–µ–ª–∏.\n",
    "    \n",
    "    Thread-safety: ‚úÖ –°–æ–∑–¥–∞—ë—Ç—Å—è –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤—Å–µ—Ö –ø–æ—Ç–æ–∫–æ–≤\n",
    "    \"\"\"\n",
    "    model_name: str\n",
    "    total_images: int\n",
    "    folders_processed: int\n",
    "    total_time: float\n",
    "    avg_time_per_image: float\n",
    "    total_tokens: int\n",
    "    total_chars: int\n",
    "    success_rate: float\n",
    "    errors: int\n",
    "    throughput: float  # images per second\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FolderStats:\n",
    "    \"\"\"\n",
    "    –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –æ–¥–Ω–æ–π –ø–∞–ø–∫–µ/–∫–Ω–∏–≥–µ.\n",
    "    \n",
    "    Thread-safety: ‚úÖ –°–æ–∑–¥–∞—ë—Ç—Å—è –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤—Å–µ—Ö –ø–æ—Ç–æ–∫–æ–≤\n",
    "    \"\"\"\n",
    "    folder_name: str\n",
    "    total_images: int\n",
    "    total_time: float\n",
    "    avg_time_per_image: float\n",
    "    total_chars: int\n",
    "    avg_chars_per_image: float\n",
    "    success_rate: float\n",
    "\n",
    "\n",
    "# –ì–ª–æ–±–∞–ª—å–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "ALL_RESULTS = {}      # {model_name: [OCRResult, ...]}\n",
    "FOLDER_GROUPS = {}    # {folder_name: [image_paths, ...]}\n",
    "IMAGE_PATHS = []      # –°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –ø—É—Ç–µ–π –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º\n",
    "\n",
    "print(\"‚úÖ –Ø—á–µ–π–∫–∞ 2: –°—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã\")\n",
    "print(\"   - OCRResult: —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\")\n",
    "print(\"   - ModelStats: —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –º–æ–¥–µ–ª–∏\")\n",
    "print(\"   - FolderStats: —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø–∞–ø–∫–µ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a13024be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –Ø—á–µ–π–∫–∞ 3: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏\n",
      "   –ú–æ–¥–µ–ª—å: allenai/olmocr-2-7b\n",
      "   –ü–æ—Ä—Ç: 1234\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "============================================================================\n",
    "–Ø–ß–ï–ô–ö–ê 3: –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø (–ò–°–ü–†–ê–í–õ–ï–ù–û –ò–ú–Ø –ú–û–î–ï–õ–ò)\n",
    "============================================================================\n",
    "\"\"\"\n",
    "\n",
    "MODELS_CONFIG = {\n",
    "    \"olmOCR-2-7B-1025\": {\n",
    "        \"enabled\": True,\n",
    "        \"port\": 1234,\n",
    "        \"model_path\": \"allenai/olmocr-2-7b\",  # ‚Üê –ò–°–ü–†–ê–í–õ–ï–ù–û! –¢–æ—á–Ω–æ–µ –∏–º—è –∏–∑ LM Studio\n",
    "        \n",
    "        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\n",
    "        \"target_longest_dim\": 512,     # –†–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
    "        \"jpeg_quality\": 85,            # –ö–∞—á–µ—Å—Ç–≤–æ JPEG\n",
    "        \n",
    "        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞\n",
    "        \"max_tokens\": 4096,            # –ò–ª–∏ -1 –¥–ª—è –∞–≤—Ç–æ\n",
    "        \"temperature\": 0.0,            # –î–µ—Ç–µ—Ä–º–∏–Ω–∏–∑–º –¥–ª—è OCR\n",
    "        \n",
    "        # Retry –ª–æ–≥–∏–∫–∞\n",
    "        \"max_retries\": 3,\n",
    "        \"retry_scale_factor\": 0.8,\n",
    "        \n",
    "        \"prompts\": {\n",
    "            \"default\": \"\",\n",
    "        }\n",
    "    },\n",
    "    \"qwen3-vl-4b\": {\n",
    "        \"enabled\": True,\n",
    "        \"port\": 1234,\n",
    "        \"model_path\": \"allenai/olmocr-2-7b\",  # ‚Üê –ò–°–ü–†–ê–í–õ–ï–ù–û! –¢–æ—á–Ω–æ–µ –∏–º—è –∏–∑ LM Studio\n",
    "        \n",
    "        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\n",
    "        \"target_longest_dim\": 512,     # –†–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
    "        \"jpeg_quality\": 85,            # –ö–∞—á–µ—Å—Ç–≤–æ JPEG\n",
    "        \n",
    "        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞\n",
    "        \"max_tokens\": 4096,            # –ò–ª–∏ -1 –¥–ª—è –∞–≤—Ç–æ\n",
    "        \"temperature\": 0.0,            # –î–µ—Ç–µ—Ä–º–∏–Ω–∏–∑–º –¥–ª—è OCR\n",
    "        \n",
    "        # Retry –ª–æ–≥–∏–∫–∞\n",
    "        \"max_retries\": 3,\n",
    "        \"retry_scale_factor\": 0.8,\n",
    "        \n",
    "        \"prompts\": {\n",
    "            \"default\": \"\",\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "PROMPT_TYPE = \"default\"\n",
    "\n",
    "print(\"‚úÖ –Ø—á–µ–π–∫–∞ 3: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏\")\n",
    "print(f\"   –ú–æ–¥–µ–ª—å: {MODELS_CONFIG['olmOCR-2-7B-1025']['model_path']}\")\n",
    "print(f\"   –ü–æ—Ä—Ç: {MODELS_CONFIG['olmOCR-2-7B-1025']['port']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "326bcb67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç –ü–æ–∏—Å–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π...\n",
      "  üìÇ o-predelnom-mnogomernom-raspredelenii: 4 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\n",
      "  üìÇ standartizatsiya-i-kachestvo-zhizni: 3 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\n",
      "  üìÇ –ö–Ω–∏–≥–∞_Python_–ö_–≤–µ—Ä—à–∏–Ω–∞–º_–º–∞—Å—Ç–µ—Ä—Å—Ç–≤–∞_–†–∞–º–∞–ª—å–æ_–õ—É—á–∞–Ω–æ: 5 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\n",
      "\n",
      "‚úÖ –Ø—á–µ–π–∫–∞ 4: –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω—ã\n",
      "   –í—Å–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: 12\n",
      "   –ü–∞–ø–æ–∫: 3\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "============================================================================\n",
    "–Ø–ß–ï–ô–ö–ê 4: –ó–ê–ì–†–£–ó–ö–ê –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–ô\n",
    "============================================================================\n",
    "\"\"\"\n",
    "\n",
    "def load_images_from_directory(images_dir: Path) -> Tuple[List[Path], Dict[str, List[Path]]]:\n",
    "    \"\"\"\n",
    "    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏, –≥—Ä—É–ø–ø–∏—Ä—É—è –ø–æ –ø–∞–ø–∫–∞–º.\n",
    "    \n",
    "    Returns:\n",
    "        (image_paths, folder_groups)\n",
    "    \"\"\"\n",
    "    image_paths = []\n",
    "    folder_groups = {}\n",
    "    \n",
    "    # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã\n",
    "    image_extensions = {'.png', '.jpg', '.jpeg', '.webp', '.bmp'}\n",
    "    \n",
    "    # –†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –æ–±—Ö–æ–¥\n",
    "    for folder in sorted(images_dir.iterdir()):\n",
    "        if not folder.is_dir():\n",
    "            continue\n",
    "        \n",
    "        folder_name = folder.name\n",
    "        folder_images = []\n",
    "        \n",
    "        for img_file in sorted(folder.iterdir()):\n",
    "            if img_file.suffix.lower() in image_extensions:\n",
    "                image_paths.append(img_file)\n",
    "                folder_images.append(img_file)\n",
    "        \n",
    "        if folder_images:\n",
    "            folder_groups[folder_name] = folder_images\n",
    "            print(f\"  üìÇ {folder_name}: {len(folder_images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\")\n",
    "    \n",
    "    return image_paths, folder_groups\n",
    "\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞\n",
    "print(\"üîç –ü–æ–∏—Å–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π...\")\n",
    "IMAGE_PATHS, FOLDER_GROUPS = load_images_from_directory(IMAGES_DIR)\n",
    "\n",
    "print(f\"\\n‚úÖ –Ø—á–µ–π–∫–∞ 4: –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω—ã\")\n",
    "print(f\"   –í—Å–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(IMAGE_PATHS)}\")\n",
    "print(f\"   –ü–∞–ø–æ–∫: {len(FOLDER_GROUPS)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "551d3d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –Ø—á–µ–π–∫–∞ 5: –§—É–Ω–∫—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "============================================================================\n",
    "–Ø–ß–ï–ô–ö–ê 5: –§–£–ù–ö–¶–ò–ò –°–û–•–†–ê–ù–ï–ù–ò–Ø –†–ï–ó–£–õ–¨–¢–ê–¢–û–í\n",
    "============================================================================\n",
    "\"\"\"\n",
    "\n",
    "def save_results_to_files(\n",
    "    results: List[OCRResult],\n",
    "    output_dir: Path,\n",
    "    model_name: str\n",
    ") -> Dict[str, Path]:\n",
    "    \"\"\"\n",
    "    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö.\n",
    "    \n",
    "    Thread-safety: ‚úÖ –í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤—Å–µ—Ö –ø–æ—Ç–æ–∫–æ–≤\n",
    "    \"\"\"\n",
    "    saved_files = {}\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –ø–∞–ø–∫–∞–º\n",
    "    folder_groups = {}\n",
    "    for result in results:\n",
    "        if result.folder_name not in folder_groups:\n",
    "            folder_groups[result.folder_name] = []\n",
    "        folder_groups[result.folder_name].append(result)\n",
    "    \n",
    "    # 1. MARKDOWN —Ñ–∞–π–ª—ã –ø–æ –ø–∞–ø–∫–∞–º\n",
    "    for folder_name, folder_results in folder_groups.items():\n",
    "        md_path = output_dir / f\"{model_name}_{folder_name}_{timestamp}.md\"\n",
    "        \n",
    "        with open(md_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"# OCR –†–µ–∑—É–ª—å—Ç–∞—Ç—ã: {folder_name}\\n\\n\")\n",
    "            f.write(f\"–ú–æ–¥–µ–ª—å: {model_name}\\n\")\n",
    "            f.write(f\"–î–∞—Ç–∞: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "            f.write(f\"{'='*80}\\n\\n\")\n",
    "            \n",
    "            for result in sorted(folder_results, key=lambda r: r.page_number):\n",
    "                f.write(f\"\\n{'‚îÄ'*80}\\n\")\n",
    "                f.write(f\"–°–¢–†–ê–ù–ò–¶–ê {result.page_number}\\n\")\n",
    "                f.write(f\"–§–∞–π–ª: {result.filename}\\n\")\n",
    "                f.write(f\"–í—Ä–µ–º—è: {result.processing_time:.2f}—Å\\n\")\n",
    "                f.write(f\"–°–∏–º–≤–æ–ª–æ–≤: {result.text_length}\\n\")\n",
    "                \n",
    "                if result.error:\n",
    "                    f.write(f\"‚ùå –û–®–ò–ë–ö–ê: {result.error}\\n\")\n",
    "                else:\n",
    "                    f.write(f\"{'‚îÄ'*80}\\n\\n\")\n",
    "                    f.write(result.text)\n",
    "                    f.write(f\"\\n\\n\")\n",
    "        \n",
    "        saved_files[f\"md_{folder_name}\"] = md_path\n",
    "    \n",
    "    # 2. JSON —Ñ–∞–π–ª (–ø–æ–ª–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã)\n",
    "    json_path = output_dir / f\"{model_name}_results_{timestamp}.json\"\n",
    "    json_data = [asdict(r) for r in results]\n",
    "    \n",
    "    with open(json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(json_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    saved_files['json'] = json_path\n",
    "    \n",
    "    # 3. CSV —Ñ–∞–π–ª (—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞)\n",
    "    csv_path = output_dir / f\"{model_name}_stats_{timestamp}.csv\"\n",
    "    \n",
    "    with open(csv_path, 'w', encoding='utf-8', newline='') as f:\n",
    "        fieldnames = [\n",
    "            'folder_name', 'filename', 'page_number', 'text_length',\n",
    "            'tokens', 'processing_time', 'error'\n",
    "        ]\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        \n",
    "        for result in results:\n",
    "            writer.writerow({\n",
    "                'folder_name': result.folder_name,\n",
    "                'filename': result.filename,\n",
    "                'page_number': result.page_number,\n",
    "                'text_length': result.text_length,\n",
    "                'tokens': result.tokens,\n",
    "                'processing_time': result.processing_time,\n",
    "                'error': result.error or ''\n",
    "            })\n",
    "    \n",
    "    saved_files['csv'] = csv_path\n",
    "    \n",
    "    return saved_files\n",
    "\n",
    "\n",
    "print(\"‚úÖ –Ø—á–µ–π–∫–∞ 5: –§—É–Ω–∫—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "258773a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –Ø—á–µ–π–∫–∞ 6: –§—É–Ω–∫—Ü–∏–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "============================================================================\n",
    "–Ø–ß–ï–ô–ö–ê 6: –§–£–ù–ö–¶–ò–ò –°–¢–ê–¢–ò–°–¢–ò–ö–ò\n",
    "============================================================================\n",
    "\"\"\"\n",
    "\n",
    "def calculate_model_stats(results: List[OCRResult]) -> ModelStats:\n",
    "    \"\"\"\n",
    "    –í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–ª—è –º–æ–¥–µ–ª–∏.\n",
    "    \n",
    "    Thread-safety: ‚úÖ –í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤—Å–µ—Ö –ø–æ—Ç–æ–∫–æ–≤\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        return ModelStats(\n",
    "            model_name=\"Unknown\",\n",
    "            total_images=0,\n",
    "            folders_processed=0,\n",
    "            total_time=0,\n",
    "            avg_time_per_image=0,\n",
    "            total_tokens=0,\n",
    "            total_chars=0,\n",
    "            success_rate=0,\n",
    "            errors=0,\n",
    "            throughput=0\n",
    "        )\n",
    "    \n",
    "    model_name = results[0].model_name\n",
    "    total_time = sum(r.processing_time for r in results)\n",
    "    total_tokens = sum(r.tokens for r in results)\n",
    "    total_chars = sum(r.text_length for r in results)\n",
    "    \n",
    "    # –£—Å–ø–µ—à–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (–±–µ–∑ –æ—à–∏–±–æ–∫)\n",
    "    success_count = sum(1 for r in results if not r.error)\n",
    "    success_rate = (success_count / len(results)) * 100 if results else 0\n",
    "    \n",
    "    # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—à–∏–±–æ–∫\n",
    "    errors = sum(1 for r in results if r.error)\n",
    "    \n",
    "    # –ü–∞–ø–∫–∏\n",
    "    folders = set(r.folder_name for r in results)\n",
    "    \n",
    "    return ModelStats(\n",
    "        model_name=model_name,\n",
    "        total_images=len(results),\n",
    "        folders_processed=len(folders),\n",
    "        total_time=total_time,\n",
    "        avg_time_per_image=total_time / len(results) if results else 0,\n",
    "        total_tokens=total_tokens,\n",
    "        total_chars=total_chars,\n",
    "        success_rate=success_rate,\n",
    "        errors=errors,\n",
    "        throughput=len(results) / total_time if total_time > 0 else 0\n",
    "    )\n",
    "\n",
    "\n",
    "def calculate_folder_stats(results: List[OCRResult], folder_name: str) -> Optional[FolderStats]:\n",
    "    \"\"\"\n",
    "    –í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–ª—è –æ–¥–Ω–æ–π –ø–∞–ø–∫–∏.\n",
    "    \n",
    "    Thread-safety: ‚úÖ –í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –ø–æ—Ç–æ–∫–æ–≤\n",
    "    \"\"\"\n",
    "    folder_results = [r for r in results if r.folder_name == folder_name]\n",
    "    \n",
    "    if not folder_results:\n",
    "        return None\n",
    "    \n",
    "    total_time = sum(r.processing_time for r in folder_results)\n",
    "    total_chars = sum(r.text_length for r in folder_results)\n",
    "    \n",
    "    # –£—Å–ø–µ—à–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
    "    success_count = sum(1 for r in folder_results if not r.error)\n",
    "    success_rate = (success_count / len(folder_results)) * 100 if folder_results else 0\n",
    "    \n",
    "    return FolderStats(\n",
    "        folder_name=folder_name,\n",
    "        total_images=len(folder_results),\n",
    "        total_time=total_time,\n",
    "        avg_time_per_image=total_time / len(folder_results) if folder_results else 0,\n",
    "        total_chars=total_chars,\n",
    "        avg_chars_per_image=total_chars / len(folder_results) if folder_results else 0,\n",
    "        success_rate=success_rate\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"‚úÖ –Ø—á–µ–π–∫–∞ 6: –§—É–Ω–∫—Ü–∏–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78a92686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –Ø—á–µ–π–∫–∞ 7: –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã\n",
      "   - image_to_base64() - –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (—Å—Ç–∏–ª—å olmOCR)\n",
      "   - check_model_availability() - –ø—Ä–æ–≤–µ—Ä–∫–∞ LM Studio\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "============================================================================\n",
    "–Ø–ß–ï–ô–ö–ê 7: –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò (–°–¢–ò–õ–¨ olmOCR)\n",
    "============================================================================\n",
    "\"\"\"\n",
    "\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import requests\n",
    "from typing import Optional\n",
    "\n",
    "def image_to_base64(\n",
    "    image: Image.Image, \n",
    "    target_longest_dim: int = 512,\n",
    "    jpeg_quality: int = 85\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç PIL Image –≤ base64 –ø–æ –º–µ—Ç–æ–¥–∏–∫–µ Allen Institute olmOCR.\n",
    "    \n",
    "    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è LM Studio —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º.\n",
    "    \n",
    "    Args:\n",
    "        image: PIL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\n",
    "        target_longest_dim: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –ø–æ –¥–ª–∏–Ω–Ω–æ–π —Å—Ç–æ—Ä–æ–Ω–µ (512-672px)\n",
    "        jpeg_quality: –ö–∞—á–µ—Å—Ç–≤–æ JPEG —Å–∂–∞—Ç–∏—è (70-95, —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 85)\n",
    "    \n",
    "    Returns:\n",
    "        Base64-—Å—Ç—Ä–æ–∫–∞\n",
    "    \"\"\"\n",
    "    # 1. Resize –ø–æ –¥–ª–∏–Ω–Ω–æ–π —Å—Ç–æ—Ä–æ–Ω–µ (–º–µ—Ç–æ–¥ olmOCR)\n",
    "    if max(image.size) > target_longest_dim:\n",
    "        scale = target_longest_dim / max(image.size)\n",
    "        new_size = (int(image.width * scale), int(image.height * scale))\n",
    "        image = image.resize(new_size, Image.Resampling.LANCZOS)\n",
    "    \n",
    "    # 2. –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ RGB\n",
    "    if image.mode != 'RGB':\n",
    "        # –î–ª—è RGBA/LA –¥–µ–ª–∞–µ–º –±–µ–ª—ã–π —Ñ–æ–Ω (–≤–∞–∂–Ω–æ –¥–ª—è —Ñ–æ—Ä–º—É–ª)\n",
    "        if image.mode in ('RGBA', 'LA'):\n",
    "            background = Image.new('RGB', image.size, (255, 255, 255))\n",
    "            if image.mode == 'RGBA':\n",
    "                background.paste(image, mask=image.split()[3])\n",
    "            else:\n",
    "                background.paste(image, mask=image.split()[1])\n",
    "            image = background\n",
    "        else:\n",
    "            image = image.convert('RGB')\n",
    "    \n",
    "    # 3. JPEG —Å–∂–∞—Ç–∏–µ\n",
    "    buffered = BytesIO()\n",
    "    image.save(\n",
    "        buffered,\n",
    "        format='JPEG',\n",
    "        quality=jpeg_quality,\n",
    "        optimize=True,\n",
    "        progressive=True\n",
    "    )\n",
    "    \n",
    "    img_bytes = buffered.getvalue()\n",
    "    \n",
    "    # 4. –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)\n",
    "    size_kb = len(img_bytes) / 1024\n",
    "    if size_kb > 150:\n",
    "        logger.warning(f\"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ {size_kb:.1f}KB –º–æ–∂–µ—Ç –±—ã—Ç—å –±–æ–ª—å—à–∏–º –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞\")\n",
    "    \n",
    "    return base64.b64encode(img_bytes).decode()\n",
    "\n",
    "\n",
    "def check_model_availability(port: int, timeout: int = 5) -> bool:\n",
    "    \"\"\"\n",
    "    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å LM Studio —Å–µ—Ä–≤–µ—Ä–∞.\n",
    "    \n",
    "    Args:\n",
    "        port: –ü–æ—Ä—Ç LM Studio\n",
    "        timeout: –¢–∞–π–º–∞—É—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è\n",
    "    \n",
    "    Returns:\n",
    "        True –µ—Å–ª–∏ —Å–µ—Ä–≤–µ—Ä –¥–æ—Å—Ç—É–ø–µ–Ω\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(\n",
    "            f\"http://localhost:{port}/v1/models\",\n",
    "            timeout=timeout\n",
    "        )\n",
    "        return response.status_code == 200\n",
    "    except Exception as e:\n",
    "        logger.error(f\"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–æ—Ä—Ç–∞ {port}: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "print(\"‚úÖ –Ø—á–µ–π–∫–∞ 7: –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã\")\n",
    "print(\"   - image_to_base64() - –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (—Å—Ç–∏–ª—å olmOCR)\")\n",
    "print(\"   - check_model_availability() - –ø—Ä–æ–≤–µ—Ä–∫–∞ LM Studio\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71936de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –Ø—á–µ–π–∫–∞ 8: –ö–ª–∏–µ–Ω—Ç –¥–ª—è LM Studio –æ–ø—Ä–µ–¥–µ–ª–µ–Ω\n",
      "   ‚ö†Ô∏è  –ù–ï –∑–∞–ø—É—Å–∫–∞–µ—Ç —Å–µ—Ä–≤–µ—Ä - —Ç–æ–ª—å–∫–æ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∑–∞–ø—Ä–æ—Å—ã!\n",
      "   ‚ö†Ô∏è  –ö–æ–Ω—Ç–µ–∫—Å—Ç –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç—Å—è –≤ GUI LM Studio\n",
      "\n",
      "üí° –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:\n",
      "   results = test_model(\"olmOCR-2-7B-1025\")\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "============================================================================\n",
    "–Ø–ß–ï–ô–ö–ê 8: –ö–õ–ò–ï–ù–¢ –î–õ–Ø LM STUDIO (–ë–ï–ó –ó–ê–ü–£–°–ö–ê –°–ï–†–í–ï–†–ê)\n",
    "============================================================================\n",
    "\"\"\"\n",
    "\n",
    "from openai import OpenAI\n",
    "from PIL import Image\n",
    "import time\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, Any, Tuple, Optional, List\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def ocr_single_image(\n",
    "    img_path: Path,\n",
    "    folder_name: str,\n",
    "    model_name: str,\n",
    "    model_config: Dict[str, Any],\n",
    "    prompt_type: str,\n",
    "    page_num: int\n",
    ") -> OCRResult:\n",
    "    \"\"\"\n",
    "    –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ –£–ñ–ï –ó–ê–ü–£–©–ï–ù–ù–´–ô LM Studio.\n",
    "    \n",
    "    –ù–ï –∑–∞–ø—É—Å–∫–∞–µ—Ç —Å–µ—Ä–≤–µ—Ä - —Ç–æ–ª—å–∫–æ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∑–∞–ø—Ä–æ—Å—ã!\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
    "        image = Image.open(img_path)\n",
    "        original_size = image.size\n",
    "        \n",
    "        # 2. –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ base64 —Å —Å–∂–∞—Ç–∏–µ–º\n",
    "        target_dim = model_config.get(\"target_longest_dim\", 512)\n",
    "        jpeg_quality = model_config.get(\"jpeg_quality\", 85)\n",
    "        \n",
    "        image_base64 = image_to_base64(\n",
    "            image, \n",
    "            target_longest_dim=target_dim,\n",
    "            jpeg_quality=jpeg_quality\n",
    "        )\n",
    "        \n",
    "        base64_kb = len(image_base64) / 1024\n",
    "        \n",
    "        # 3. –°–æ–∑–¥–∞–Ω–∏–µ –∫–ª–∏–µ–Ω—Ç–∞ (–ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –£–ñ–ï –ó–ê–ü–£–©–ï–ù–ù–û–ú–£ LM Studio)\n",
    "        client = OpenAI(\n",
    "            base_url=f\"http://localhost:{model_config['port']}/v1\",\n",
    "            api_key=\"lm-studio\",  # –õ—é–±–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è LM Studio\n",
    "            timeout=180\n",
    "        )\n",
    "        \n",
    "        # 4. –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"data:image/jpeg;base64,{image_base64}\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # 5. –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –≤ LM Studio\n",
    "        # –¢–û–õ–¨–ö–û –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –ù–ï –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏!\n",
    "        response = client.chat.completions.create(\n",
    "            # model=model_config[\"model_path\"],\n",
    "            model=\"qwen3-vl-4b\",\n",
    "            messages=messages,\n",
    "            temperature=model_config.get(\"temperature\", 0.0),\n",
    "            max_tokens=model_config.get(\"max_tokens\", 4096)\n",
    "        )\n",
    "        \n",
    "        # 6. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞\n",
    "        text = response.choices[0].message.content\n",
    "        tokens = response.usage.total_tokens if response.usage else 0\n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        logger.info(\n",
    "            f\"‚úÖ {img_path.name}: {original_size} ‚Üí {target_dim}px ‚Üí \"\n",
    "            f\"{base64_kb:.1f}KB ‚Üí {tokens:,} tok ‚Üí \"\n",
    "            f\"{len(text):,} —Å–∏–º–≤ –∑–∞ {processing_time:.2f}—Å\"\n",
    "        )\n",
    "        \n",
    "        return OCRResult(\n",
    "            model_name=model_name,\n",
    "            folder_name=folder_name,\n",
    "            filename=img_path.name,\n",
    "            page_number=page_num,\n",
    "            text=text,\n",
    "            tokens=tokens,\n",
    "            processing_time=processing_time,\n",
    "            text_length=len(text),\n",
    "            timestamp=datetime.now().isoformat(),\n",
    "            error=None\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        processing_time = time.time() - start_time\n",
    "        error_msg = str(e)\n",
    "        logger.error(f\"‚ùå {img_path.name}: {error_msg}\")\n",
    "        \n",
    "        return OCRResult(\n",
    "            model_name=model_name,\n",
    "            folder_name=folder_name,\n",
    "            filename=img_path.name,\n",
    "            page_number=page_num,\n",
    "            text=\"\",\n",
    "            tokens=0,\n",
    "            processing_time=processing_time,\n",
    "            text_length=0,\n",
    "            timestamp=datetime.now().isoformat(),\n",
    "            error=error_msg\n",
    "        )\n",
    "\n",
    "\n",
    "def ocr_with_retry(\n",
    "    img_path: Path,\n",
    "    folder_name: str,\n",
    "    model_name: str,\n",
    "    model_config: Dict[str, Any],\n",
    "    prompt_type: str,\n",
    "    page_num: int\n",
    ") -> OCRResult:\n",
    "    \"\"\"\n",
    "    Retry –ª–æ–≥–∏–∫–∞ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º —É–º–µ–Ω—å—à–µ–Ω–∏–µ–º —Ä–∞–∑–º–µ—Ä–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.\n",
    "    \"\"\"\n",
    "    max_retries = model_config.get(\"max_retries\", 3)\n",
    "    retry_scale = model_config.get(\"retry_scale_factor\", 0.8)\n",
    "    \n",
    "    config_copy = model_config.copy()\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        result = ocr_single_image(\n",
    "            img_path, folder_name, model_name, \n",
    "            config_copy, prompt_type, page_num\n",
    "        )\n",
    "        \n",
    "        if not result.error:\n",
    "            if attempt > 0:\n",
    "                logger.info(f\"‚úÖ {img_path.name}: –£—Å–ø–µ—Ö –Ω–∞ –ø–æ–ø—ã—Ç–∫–µ {attempt + 1}\")\n",
    "            return result\n",
    "        \n",
    "        # –û—à–∏–±–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ - —É–º–µ–Ω—å—à–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\n",
    "        if \"context\" in result.error.lower() or \"tokens\" in result.error.lower():\n",
    "            if attempt < max_retries - 1:\n",
    "                new_dim = int(config_copy[\"target_longest_dim\"] * retry_scale)\n",
    "                logger.warning(\n",
    "                    f\"‚ö†Ô∏è {img_path.name}: Retry {attempt + 1}/{max_retries}, \"\n",
    "                    f\"—É–º–µ–Ω—å—à–∞—é —Ä–∞–∑–º–µ—Ä {config_copy['target_longest_dim']}px ‚Üí {new_dim}px\"\n",
    "                )\n",
    "                config_copy[\"target_longest_dim\"] = new_dim\n",
    "                time.sleep(1)\n",
    "                continue\n",
    "        \n",
    "        if attempt < max_retries - 1:\n",
    "            logger.warning(f\"‚ö†Ô∏è {img_path.name}: Retry {attempt + 1}/{max_retries}\")\n",
    "            time.sleep(1)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def ocr_single_image_wrapper(args: Tuple) -> OCRResult:\n",
    "    \"\"\"Wrapper –¥–ª—è ThreadPoolExecutor\"\"\"\n",
    "    return ocr_with_retry(*args)\n",
    "\n",
    "\n",
    "def test_model_parallel(\n",
    "    model_name: str,\n",
    "    max_workers: int = 4\n",
    ") -> Optional[List[OCRResult]]:\n",
    "    \"\"\"\n",
    "    –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ LM Studio.\n",
    "    \n",
    "    –ù–ï –∑–∞–ø—É—Å–∫–∞–µ—Ç —Å–µ—Ä–≤–µ—Ä - —Ç–æ–ª—å–∫–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –£–ñ–ï –ó–ê–ü–£–©–ï–ù–ù–´–ô!\n",
    "    \"\"\"\n",
    "    \n",
    "    # –í–∞–ª–∏–¥–∞—Ü–∏—è\n",
    "    if 'IMAGE_PATHS' not in globals() or not IMAGE_PATHS:\n",
    "        print(f\"‚ùå –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã! –ó–∞–ø—É—Å—Ç–∏—Ç–µ –Ø—á–µ–π–∫—É 4\")\n",
    "        return None\n",
    "    \n",
    "    if model_name not in MODELS_CONFIG:\n",
    "        print(f\"‚ùå –ú–æ–¥–µ–ª—å {model_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ MODELS_CONFIG\")\n",
    "        return None\n",
    "    \n",
    "    model_config = MODELS_CONFIG[model_name]\n",
    "    \n",
    "    if not model_config[\"enabled\"]:\n",
    "        print(f\"‚è∏Ô∏è  {model_name} –æ—Ç–∫–ª—é—á–µ–Ω–∞\")\n",
    "        return None\n",
    "    \n",
    "    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ LM Studio\n",
    "    if not check_model_availability(model_config[\"port\"]):\n",
    "        print(f\"‚ùå LM Studio –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –Ω–∞ –ø–æ—Ä—Ç—É {model_config['port']}\")\n",
    "        print(f\"   üí° –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ:\")\n",
    "        print(f\"      1. LM Studio –∑–∞–ø—É—â–µ–Ω\")\n",
    "        print(f\"      2. –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ (–∫–Ω–æ–ø–∫–∞ 'Load Model')\")\n",
    "        print(f\"      3. –°–µ—Ä–≤–µ—Ä –∑–∞–ø—É—â–µ–Ω (–≤–∫–ª–∞–¥–∫–∞ 'Local Server')\")\n",
    "        return None\n",
    "    \n",
    "    # –ó–∞–≥–æ–ª–æ–≤–æ–∫\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"üöÄ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï: {model_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"‚úÖ LM Studio –¥–æ—Å—Ç—É–ø–µ–Ω –Ω–∞ –ø–æ—Ä—Ç—É {model_config['port']}\")\n",
    "    print(f\"üìä –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(IMAGE_PATHS)}\")\n",
    "    print(f\"‚ö° –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤: {max_workers}\")\n",
    "    print(f\"üñºÔ∏è  –†–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {model_config['target_longest_dim']}px\")\n",
    "    print(f\"üîÑ Retry: {model_config.get('max_retries', 3)} –ø–æ–ø—ã—Ç–∫–∏\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∑–∞–¥–∞—á\n",
    "    tasks = []\n",
    "    for folder_name, folder_paths in FOLDER_GROUPS.items():\n",
    "        for page_num, img_path in enumerate(folder_paths, 1):\n",
    "            task = (\n",
    "                img_path, folder_name, model_name,\n",
    "                model_config, PROMPT_TYPE, page_num\n",
    "            )\n",
    "            tasks.append(task)\n",
    "    \n",
    "    print(f\"üì¶ –ó–∞–¥–∞—á: {len(tasks)}\")\n",
    "    print(f\"   ~{len(tasks) / max_workers:.1f} –∑–∞–¥–∞—á –Ω–∞ worker\\n\")\n",
    "    \n",
    "    # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞\n",
    "    start_time = time.time()\n",
    "    results = []\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_task = {\n",
    "            executor.submit(ocr_single_image_wrapper, task): task\n",
    "            for task in tasks\n",
    "        }\n",
    "        \n",
    "        with tqdm(total=len(tasks), desc=f\"‚ö° {model_name}\", unit=\"img\") as pbar:\n",
    "            for future in as_completed(future_to_task):\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    results.append(result)\n",
    "                    \n",
    "                    status = \"‚úÖ\" if not result.error else \"‚ùå\"\n",
    "                    pbar.set_postfix({\n",
    "                        '—Ñ–∞–π–ª': result.filename[:18],\n",
    "                        '–≤—Ä–µ–º—è': f\"{result.processing_time:.1f}—Å\",\n",
    "                        '—Ç–æ–∫–µ–Ω—ã': f\"{result.tokens}\",\n",
    "                        '—Å–∏–º–≤–æ–ª–æ–≤': result.text_length,\n",
    "                        '—Å—Ç–∞—Ç—É—Å': status\n",
    "                    })\n",
    "                    \n",
    "                except Exception as exc:\n",
    "                    logger.error(f\"Worker exception: {exc}\")\n",
    "                    img_path = future_to_task[future][0]\n",
    "                    results.append(OCRResult(\n",
    "                        model_name=model_name,\n",
    "                        folder_name=future_to_task[future][1],\n",
    "                        filename=img_path.name,\n",
    "                        page_number=future_to_task[future][5],\n",
    "                        text=\"\",\n",
    "                        tokens=0,\n",
    "                        processing_time=0,\n",
    "                        text_length=0,\n",
    "                        timestamp=datetime.now().isoformat(),\n",
    "                        error=f\"Worker exception: {exc}\"\n",
    "                    ))\n",
    "                \n",
    "                pbar.update(1)\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ\n",
    "    ALL_RESULTS[model_name] = results\n",
    "    print(f\"\\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...\")\n",
    "    saved_files = save_results_to_files(results, OUTPUT_DIR, model_name)\n",
    "    \n",
    "    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n",
    "    stats = calculate_model_stats(results)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê {model_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"  –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π:      {stats.total_images}\")\n",
    "    print(f\"  –ü–∞–ø–æ–∫:            {stats.folders_processed}\")\n",
    "    print(f\"  –û–±—â–µ–µ –≤—Ä–µ–º—è:      {stats.total_time:.1f}—Å ({stats.total_time/60:.1f} –º–∏–Ω)\")\n",
    "    print(f\"  Throughput:       {stats.throughput:.2f} –∏–∑–æ–±—Ä/—Å\")\n",
    "    print(f\"  –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è:    {stats.avg_time_per_image:.2f}—Å/–∏–∑–æ–±—Ä\")\n",
    "    print(f\"  –¢–æ–∫–µ–Ω–æ–≤:          {stats.total_tokens:,}\")\n",
    "    print(f\"  –°–∏–º–≤–æ–ª–æ–≤:         {stats.total_chars:,}\")\n",
    "    print(f\"  –£—Å–ø–µ—à–Ω–æ:          {stats.success_rate:.1f}%\")\n",
    "    \n",
    "    if stats.errors > 0:\n",
    "        print(f\"  ‚ö†Ô∏è  –û—à–∏–±–æ–∫:       {stats.errors}\")\n",
    "    \n",
    "    if len(FOLDER_GROUPS) > 1:\n",
    "        print(f\"\\nüìÇ –ü–æ –ø–∞–ø–∫–∞–º:\")\n",
    "        for folder_name in FOLDER_GROUPS.keys():\n",
    "            folder_stats = calculate_folder_stats(results, folder_name)\n",
    "            if folder_stats:\n",
    "                print(f\"  {folder_name}: {folder_stats.total_images} –∏–∑–æ–±—Ä, \"\n",
    "                      f\"{folder_stats.avg_time_per_image:.2f}—Å, \"\n",
    "                      f\"{folder_stats.avg_chars_per_image:.0f} —Å–∏–º–≤\")\n",
    "    \n",
    "    print(f\"\\nüíæ –§–∞–π–ª—ã: {len(saved_files)}\")\n",
    "    for filepath in saved_files.values():\n",
    "        print(f\"  üìÑ {filepath.name}\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"‚úÖ –ó–ê–í–ï–†–®–ï–ù–û!\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def test_model(model_name: str, max_workers: int = None):\n",
    "    \"\"\"–ê–ª–∏–∞—Å —Å –∞–≤—Ç–æ–≤—ã–±–æ—Ä–æ–º workers\"\"\"\n",
    "    if max_workers is None:\n",
    "        if 'IMAGE_PATHS' in globals():\n",
    "            num_images = len(IMAGE_PATHS)\n",
    "            max_workers = 2 if num_images <= 4 else (4 if num_images <= 12 else 6)\n",
    "        else:\n",
    "            max_workers = 4\n",
    "    \n",
    "    return test_model_parallel(model_name, max_workers=max_workers)\n",
    "\n",
    "\n",
    "print(\"‚úÖ –Ø—á–µ–π–∫–∞ 8: –ö–ª–∏–µ–Ω—Ç –¥–ª—è LM Studio –æ–ø—Ä–µ–¥–µ–ª–µ–Ω\")\n",
    "print(\"   ‚ö†Ô∏è  –ù–ï –∑–∞–ø—É—Å–∫–∞–µ—Ç —Å–µ—Ä–≤–µ—Ä - —Ç–æ–ª—å–∫–æ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∑–∞–ø—Ä–æ—Å—ã!\")\n",
    "print(\"   ‚ö†Ô∏è  –ö–æ–Ω—Ç–µ–∫—Å—Ç –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç—Å—è –≤ GUI LM Studio\")\n",
    "print(\"\\nüí° –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:\")\n",
    "print('   results = test_model(\"olmOCR-2-7B-1025\")')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "905f3ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üöÄ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï: olmOCR-2-7B-1025\n",
      "======================================================================\n",
      "‚úÖ LM Studio –¥–æ—Å—Ç—É–ø–µ–Ω –Ω–∞ –ø–æ—Ä—Ç—É 1234\n",
      "üìä –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: 12\n",
      "‚ö° –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤: 4\n",
      "üñºÔ∏è  –†–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: 512px\n",
      "üîÑ Retry: 3 –ø–æ–ø—ã—Ç–∫–∏\n",
      "======================================================================\n",
      "\n",
      "üì¶ –ó–∞–¥–∞—á: 12\n",
      "   ~3.0 –∑–∞–¥–∞—á –Ω–∞ worker\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "‚ö° olmOCR-2-7B-1025:   0%|          | 0/12 [00:00<?, ?img/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-22 03:06:12,520 - INFO - HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "2025-12-22 03:06:12,521 - ERROR - ‚ùå page_002.png: Error code: 400 - {'error': 'Trying to keep the first 43362 tokens when context the overflows. However, the model is loaded with context length of only 32768 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}\n",
      "2025-12-22 03:06:12,521 - WARNING - ‚ö†Ô∏è page_002.png: Retry 1/3, —É–º–µ–Ω—å—à–∞—é —Ä–∞–∑–º–µ—Ä 512px ‚Üí 409px\n",
      "2025-12-22 03:06:40,770 - INFO - HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-22 03:06:40,775 - INFO - ‚úÖ page_004.png: (1211, 1713) ‚Üí 512px ‚Üí 35.4KB ‚Üí 26,279 tok ‚Üí 284 —Å–∏–º–≤ –∑–∞ 28.46—Å\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "‚ö° olmOCR-2-7B-1025:   8%|‚ñä         | 1/12 [00:28<05:13, 28.46s/img, —Ñ–∞–π–ª=page_004.png, –≤—Ä–µ–º—è=28.5—Å, —Ç–æ–∫–µ–Ω—ã=26279, —Å–∏–º–≤–æ–ª–æ–≤=284, —Å—Ç–∞—Ç—É—Å=‚úÖ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-22 03:06:40,835 - INFO - HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "2025-12-22 03:06:40,836 - ERROR - ‚ùå page_001.png: Error code: 400 - {'error': 'Trying to keep the first 43018 tokens when context the overflows. However, the model is loaded with context length of only 32768 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}\n",
      "2025-12-22 03:06:40,836 - WARNING - ‚ö†Ô∏è page_001.png: Retry 1/3, —É–º–µ–Ω—å—à–∞—é —Ä–∞–∑–º–µ—Ä 512px ‚Üí 409px\n",
      "2025-12-22 03:06:40,899 - INFO - HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "2025-12-22 03:06:40,900 - ERROR - ‚ùå page_003.png: Error code: 400 - {'error': 'Trying to keep the first 41445 tokens when context the overflows. However, the model is loaded with context length of only 32768 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}\n",
      "2025-12-22 03:06:40,900 - WARNING - ‚ö†Ô∏è page_003.png: Retry 1/3, —É–º–µ–Ω—å—à–∞—é —Ä–∞–∑–º–µ—Ä 512px ‚Üí 409px\n",
      "2025-12-22 03:07:12,727 - INFO - HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-22 03:07:12,728 - INFO - ‚úÖ page_002.png: (1211, 1713) ‚Üí 409px ‚Üí 37.8KB ‚Üí 28,223 tok ‚Üí 413 —Å–∏–º–≤ –∑–∞ 59.21—Å\n",
      "2025-12-22 03:07:12,728 - INFO - ‚úÖ page_002.png: –£—Å–ø–µ—Ö –Ω–∞ –ø–æ–ø—ã—Ç–∫–µ 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "‚ö° olmOCR-2-7B-1025:  17%|‚ñà‚ñã        | 2/12 [01:00<05:05, 30.51s/img, —Ñ–∞–π–ª=page_002.png, –≤—Ä–µ–º—è=59.2—Å, —Ç–æ–∫–µ–Ω—ã=28223, —Å–∏–º–≤–æ–ª–æ–≤=413, —Å—Ç–∞—Ç—É—Å=‚úÖ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-22 03:07:12,794 - INFO - HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "2025-12-22 03:07:12,795 - ERROR - ‚ùå page_001.png: Error code: 400 - {'error': 'Trying to keep the first 49020 tokens when context the overflows. However, the model is loaded with context length of only 32768 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}\n",
      "2025-12-22 03:07:12,795 - WARNING - ‚ö†Ô∏è page_001.png: Retry 1/3, —É–º–µ–Ω—å—à–∞—é —Ä–∞–∑–º–µ—Ä 512px ‚Üí 409px\n",
      "2025-12-22 03:07:43,890 - INFO - HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-22 03:07:43,891 - INFO - ‚úÖ page_001.png: (1211, 1713) ‚Üí 409px ‚Üí 36.6KB ‚Üí 27,499 tok ‚Üí 649 —Å–∏–º–≤ –∑–∞ 62.05—Å\n",
      "2025-12-22 03:07:43,891 - INFO - ‚úÖ page_001.png: –£—Å–ø–µ—Ö –Ω–∞ –ø–æ–ø—ã—Ç–∫–µ 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "‚ö° olmOCR-2-7B-1025:  25%|‚ñà‚ñà‚ñå       | 3/12 [01:31<04:37, 30.81s/img, —Ñ–∞–π–ª=page_001.png, –≤—Ä–µ–º—è=62.1—Å, —Ç–æ–∫–µ–Ω—ã=27499, —Å–∏–º–≤–æ–ª–æ–≤=649, —Å—Ç–∞—Ç—É—Å=‚úÖ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-22 03:08:13,535 - INFO - HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-22 03:08:13,536 - INFO - ‚úÖ page_003.png: (1211, 1713) ‚Üí 409px ‚Üí 35.4KB ‚Üí 26,501 tok ‚Üí 551 —Å–∏–º–≤ –∑–∞ 91.64—Å\n",
      "2025-12-22 03:08:13,536 - INFO - ‚úÖ page_003.png: –£—Å–ø–µ—Ö –Ω–∞ –ø–æ–ø—ã—Ç–∫–µ 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "‚ö° olmOCR-2-7B-1025:  33%|‚ñà‚ñà‚ñà‚ñé      | 4/12 [02:01<04:02, 30.35s/img, —Ñ–∞–π–ª=page_003.png, –≤—Ä–µ–º—è=91.6—Å, —Ç–æ–∫–µ–Ω—ã=26501, —Å–∏–º–≤–æ–ª–æ–≤=551, —Å—Ç–∞—Ç—É—Å=‚úÖ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-22 03:08:13,615 - INFO - HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "2025-12-22 03:08:13,616 - ERROR - ‚ùå page_002.png: Error code: 400 - {'error': 'Trying to keep the first 62970 tokens when context the overflows. However, the model is loaded with context length of only 32768 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}\n",
      "2025-12-22 03:08:13,616 - WARNING - ‚ö†Ô∏è page_002.png: Retry 1/3, —É–º–µ–Ω—å—à–∞—é —Ä–∞–∑–º–µ—Ä 512px ‚Üí 409px\n",
      "2025-12-22 03:08:50,178 - INFO - HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-22 03:08:50,179 - INFO - ‚úÖ page_001.png: (1241, 1755) ‚Üí 409px ‚Üí 41.4KB ‚Üí 30,839 tok ‚Üí 487 —Å–∏–º–≤ –∑–∞ 96.38—Å\n",
      "2025-12-22 03:08:50,179 - INFO - ‚úÖ page_001.png: –£—Å–ø–µ—Ö –Ω–∞ –ø–æ–ø—ã—Ç–∫–µ 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "‚ö° olmOCR-2-7B-1025:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 5/12 [02:37<03:48, 32.62s/img, —Ñ–∞–π–ª=page_001.png, –≤—Ä–µ–º—è=96.4—Å, —Ç–æ–∫–µ–Ω—ã=30839, —Å–∏–º–≤–æ–ª–æ–≤=487, —Å—Ç–∞—Ç—É—Å=‚úÖ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-22 03:08:50,250 - INFO - HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "2025-12-22 03:08:50,251 - ERROR - ‚ùå page_003.png: Error code: 400 - {'error': 'Trying to keep the first 50783 tokens when context the overflows. However, the model is loaded with context length of only 32768 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}\n",
      "2025-12-22 03:08:50,251 - WARNING - ‚ö†Ô∏è page_003.png: Retry 1/3, —É–º–µ–Ω—å—à–∞—é —Ä–∞–∑–º–µ—Ä 512px ‚Üí 409px\n",
      "2025-12-22 03:08:50,301 - INFO - HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "2025-12-22 03:08:50,301 - ERROR - ‚ùå page_001.png: Error code: 400 - {'error': 'Trying to keep the first 36068 tokens when context the overflows. However, the model is loaded with context length of only 32768 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}\n",
      "2025-12-22 03:08:50,301 - WARNING - ‚ö†Ô∏è page_001.png: Retry 1/3, —É–º–µ–Ω—å—à–∞—é —Ä–∞–∑–º–µ—Ä 512px ‚Üí 409px\n",
      "2025-12-22 03:08:50,351 - INFO - HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "2025-12-22 03:08:50,352 - ERROR - ‚ùå page_002.png: Error code: 400 - {'error': 'Trying to keep the first 37853 tokens when context the overflows. However, the model is loaded with context length of only 32768 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}\n",
      "2025-12-22 03:08:50,352 - WARNING - ‚ö†Ô∏è page_002.png: Retry 2/3, —É–º–µ–Ω—å—à–∞—é —Ä–∞–∑–º–µ—Ä 409px ‚Üí 327px\n",
      "2025-12-22 03:08:50,416 - INFO - HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "2025-12-22 03:08:50,417 - ERROR - ‚ùå page_002.png: Error code: 400 - {'error': 'Trying to keep the first 54344 tokens when context the overflows. However, the model is loaded with context length of only 32768 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}\n",
      "2025-12-22 03:08:50,417 - WARNING - ‚ö†Ô∏è page_002.png: Retry 1/3, —É–º–µ–Ω—å—à–∞—é —Ä–∞–∑–º–µ—Ä 512px ‚Üí 409px\n",
      "2025-12-22 03:09:28,823 - INFO - HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-22 03:09:28,824 - INFO - ‚úÖ page_003.png: (1241, 1755) ‚Üí 409px ‚Üí 42.1KB ‚Üí 31,549 tok ‚Üí 465 —Å–∏–º–≤ –∑–∞ 37.57—Å\n",
      "2025-12-22 03:09:28,824 - INFO - ‚úÖ page_003.png: –£—Å–ø–µ—Ö –Ω–∞ –ø–æ–ø—ã—Ç–∫–µ 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "‚ö° olmOCR-2-7B-1025:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 6/12 [03:16<03:28, 34.67s/img, —Ñ–∞–π–ª=page_003.png, –≤—Ä–µ–º—è=37.6—Å, —Ç–æ–∫–µ–Ω—ã=31549, —Å–∏–º–≤–æ–ª–æ–≤=465, —Å—Ç–∞—Ç—É—Å=‚úÖ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-22 03:11:16,142 - INFO - HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-22 03:11:16,143 - INFO - ‚úÖ page_001.png: (771, 1114) ‚Üí 409px ‚Üí 32.5KB ‚Üí 28,632 tok ‚Üí 5,655 —Å–∏–º–≤ –∑–∞ 144.84—Å\n",
      "2025-12-22 03:11:16,143 - INFO - ‚úÖ page_001.png: –£—Å–ø–µ—Ö –Ω–∞ –ø–æ–ø—ã—Ç–∫–µ 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "‚ö° olmOCR-2-7B-1025:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 7/12 [05:03<04:52, 58.42s/img, —Ñ–∞–π–ª=page_001.png, –≤—Ä–µ–º—è=144.8—Å, —Ç–æ–∫–µ–Ω—ã=28632, —Å–∏–º–≤–æ–ª–æ–≤=5655, —Å—Ç–∞—Ç—É—Å=‚úÖ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-22 03:11:41,508 - INFO - HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-22 03:11:41,508 - INFO - ‚úÖ page_002.png: (1241, 1755) ‚Üí 327px ‚Üí 32.4KB ‚Üí 24,444 tok ‚Üí 321 —Å–∏–º–≤ –∑–∞ 170.16—Å\n",
      "2025-12-22 03:11:41,509 - INFO - ‚úÖ page_002.png: –£—Å–ø–µ—Ö –Ω–∞ –ø–æ–ø—ã—Ç–∫–µ 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "‚ö° olmOCR-2-7B-1025:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 8/12 [05:29<03:11, 47.90s/img, —Ñ–∞–π–ª=page_002.png, –≤—Ä–µ–º—è=170.2—Å, —Ç–æ–∫–µ–Ω—ã=24444, —Å–∏–º–≤–æ–ª–æ–≤=321, —Å—Ç–∞—Ç—É—Å=‚úÖ] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-22 03:11:41,568 - INFO - HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "2025-12-22 03:11:41,569 - ERROR - ‚ùå page_002.png: Error code: 400 - {'error': 'Trying to keep the first 36159 tokens when context the overflows. However, the model is loaded with context length of only 32768 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}\n",
      "2025-12-22 03:11:41,569 - WARNING - ‚ö†Ô∏è page_002.png: Retry 2/3, —É–º–µ–Ω—å—à–∞—é —Ä–∞–∑–º–µ—Ä 409px ‚Üí 327px\n",
      "2025-12-22 03:11:41,622 - INFO - HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "2025-12-22 03:11:41,623 - ERROR - ‚ùå page_003.png: Error code: 400 - {'error': 'Trying to keep the first 38482 tokens when context the overflows. However, the model is loaded with context length of only 32768 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}\n",
      "2025-12-22 03:11:41,623 - WARNING - ‚ö†Ô∏è page_003.png: Retry 1/3, —É–º–µ–Ω—å—à–∞—é —Ä–∞–∑–º–µ—Ä 512px ‚Üí 409px\n",
      "2025-12-22 03:11:41,674 - INFO - HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "2025-12-22 03:11:41,674 - ERROR - ‚ùå page_004.png: Error code: 400 - {'error': 'Trying to keep the first 39386 tokens when context the overflows. However, the model is loaded with context length of only 32768 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}\n",
      "2025-12-22 03:11:41,674 - WARNING - ‚ö†Ô∏è page_004.png: Retry 1/3, —É–º–µ–Ω—å—à–∞—é —Ä–∞–∑–º–µ—Ä 512px ‚Üí 409px\n",
      "2025-12-22 03:11:41,739 - INFO - HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "2025-12-22 03:11:41,740 - ERROR - ‚ùå page_005.png: Error code: 400 - {'error': 'Trying to keep the first 57556 tokens when context the overflows. However, the model is loaded with context length of only 32768 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}\n",
      "2025-12-22 03:11:41,740 - WARNING - ‚ö†Ô∏è page_005.png: Retry 1/3, —É–º–µ–Ω—å—à–∞—é —Ä–∞–∑–º–µ—Ä 512px ‚Üí 409px\n",
      "2025-12-22 03:12:06,141 - INFO - HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-22 03:12:06,142 - INFO - ‚úÖ page_002.png: (778, 1105) ‚Üí 327px ‚Üí 30.6KB ‚Üí 22,997 tok ‚Üí 379 —Å–∏–º–≤ –∑–∞ 23.57—Å\n",
      "2025-12-22 03:12:06,142 - INFO - ‚úÖ page_002.png: –£—Å–ø–µ—Ö –Ω–∞ –ø–æ–ø—ã—Ç–∫–µ 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "‚ö° olmOCR-2-7B-1025:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 9/12 [05:53<02:01, 40.62s/img, —Ñ–∞–π–ª=page_002.png, –≤—Ä–µ–º—è=23.6—Å, —Ç–æ–∫–µ–Ω—ã=22997, —Å–∏–º–≤–æ–ª–æ–≤=379, —Å—Ç–∞—Ç—É—Å=‚úÖ] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-22 03:12:35,143 - INFO - HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-22 03:12:35,143 - INFO - ‚úÖ page_003.png: (778, 1105) ‚Üí 409px ‚Üí 35.0KB ‚Üí 26,271 tok ‚Üí 525 —Å–∏–º–≤ –∑–∞ 52.52—Å\n",
      "2025-12-22 03:12:35,144 - INFO - ‚úÖ page_003.png: –£—Å–ø–µ—Ö –Ω–∞ –ø–æ–ø—ã—Ç–∫–µ 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "‚ö° olmOCR-2-7B-1025:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 10/12 [06:22<01:14, 37.04s/img, —Ñ–∞–π–ª=page_003.png, –≤—Ä–µ–º—è=52.5—Å, —Ç–æ–∫–µ–Ω—ã=26271, —Å–∏–º–≤–æ–ª–æ–≤=525, —Å—Ç–∞—Ç—É—Å=‚úÖ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-22 03:13:03,645 - INFO - HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-22 03:13:03,645 - INFO - ‚úÖ page_004.png: (774, 1082) ‚Üí 409px ‚Üí 35.4KB ‚Üí 26,464 tok ‚Üí 299 —Å–∏–º–≤ –∑–∞ 80.97—Å\n",
      "2025-12-22 03:13:03,646 - INFO - ‚úÖ page_004.png: –£—Å–ø–µ—Ö –Ω–∞ –ø–æ–ø—ã—Ç–∫–µ 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "‚ö° olmOCR-2-7B-1025:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 11/12 [06:51<00:34, 34.42s/img, —Ñ–∞–π–ª=page_004.png, –≤—Ä–µ–º—è=81.0—Å, —Ç–æ–∫–µ–Ω—ã=26464, —Å–∏–º–≤–æ–ª–æ–≤=299, —Å—Ç–∞—Ç—É—Å=‚úÖ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-22 03:13:03,697 - INFO - HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "2025-12-22 03:13:03,697 - ERROR - ‚ùå page_005.png: Error code: 400 - {'error': 'Trying to keep the first 35593 tokens when context the overflows. However, the model is loaded with context length of only 32768 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}\n",
      "2025-12-22 03:13:03,698 - WARNING - ‚ö†Ô∏è page_005.png: Retry 2/3, —É–º–µ–Ω—å—à–∞—é —Ä–∞–∑–º–µ—Ä 409px ‚Üí 327px\n",
      "2025-12-22 03:14:50,414 - INFO - HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-22 03:14:50,415 - INFO - ‚úÖ page_005.png: (774, 1082) ‚Üí 327px ‚Üí 32.0KB ‚Üí 27,961 tok ‚Üí 6,213 —Å–∏–º–≤ –∑–∞ 105.72—Å\n",
      "2025-12-22 03:14:50,416 - INFO - ‚úÖ page_005.png: –£—Å–ø–µ—Ö –Ω–∞ –ø–æ–ø—ã—Ç–∫–µ 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "‚ö° olmOCR-2-7B-1025: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [08:38<00:00, 43.17s/img, —Ñ–∞–π–ª=page_005.png, –≤—Ä–µ–º—è=105.7—Å, —Ç–æ–∫–µ–Ω—ã=27961, —Å–∏–º–≤–æ–ª–æ–≤=6213, —Å—Ç–∞—Ç—É—Å=‚úÖ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...\n",
      "\n",
      "======================================================================\n",
      "üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê olmOCR-2-7B-1025\n",
      "======================================================================\n",
      "  –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π:      12\n",
      "  –ü–∞–ø–æ–∫:            3\n",
      "  –û–±—â–µ–µ –≤—Ä–µ–º—è:      953.1—Å (15.9 –º–∏–Ω)\n",
      "  Throughput:       0.01 –∏–∑–æ–±—Ä/—Å\n",
      "  –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è:    79.42—Å/–∏–∑–æ–±—Ä\n",
      "  –¢–æ–∫–µ–Ω–æ–≤:          327,659\n",
      "  –°–∏–º–≤–æ–ª–æ–≤:         16,241\n",
      "  –£—Å–ø–µ—à–Ω–æ:          100.0%\n",
      "\n",
      "üìÇ –ü–æ –ø–∞–ø–∫–∞–º:\n",
      "  o-predelnom-mnogomernom-raspredelenii: 4 –∏–∑–æ–±—Ä, 60.34—Å, 474 —Å–∏–º–≤\n",
      "  standartizatsiya-i-kachestvo-zhizni: 3 –∏–∑–æ–±—Ä, 101.37—Å, 424 —Å–∏–º–≤\n",
      "  –ö–Ω–∏–≥–∞_Python_–ö_–≤–µ—Ä—à–∏–Ω–∞–º_–º–∞—Å—Ç–µ—Ä—Å—Ç–≤–∞_–†–∞–º–∞–ª—å–æ_–õ—É—á–∞–Ω–æ: 5 –∏–∑–æ–±—Ä, 81.52—Å, 2614 —Å–∏–º–≤\n",
      "\n",
      "üíæ –§–∞–π–ª—ã: 5\n",
      "  üìÑ olmOCR-2-7B-1025_o-predelnom-mnogomernom-raspredelenii_20251222_031450.md\n",
      "  üìÑ olmOCR-2-7B-1025_standartizatsiya-i-kachestvo-zhizni_20251222_031450.md\n",
      "  üìÑ olmOCR-2-7B-1025_–ö–Ω–∏–≥–∞_Python_–ö_–≤–µ—Ä—à–∏–Ω–∞–º_–º–∞—Å—Ç–µ—Ä—Å—Ç–≤–∞_–†–∞–º–∞–ª—å–æ_–õ—É—á–∞–Ω–æ_20251222_031450.md\n",
      "  üìÑ olmOCR-2-7B-1025_results_20251222_031450.json\n",
      "  üìÑ olmOCR-2-7B-1025_stats_20251222_031450.csv\n",
      "\n",
      "======================================================================\n",
      "‚úÖ –ó–ê–í–ï–†–®–ï–ù–û!\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = test_model(\"olmOCR-2-7B-1025\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec205f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üöÄ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï: qwen3-vl-4b\n",
      "======================================================================\n",
      "‚úÖ LM Studio –¥–æ—Å—Ç—É–ø–µ–Ω –Ω–∞ –ø–æ—Ä—Ç—É 1234\n",
      "üìä –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: 12\n",
      "‚ö° –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤: 4\n",
      "üñºÔ∏è  –†–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: 512px\n",
      "üîÑ Retry: 3 –ø–æ–ø—ã—Ç–∫–∏\n",
      "======================================================================\n",
      "\n",
      "üì¶ –ó–∞–¥–∞—á: 12\n",
      "   ~3.0 –∑–∞–¥–∞—á –Ω–∞ worker\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "‚ö° qwen3-vl-4b:   0%|          | 0/12 [00:00<?, ?img/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-23 09:56:55,613 - INFO - Retrying request to /chat/completions in 0.463495 seconds\n",
      "2025-12-23 09:56:55,658 - INFO - Retrying request to /chat/completions in 0.401237 seconds\n",
      "2025-12-23 09:56:55,660 - INFO - Retrying request to /chat/completions in 0.416213 seconds\n",
      "2025-12-23 09:56:55,661 - INFO - Retrying request to /chat/completions in 0.480083 seconds\n",
      "2025-12-23 09:59:40,920 - INFO - HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-23 09:59:40,924 - INFO - ‚úÖ page_004.png: (1211, 1713) ‚Üí 512px ‚Üí 35.4KB ‚Üí 30,300 tok ‚Üí 6,491 —Å–∏–º–≤ –∑–∞ 345.48—Å\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "‚ö° qwen3-vl-4b:   8%|‚ñä         | 1/12 [05:45<1:03:20, 345.48s/img, —Ñ–∞–π–ª=page_004.png, –≤—Ä–µ–º—è=345.5—Å, —Ç–æ–∫–µ–Ω—ã=30300, —Å–∏–º–≤–æ–ª–æ–≤=6491, —Å—Ç–∞—Ç—É—Å=‚úÖ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-23 09:59:40,994 - INFO - HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "2025-12-23 09:59:40,995 - ERROR - ‚ùå page_003.png: Error code: 400 - {'error': 'Trying to keep the first 41434 tokens when context the overflows. However, the model is loaded with context length of only 33178 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}\n",
      "2025-12-23 09:59:40,995 - WARNING - ‚ö†Ô∏è page_003.png: Retry 1/3, —É–º–µ–Ω—å—à–∞—é —Ä–∞–∑–º–µ—Ä 512px ‚Üí 409px\n",
      "2025-12-23 09:59:41,054 - INFO - HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "2025-12-23 09:59:41,055 - ERROR - ‚ùå page_001.png: Error code: 400 - {'error': 'Trying to keep the first 43007 tokens when context the overflows. However, the model is loaded with context length of only 33178 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}\n",
      "2025-12-23 09:59:41,055 - WARNING - ‚ö†Ô∏è page_001.png: Retry 1/3, —É–º–µ–Ω—å—à–∞—é —Ä–∞–∑–º–µ—Ä 512px ‚Üí 409px\n",
      "2025-12-23 09:59:41,108 - INFO - HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "2025-12-23 09:59:41,109 - ERROR - ‚ùå page_002.png: Error code: 400 - {'error': 'Trying to keep the first 43351 tokens when context the overflows. However, the model is loaded with context length of only 33178 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}\n",
      "2025-12-23 09:59:41,109 - WARNING - ‚ö†Ô∏è page_002.png: Retry 1/3, —É–º–µ–Ω—å—à–∞—é —Ä–∞–∑–º–µ—Ä 512px ‚Üí 409px\n",
      "2025-12-23 09:59:41,175 - INFO - HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "2025-12-23 09:59:41,176 - ERROR - ‚ùå page_001.png: Error code: 400 - {'error': 'Trying to keep the first 49009 tokens when context the overflows. However, the model is loaded with context length of only 33178 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}\n",
      "2025-12-23 09:59:41,176 - WARNING - ‚ö†Ô∏è page_001.png: Retry 1/3, —É–º–µ–Ω—å—à–∞—é —Ä–∞–∑–º–µ—Ä 512px ‚Üí 409px\n",
      "2025-12-23 10:02:42,034 - INFO - Retrying request to /chat/completions in 0.409071 seconds\n",
      "2025-12-23 10:02:42,190 - INFO - Retrying request to /chat/completions in 0.474484 seconds\n",
      "2025-12-23 10:02:42,197 - INFO - Retrying request to /chat/completions in 0.473574 seconds\n",
      "2025-12-23 10:02:42,254 - INFO - Retrying request to /chat/completions in 0.409479 seconds\n",
      "2025-12-23 10:03:38,920 - INFO - HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-23 10:03:38,921 - INFO - ‚úÖ page_003.png: (1211, 1713) ‚Üí 409px ‚Üí 35.4KB ‚Üí 27,056 tok ‚Üí 2,678 —Å–∏–º–≤ –∑–∞ 236.93—Å\n",
      "2025-12-23 10:03:38,921 - INFO - ‚úÖ page_003.png: –£—Å–ø–µ—Ö –Ω–∞ –ø–æ–ø—ã—Ç–∫–µ 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "‚ö° qwen3-vl-4b:  17%|‚ñà‚ñã        | 2/12 [09:43<47:02, 282.25s/img, —Ñ–∞–π–ª=page_003.png, –≤—Ä–µ–º—è=236.9—Å, —Ç–æ–∫–µ–Ω—ã=27056, —Å–∏–º–≤–æ–ª–æ–≤=2678, —Å—Ç–∞—Ç—É—Å=‚úÖ]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-23 10:04:42,356 - INFO - HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-23 10:04:42,357 - INFO - ‚úÖ page_001.png: (1241, 1755) ‚Üí 409px ‚Üí 41.4KB ‚Üí 31,290 tok ‚Üí 2,051 —Å–∏–º–≤ –∑–∞ 300.18—Å\n",
      "2025-12-23 10:04:42,357 - INFO - ‚úÖ page_001.png: –£—Å–ø–µ—Ö –Ω–∞ –ø–æ–ø—ã—Ç–∫–µ 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "‚ö° qwen3-vl-4b:  25%|‚ñà‚ñà‚ñå       | 3/12 [10:46<27:21, 182.34s/img, —Ñ–∞–π–ª=page_001.png, –≤—Ä–µ–º—è=300.2—Å, —Ç–æ–∫–µ–Ω—ã=31290, —Å–∏–º–≤–æ–ª–æ–≤=2051, —Å—Ç–∞—Ç—É—Å=‚úÖ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-23 10:05:42,699 - INFO - Retrying request to /chat/completions in 0.775130 seconds\n",
      "2025-12-23 10:05:42,765 - INFO - Retrying request to /chat/completions in 0.951816 seconds\n",
      "2025-12-23 10:05:42,860 - INFO - HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "2025-12-23 10:05:42,860 - ERROR - ‚ùå page_002.png: Error code: 400 - {'error': 'Trying to keep the first 62959 tokens when context the overflows. However, the model is loaded with context length of only 33178 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}\n",
      "2025-12-23 10:05:42,861 - WARNING - ‚ö†Ô∏è page_002.png: Retry 1/3, —É–º–µ–Ω—å—à–∞—é —Ä–∞–∑–º–µ—Ä 512px ‚Üí 409px\n",
      "2025-12-23 10:05:42,931 - INFO - HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "2025-12-23 10:05:42,931 - ERROR - ‚ùå page_003.png: Error code: 400 - {'error': 'Trying to keep the first 50772 tokens when context the overflows. However, the model is loaded with context length of only 33178 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}\n",
      "2025-12-23 10:05:42,932 - WARNING - ‚ö†Ô∏è page_003.png: Retry 1/3, —É–º–µ–Ω—å—à–∞—é —Ä–∞–∑–º–µ—Ä 512px ‚Üí 409px\n",
      "2025-12-23 10:06:40,762 - INFO - HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-23 10:06:40,763 - INFO - ‚úÖ page_002.png: (1211, 1713) ‚Üí 409px ‚Üí 37.8KB ‚Üí 28,710 tok ‚Üí 2,244 —Å–∏–º–≤ –∑–∞ 418.65—Å\n",
      "2025-12-23 10:06:40,763 - INFO - ‚úÖ page_002.png: –£—Å–ø–µ—Ö –Ω–∞ –ø–æ–ø—ã—Ç–∫–µ 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "‚ö° qwen3-vl-4b:  33%|‚ñà‚ñà‚ñà‚ñé      | 4/12 [12:45<20:56, 157.10s/img, —Ñ–∞–π–ª=page_002.png, –≤—Ä–µ–º—è=418.7—Å, —Ç–æ–∫–µ–Ω—ã=28710, —Å–∏–º–≤–æ–ª–æ–≤=2244, —Å—Ç–∞—Ç—É—Å=‚úÖ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-23 10:08:43,818 - ERROR - ‚ùå page_001.png: Request timed out.\n",
      "2025-12-23 10:08:43,820 - WARNING - ‚ö†Ô∏è page_001.png: Retry 2/3\n",
      "2025-12-23 10:08:43,929 - INFO - HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "2025-12-23 10:08:43,929 - ERROR - ‚ùå page_002.png: Error code: 400 - {'error': 'Trying to keep the first 37842 tokens when context the overflows. However, the model is loaded with context length of only 33178 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}\n",
      "2025-12-23 10:08:43,930 - WARNING - ‚ö†Ô∏è page_002.png: Retry 2/3, —É–º–µ–Ω—å—à–∞—é —Ä–∞–∑–º–µ—Ä 409px ‚Üí 327px\n",
      "2025-12-23 10:08:44,082 - INFO - Retrying request to /chat/completions in 0.458515 seconds\n",
      "2025-12-23 10:08:44,189 - INFO - HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "2025-12-23 10:08:44,189 - ERROR - ‚ùå page_001.png: Error code: 400 - {'error': 'Trying to keep the first 36057 tokens when context the overflows. However, the model is loaded with context length of only 33178 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}\n",
      "2025-12-23 10:08:44,190 - WARNING - ‚ö†Ô∏è page_001.png: Retry 1/3, —É–º–µ–Ω—å—à–∞—é —Ä–∞–∑–º–µ—Ä 512px ‚Üí 409px\n",
      "2025-12-23 10:10:38,825 - INFO - HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "2025-12-23 10:10:38,825 - ERROR - ‚ùå page_003.png: Error code: 400 - {'error': 'Reached context length of 33178 tokens, but this model does not currently support mid-generation context overflow because llama_memory_can_shift is 0. Try reloading with a larger context length or shortening the prompt/chat.'}\n",
      "2025-12-23 10:10:38,826 - WARNING - ‚ö†Ô∏è page_003.png: Retry 2/3, —É–º–µ–Ω—å—à–∞—é —Ä–∞–∑–º–µ—Ä 409px ‚Üí 327px\n",
      "2025-12-23 10:11:44,967 - INFO - Retrying request to /chat/completions in 0.468886 seconds\n",
      "2025-12-23 10:11:45,067 - INFO - Retrying request to /chat/completions in 0.415304 seconds\n",
      "2025-12-23 10:11:45,316 - INFO - Retrying request to /chat/completions in 0.495801 seconds\n",
      "2025-12-23 10:13:39,963 - INFO - Retrying request to /chat/completions in 0.439666 seconds\n",
      "2025-12-23 10:14:45,537 - INFO - Retrying request to /chat/completions in 0.889042 seconds\n",
      "2025-12-23 10:14:45,583 - INFO - Retrying request to /chat/completions in 0.918638 seconds\n",
      "2025-12-23 10:14:45,914 - INFO - Retrying request to /chat/completions in 0.886171 seconds\n",
      "2025-12-23 10:16:40,504 - INFO - Retrying request to /chat/completions in 0.771315 seconds\n",
      "2025-12-23 10:17:46,528 - ERROR - ‚ùå page_001.png: Request timed out.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "‚ö° qwen3-vl-4b:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 5/12 [23:51<39:43, 340.53s/img, —Ñ–∞–π–ª=page_001.png, –≤—Ä–µ–º—è=541.7—Å, —Ç–æ–∫–µ–Ω—ã=0, —Å–∏–º–≤–æ–ª–æ–≤=0, —Å—Ç–∞—Ç—É—Å=‚ùå]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-23 10:17:46,603 - ERROR - ‚ùå page_002.png: Request timed out.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "‚ö° qwen3-vl-4b:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 5/12 [23:51<39:43, 340.53s/img, —Ñ–∞–π–ª=page_002.png, –≤—Ä–µ–º—è=541.7—Å, —Ç–æ–∫–µ–Ω—ã=0, —Å–∏–º–≤–æ–ª–æ–≤=0, —Å—Ç–∞—Ç—É—Å=‚ùå]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-23 10:17:46,901 - ERROR - ‚ùå page_001.png: Request timed out.\n",
      "2025-12-23 10:17:46,902 - WARNING - ‚ö†Ô∏è page_001.png: Retry 2/3\n",
      "2025-12-23 10:19:41,345 - ERROR - ‚ùå page_003.png: Request timed out.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "‚ö° qwen3-vl-4b:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 7/12 [25:45<16:24, 196.85s/img, —Ñ–∞–π–ª=page_003.png, –≤—Ä–µ–º—è=541.5—Å, —Ç–æ–∫–µ–Ω—ã=0, —Å–∏–º–≤–æ–ª–æ–≤=0, —Å—Ç–∞—Ç—É—Å=‚ùå]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-23 10:19:41,445 - INFO - HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "2025-12-23 10:19:41,446 - ERROR - ‚ùå page_002.png: Error code: 400 - {'error': 'Trying to keep the first 54333 tokens when context the overflows. However, the model is loaded with context length of only 33178 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}\n",
      "2025-12-23 10:19:41,446 - WARNING - ‚ö†Ô∏è page_002.png: Retry 1/3, —É–º–µ–Ω—å—à–∞—é —Ä–∞–∑–º–µ—Ä 512px ‚Üí 409px\n",
      "2025-12-23 10:19:41,488 - INFO - HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "2025-12-23 10:19:41,489 - ERROR - ‚ùå page_003.png: Error code: 400 - {'error': 'Trying to keep the first 38471 tokens when context the overflows. However, the model is loaded with context length of only 33178 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}\n",
      "2025-12-23 10:19:41,489 - WARNING - ‚ö†Ô∏è page_003.png: Retry 1/3, —É–º–µ–Ω—å—à–∞—é —Ä–∞–∑–º–µ—Ä 512px ‚Üí 409px\n",
      "2025-12-23 10:20:39,092 - INFO - HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-23 10:20:39,093 - INFO - ‚úÖ page_001.png: (771, 1114) ‚Üí 409px ‚Üí 32.5KB ‚Üí 25,359 tok ‚Üí 3,639 —Å–∏–º–≤ –∑–∞ 171.19—Å\n",
      "2025-12-23 10:20:39,094 - INFO - ‚úÖ page_001.png: –£—Å–ø–µ—Ö –Ω–∞ –ø–æ–ø—ã—Ç–∫–µ 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "‚ö° qwen3-vl-4b:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 8/12 [26:43<10:39, 159.85s/img, —Ñ–∞–π–ª=page_001.png, –≤—Ä–µ–º—è=171.2—Å, —Ç–æ–∫–µ–Ω—ã=25359, —Å–∏–º–≤–æ–ª–æ–≤=3639, —Å—Ç–∞—Ç—É—Å=‚úÖ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-23 10:20:39,160 - INFO - HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "2025-12-23 10:20:39,161 - ERROR - ‚ùå page_004.png: Error code: 400 - {'error': 'Trying to keep the first 39375 tokens when context the overflows. However, the model is loaded with context length of only 33178 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}\n",
      "2025-12-23 10:20:39,161 - WARNING - ‚ö†Ô∏è page_004.png: Retry 1/3, —É–º–µ–Ω—å—à–∞—é —Ä–∞–∑–º–µ—Ä 512px ‚Üí 409px\n",
      "2025-12-23 10:20:39,213 - INFO - HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "2025-12-23 10:20:39,214 - ERROR - ‚ùå page_002.png: Error code: 400 - {'error': 'Trying to keep the first 36148 tokens when context the overflows. However, the model is loaded with context length of only 33178 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}\n",
      "2025-12-23 10:20:39,214 - WARNING - ‚ö†Ô∏è page_002.png: Retry 2/3, —É–º–µ–Ω—å—à–∞—é —Ä–∞–∑–º–µ—Ä 409px ‚Üí 327px\n",
      "2025-12-23 10:21:29,128 - INFO - HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-23 10:21:29,129 - INFO - ‚úÖ page_003.png: (778, 1105) ‚Üí 409px ‚Üí 35.0KB ‚Üí 26,668 tok ‚Üí 2,151 —Å–∏–º–≤ –∑–∞ 106.64—Å\n",
      "2025-12-23 10:21:29,129 - INFO - ‚úÖ page_003.png: –£—Å–ø–µ—Ö –Ω–∞ –ø–æ–ø—ã—Ç–∫–µ 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "‚ö° qwen3-vl-4b:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 9/12 [27:33<06:28, 129.61s/img, —Ñ–∞–π–ª=page_003.png, –≤—Ä–µ–º—è=106.6—Å, —Ç–æ–∫–µ–Ω—ã=26668, —Å–∏–º–≤–æ–ª–æ–≤=2151, —Å—Ç–∞—Ç—É—Å=‚úÖ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-23 10:21:29,214 - INFO - HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "2025-12-23 10:21:29,217 - ERROR - ‚ùå page_005.png: Error code: 400 - {'error': 'Trying to keep the first 57545 tokens when context the overflows. However, the model is loaded with context length of only 33178 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}\n",
      "2025-12-23 10:21:29,218 - WARNING - ‚ö†Ô∏è page_005.png: Retry 1/3, —É–º–µ–Ω—å—à–∞—é —Ä–∞–∑–º–µ—Ä 512px ‚Üí 409px\n",
      "2025-12-23 10:23:40,234 - INFO - Retrying request to /chat/completions in 0.471458 seconds\n",
      "2025-12-23 10:23:40,343 - INFO - Retrying request to /chat/completions in 0.411593 seconds\n",
      "2025-12-23 10:23:40,498 - INFO - HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "2025-12-23 10:23:40,499 - ERROR - ‚ùå page_005.png: Error code: 400 - {'error': 'Trying to keep the first 35582 tokens when context the overflows. However, the model is loaded with context length of only 33178 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input'}\n",
      "2025-12-23 10:23:40,499 - WARNING - ‚ö†Ô∏è page_005.png: Retry 2/3, —É–º–µ–Ω—å—à–∞—é —Ä–∞–∑–º–µ—Ä 409px ‚Üí 327px\n",
      "2025-12-23 10:26:40,807 - INFO - Retrying request to /chat/completions in 0.940922 seconds\n",
      "2025-12-23 10:26:40,856 - INFO - Retrying request to /chat/completions in 0.869993 seconds\n",
      "2025-12-23 10:26:41,626 - INFO - Retrying request to /chat/completions in 0.447706 seconds\n",
      "2025-12-23 10:29:41,827 - ERROR - ‚ùå page_002.png: Request timed out.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "‚ö° qwen3-vl-4b:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 10/12 [35:46<07:44, 232.12s/img, —Ñ–∞–π–ª=page_002.png, –≤—Ä–µ–º—è=541.6—Å, —Ç–æ–∫–µ–Ω—ã=0, —Å–∏–º–≤–æ–ª–æ–≤=0, —Å—Ç–∞—Ç—É—Å=‚ùå]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-23 10:29:41,849 - ERROR - ‚ùå page_004.png: Request timed out.\n",
      "2025-12-23 10:29:41,851 - WARNING - ‚ö†Ô∏è page_004.png: Retry 2/3\n",
      "2025-12-23 10:29:42,175 - INFO - Retrying request to /chat/completions in 0.975415 seconds\n",
      "2025-12-23 10:32:42,980 - INFO - Retrying request to /chat/completions in 0.486897 seconds\n",
      "2025-12-23 10:32:43,251 - ERROR - ‚ùå page_005.png: Request timed out.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "‚ö° qwen3-vl-4b:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 11/12 [38:47<03:37, 217.55s/img, —Ñ–∞–π–ª=page_005.png, –≤—Ä–µ–º—è=541.8—Å, —Ç–æ–∫–µ–Ω—ã=0, —Å–∏–º–≤–æ–ª–æ–≤=0, —Å—Ç–∞—Ç—É—Å=‚ùå]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-23 10:33:09,867 - INFO - HTTP Request: POST http://localhost:1234/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-23 10:33:09,867 - INFO - ‚úÖ page_004.png: (774, 1082) ‚Üí 409px ‚Üí 35.4KB ‚Üí 26,903 tok ‚Üí 2,255 —Å–∏–º–≤ –∑–∞ 207.02—Å\n",
      "2025-12-23 10:33:09,868 - INFO - ‚úÖ page_004.png: –£—Å–ø–µ—Ö –Ω–∞ –ø–æ–ø—ã—Ç–∫–µ 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "‚ö° qwen3-vl-4b: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [39:14<00:00, 196.20s/img, —Ñ–∞–π–ª=page_004.png, –≤—Ä–µ–º—è=207.0—Å, —Ç–æ–∫–µ–Ω—ã=26903, —Å–∏–º–≤–æ–ª–æ–≤=2255, —Å—Ç–∞—Ç—É—Å=‚úÖ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...\n",
      "\n",
      "======================================================================\n",
      "üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê qwen3-vl-4b\n",
      "======================================================================\n",
      "  –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π:      12\n",
      "  –ü–∞–ø–æ–∫:            3\n",
      "  –û–±—â–µ–µ –≤—Ä–µ–º—è:      4494.4—Å (74.9 –º–∏–Ω)\n",
      "  Throughput:       0.00 –∏–∑–æ–±—Ä/—Å\n",
      "  –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è:    374.53—Å/–∏–∑–æ–±—Ä\n",
      "  –¢–æ–∫–µ–Ω–æ–≤:          196,286\n",
      "  –°–∏–º–≤–æ–ª–æ–≤:         21,509\n",
      "  –£—Å–ø–µ—à–Ω–æ:          58.3%\n",
      "  ‚ö†Ô∏è  –û—à–∏–±–æ–∫:       5\n",
      "\n",
      "üìÇ –ü–æ –ø–∞–ø–∫–∞–º:\n",
      "  o-predelnom-mnogomernom-raspredelenii: 4 –∏–∑–æ–±—Ä, 385.69—Å, 2853 —Å–∏–º–≤\n",
      "  standartizatsiya-i-kachestvo-zhizni: 3 –∏–∑–æ–±—Ä, 461.12—Å, 684 —Å–∏–º–≤\n",
      "  –ö–Ω–∏–≥–∞_Python_–ö_–≤–µ—Ä—à–∏–Ω–∞–º_–º–∞—Å—Ç–µ—Ä—Å—Ç–≤–∞_–†–∞–º–∞–ª—å–æ_–õ—É—á–∞–Ω–æ: 5 –∏–∑–æ–±—Ä, 313.64—Å, 1609 —Å–∏–º–≤\n",
      "\n",
      "üíæ –§–∞–π–ª—ã: 5\n",
      "  üìÑ qwen3-vl-4b_o-predelnom-mnogomernom-raspredelenii_20251223_103309.md\n",
      "  üìÑ qwen3-vl-4b_standartizatsiya-i-kachestvo-zhizni_20251223_103309.md\n",
      "  üìÑ qwen3-vl-4b_–ö–Ω–∏–≥–∞_Python_–ö_–≤–µ—Ä—à–∏–Ω–∞–º_–º–∞—Å—Ç–µ—Ä—Å—Ç–≤–∞_–†–∞–º–∞–ª—å–æ_–õ—É—á–∞–Ω–æ_20251223_103309.md\n",
      "  üìÑ qwen3-vl-4b_results_20251223_103309.json\n",
      "  üìÑ qwen3-vl-4b_stats_20251223_103309.csv\n",
      "\n",
      "======================================================================\n",
      "‚úÖ –ó–ê–í–ï–†–®–ï–ù–û!\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = test_model(\"qwen3-vl-4b\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "team-5-ai-tutor (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
