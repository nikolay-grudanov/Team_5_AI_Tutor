{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc045f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üîç –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø GPU\n",
      "======================================================================\n",
      "GPU: AMD Radeon Graphics\n",
      "VRAM: 16.0 GB\n",
      "–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: gfx1100\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory\n",
      "/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "–Ø–ß–ï–ô–ö–ê 1: –ù–∞—Å—Ç—Ä–æ–π–∫–∞ GPU (–ó–ê–ü–£–°–¢–ò–¢–¨ –ü–ï–†–í–û–ô!)\n",
    "\"\"\"\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # RX 7800 XT\n",
    "os.environ[\"HIP_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# –í–∫–ª—é—á–∏—Ç—å experimental Mem Efficient Attention –¥–ª—è AMD\n",
    "os.environ[\"TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL\"] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"HIP_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import torch\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üîç –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø GPU\")\n",
    "print(\"=\"*70)\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "print(f\"–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: {torch.cuda.get_device_properties(0).gcnArchName}\")\n",
    "\n",
    "# –î–æ–ª–∂–Ω–æ –±—ã—Ç—å:\n",
    "# GPU: AMD Radeon RX 7800 XT\n",
    "# VRAM: 16.0 GB\n",
    "# –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: gfx1100\n",
    "\n",
    "# assert \"7800 XT\" in torch.cuda.get_device_name(0), \"‚ùå –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è GPU!\"\n",
    "# print(\"‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è GPU –≤—ã–±—Ä–∞–Ω–∞!\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc1c31d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gna/anaconda3/envs/lightonocr/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏: /home/gna/workspase/education/MEPHI/Team_5_AI_Tutor/models/LightOnOCR-1B-1025\n",
      "\n",
      "1Ô∏è‚É£ –ó–∞–≥—Ä—É–∑–∫–∞ tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from '/home/gna/workspase/education/MEPHI/Team_5_AI_Tutor/models/LightOnOCR-1B-1025' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n",
      "You are using a model of type mistral3 to instantiate a model of type lighton_ocr. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ <class 'transformers.models.qwen2.tokenization_qwen2.Qwen2Tokenizer'>\n",
      "\n",
      "2Ô∏è‚É£ –ó–∞–≥—Ä—É–∑–∫–∞ image_processor...\n",
      "   ‚úÖ <class 'transformers.models.pixtral.image_processing_pixtral_fast.PixtralImageProcessorFast'>\n",
      "\n",
      "3Ô∏è‚É£ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 533/533 [00:00<00:00, 714.88it/s, Materializing param=model.vision_projection.patch_merger.merging_layer.weight]               \n",
      "The tied weights mapping and config for this model specifies to tie model.language_model.embed_tokens.weight to lm_head.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ –ú–æ–¥–µ–ª—å –Ω–∞ cuda:0\n",
      "======================================================================\n",
      "üéâ –í–°–Å –ó–ê–ì–†–£–ñ–ï–ù–û!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import time\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoImageProcessor\n",
    "from transformers.models.lighton_ocr.modeling_lighton_ocr import LightOnOcrForConditionalGeneration\n",
    "import tqdm as notebook_tqdm\n",
    "\n",
    "\n",
    "# –î–æ–±–∞–≤–∏—Ç—å –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ –≤ PYTHONPATH\n",
    "PROJECT_ROOT = Path.cwd().resolve()\n",
    "if PROJECT_ROOT.name != \"Team_5_AI_Tutor\":\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parents[1]\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "IMAGES_DIR = PROJECT_ROOT / \"notebooks\" / \"vlm-ingestion-pipeline\" / \"test_images\" / \"o-predelnom-mnogomernom-raspredelenii\"\n",
    "IMAGE_PATHS = sorted(IMAGES_DIR.glob(\"*.png\"))\n",
    "OUTPUT_DIR = DATA_DIR / \"test_results\"\n",
    "MODEL_PATH =  PROJECT_ROOT / \"models\" / \"LightOnOCR-1B-1025\"\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ø—É—Ç—å\n",
    "if not MODEL_PATH.exists():\n",
    "    raise FileNotFoundError(f\"–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {MODEL_PATH}\")\n",
    "\n",
    "print(f\"‚úÖ –ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏: {MODEL_PATH}\")\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã\n",
    "print(f\"\\n1Ô∏è‚É£ –ó–∞–≥—Ä—É–∑–∫–∞ tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(str(MODEL_PATH))\n",
    "print(f\"   ‚úÖ {type(tokenizer)}\")\n",
    "\n",
    "\n",
    "print(f\"\\n2Ô∏è‚É£ –ó–∞–≥—Ä—É–∑–∫–∞ image_processor...\")\n",
    "image_processor = AutoImageProcessor.from_pretrained(str(MODEL_PATH))\n",
    "print(f\"   ‚úÖ {type(image_processor)}\")\n",
    "\n",
    "print(f\"\\n3Ô∏è‚É£ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...\")\n",
    "model = LightOnOcrForConditionalGeneration.from_pretrained(\n",
    "    str(MODEL_PATH),\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map={\"\": 0},\n",
    "    attn_implementation=\"sdpa\"\n",
    ")\n",
    "model.eval()\n",
    "print(f\"   ‚úÖ –ú–æ–¥–µ–ª—å –Ω–∞ {next(model.parameters()).device}\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üéâ –í–°–Å –ó–ê–ì–†–£–ñ–ï–ù–û!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db63bda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üß™ –ü–†–û–í–ï–†–ö–ê –ú–û–î–ï–õ–ò\n",
      "======================================================================\n",
      "‚úÖ Device: cuda:0\n",
      "‚úÖ Dtype: torch.bfloat16\n",
      "\n",
      "üì¶ –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:\n",
      "  ‚úÖ Vision encoder: True\n",
      "  ‚úÖ Vision projection: True\n",
      "  ‚úÖ Language model: True\n",
      "  ‚úÖ LM head: True\n",
      "\n",
      "üíæ VRAM:\n",
      "  Allocated: 2.17 GB\n",
      "  Reserved: 2.17 GB\n",
      "\n",
      "üéâ –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "–ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç —á—Ç–æ –º–æ–¥–µ–ª—å —Ä–∞–±–æ—Ç–∞–µ—Ç\n",
    "\"\"\"\n",
    "import torch\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üß™ –ü–†–û–í–ï–†–ö–ê –ú–û–î–ï–õ–ò\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —á—Ç–æ –º–æ–¥–µ–ª—å –Ω–∞ GPU\n",
    "device = next(model.parameters()).device\n",
    "print(f\"‚úÖ Device: {device}\")\n",
    "print(f\"‚úÖ Dtype: {next(model.parameters()).dtype}\")\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —á—Ç–æ –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã\n",
    "print(f\"\\nüì¶ –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:\")\n",
    "print(f\"  ‚úÖ Vision encoder: {model.model.vision_encoder is not None}\")\n",
    "print(f\"  ‚úÖ Vision projection: {model.model.vision_projection is not None}\")\n",
    "print(f\"  ‚úÖ Language model: {model.model.language_model is not None}\")\n",
    "print(f\"  ‚úÖ LM head: {model.lm_head is not None}\")\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ø–∞–º—è—Ç—å GPU\n",
    "if torch.cuda.is_available():\n",
    "    allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
    "    reserved = torch.cuda.memory_reserved(0) / 1024**3\n",
    "    print(f\"\\nüíæ VRAM:\")\n",
    "    print(f\"  Allocated: {allocated:.2f} GB\")\n",
    "    print(f\"  Reserved: {reserved:.2f} GB\")\n",
    "\n",
    "print(f\"\\nüéâ –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é!\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6a5a2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üìù –¢–ï–°–¢ OCR\n",
      "======================================================================\n",
      "‚úÖ –ù–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: 4\n",
      "\n",
      "üì∑ –§–∞–π–ª: page_001.png\n",
      "   –†–∞–∑–º–µ—Ä: (1211, 1713)\n",
      "\n",
      "1Ô∏è‚É£ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è...\n",
      "   ‚úÖ pixel_values: torch.Size([1, 3, 1540, 1092])\n",
      "\n",
      "2Ô∏è‚É£ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ prompt...\n",
      "   ‚úÖ Chat template: <|im_start|>system<|im_end|>\n",
      "<|im_start|>user\n",
      "<|im...\n",
      "   ‚úÖ input_ids: torch.Size([1, 13])\n",
      "\n",
      "3Ô∏è‚É£ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/lightonocr/lib/python3.10/site-packages/transformers/utils/generic.py:952\u001b[0m, in \u001b[0;36mcheck_model_inputs.<locals>.wrapped_fn.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 952\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs_without_recordable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/lightonocr/lib/python3.10/site-packages/transformers/models/lighton_ocr/modeling_lighton_ocr.py:1100\u001b[0m, in \u001b[0;36mLightOnOcrModel.forward\u001b[0;34m(self, input_ids, pixel_values, image_sizes, inputs_embeds, position_ids, past_key_values, cache_position, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pixel_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1099\u001b[0m     \u001b[38;5;66;03m# Note: image_sizes is automatically expanded by the generation framework during beam search\u001b[39;00m\n\u001b[0;32m-> 1100\u001b[0m     image_features_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_image_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1101\u001b[0m     image_features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(image_features_list, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/lightonocr/lib/python3.10/site-packages/transformers/models/lighton_ocr/modeling_lighton_ocr.py:1046\u001b[0m, in \u001b[0;36mLightOnOcrModel.get_image_features\u001b[0;34m(self, pixel_values, image_sizes)\u001b[0m\n\u001b[1;32m   1044\u001b[0m visual_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvision_encoder(pixel_values, image_sizes\u001b[38;5;241m=\u001b[39mimage_sizes)\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[0;32m-> 1046\u001b[0m image_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvision_projection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvisual_features\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# Split features per image based on the effective patch size\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/lightonocr/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lightonocr/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/lightonocr/lib/python3.10/site-packages/transformers/models/lighton_ocr/modeling_lighton_ocr.py:126\u001b[0m, in \u001b[0;36mLightOnOcrVisionProjector.forward\u001b[0;34m(self, image_features, image_sizes)\u001b[0m\n\u001b[1;32m    125\u001b[0m image_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(image_features)\n\u001b[0;32m--> 126\u001b[0m image_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch_merger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_1(image_features)\n",
      "File \u001b[0;32m~/anaconda3/envs/lightonocr/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lightonocr/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/lightonocr/lib/python3.10/site-packages/transformers/models/lighton_ocr/modeling_lighton_ocr.py:78\u001b[0m, in \u001b[0;36mLightOnOcrPatchMerger.forward\u001b[0;34m(self, image_features, image_sizes)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, image_features: torch\u001b[38;5;241m.\u001b[39mTensor, image_sizes: Union[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;28mlist\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m---> 78\u001b[0m     image_sizes_in_patches \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     79\u001b[0m         (image_size[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_size, image_size[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_size) \u001b[38;5;28;01mfor\u001b[39;00m image_size \u001b[38;5;129;01min\u001b[39;00m image_sizes\n\u001b[1;32m     80\u001b[0m     ]\n\u001b[1;32m     82\u001b[0m     tokens_per_image \u001b[38;5;241m=\u001b[39m [patch_height \u001b[38;5;241m*\u001b[39m patch_width \u001b[38;5;28;01mfor\u001b[39;00m patch_height, patch_width \u001b[38;5;129;01min\u001b[39;00m image_sizes_in_patches]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/lightonocr/lib/python3.10/site-packages/transformers/utils/generic.py:952\u001b[0m, in \u001b[0;36mcheck_model_inputs.<locals>.wrapped_fn.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 952\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs_without_recordable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/lightonocr/lib/python3.10/site-packages/transformers/models/lighton_ocr/modeling_lighton_ocr.py:1143\u001b[0m, in \u001b[0;36mLightOnOcrForConditionalGeneration.forward\u001b[0;34m(self, input_ids, pixel_values, image_sizes, inputs_embeds, position_ids, past_key_values, cache_position, use_cache, labels, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;129m@check_model_inputs\u001b[39m\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[1;32m   1130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1141\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Unpack[TransformersKwargs],\n\u001b[1;32m   1142\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CausalLMOutputWithPast:\n\u001b[0;32m-> 1143\u001b[0m     outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1155\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(outputs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlast_hidden_state\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/lightonocr/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lightonocr/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/lightonocr/lib/python3.10/site-packages/transformers/utils/generic.py:954\u001b[0m, in \u001b[0;36mcheck_model_inputs.<locals>.wrapped_fn.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m--> 954\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m original_exception\n\u001b[1;32m    955\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    956\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing `**kwargs` in the signature of the `@check_model_inputs`-decorated function \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    957\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    958\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/lightonocr/lib/python3.10/site-packages/transformers/utils/generic.py:945\u001b[0m, in \u001b[0;36mcheck_model_inputs.<locals>.wrapped_fn.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    944\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 945\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[1;32m    948\u001b[0m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[1;32m    949\u001b[0m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/lightonocr/lib/python3.10/site-packages/transformers/models/lighton_ocr/modeling_lighton_ocr.py:1100\u001b[0m, in \u001b[0;36mLightOnOcrModel.forward\u001b[0;34m(self, input_ids, pixel_values, image_sizes, inputs_embeds, position_ids, past_key_values, cache_position, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pixel_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1099\u001b[0m     \u001b[38;5;66;03m# Note: image_sizes is automatically expanded by the generation framework during beam search\u001b[39;00m\n\u001b[0;32m-> 1100\u001b[0m     image_features_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_image_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1101\u001b[0m     image_features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(image_features_list, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/lightonocr/lib/python3.10/site-packages/transformers/models/lighton_ocr/modeling_lighton_ocr.py:1046\u001b[0m, in \u001b[0;36mLightOnOcrModel.get_image_features\u001b[0;34m(self, pixel_values, image_sizes)\u001b[0m\n\u001b[1;32m   1044\u001b[0m visual_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvision_encoder(pixel_values, image_sizes\u001b[38;5;241m=\u001b[39mimage_sizes)\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[0;32m-> 1046\u001b[0m image_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvision_projection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvisual_features\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# Split features per image based on the effective patch size\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/lightonocr/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lightonocr/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/lightonocr/lib/python3.10/site-packages/transformers/models/lighton_ocr/modeling_lighton_ocr.py:126\u001b[0m, in \u001b[0;36mLightOnOcrVisionProjector.forward\u001b[0;34m(self, image_features, image_sizes)\u001b[0m\n\u001b[1;32m    125\u001b[0m image_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(image_features)\n\u001b[0;32m--> 126\u001b[0m image_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch_merger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_1(image_features)\n",
      "File \u001b[0;32m~/anaconda3/envs/lightonocr/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lightonocr/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/lightonocr/lib/python3.10/site-packages/transformers/models/lighton_ocr/modeling_lighton_ocr.py:78\u001b[0m, in \u001b[0;36mLightOnOcrPatchMerger.forward\u001b[0;34m(self, image_features, image_sizes)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, image_features: torch\u001b[38;5;241m.\u001b[39mTensor, image_sizes: Union[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;28mlist\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m---> 78\u001b[0m     image_sizes_in_patches \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     79\u001b[0m         (image_size[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_size, image_size[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_size) \u001b[38;5;28;01mfor\u001b[39;00m image_size \u001b[38;5;129;01min\u001b[39;00m image_sizes\n\u001b[1;32m     80\u001b[0m     ]\n\u001b[1;32m     82\u001b[0m     tokens_per_image \u001b[38;5;241m=\u001b[39m [patch_height \u001b[38;5;241m*\u001b[39m patch_width \u001b[38;5;28;01mfor\u001b[39;00m patch_height, patch_width \u001b[38;5;129;01min\u001b[39;00m image_sizes_in_patches]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 61\u001b[0m\n\u001b[1;32m     58\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 61\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_inputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m generation_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   ‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgeneration_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m—Å\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/lightonocr/lib/python3.10/site-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lightonocr/lib/python3.10/site-packages/transformers/generation/utils.py:2684\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2681\u001b[0m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39muse_cache\n\u001b[1;32m   2683\u001b[0m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[0;32m-> 2684\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2685\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2686\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2687\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2689\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2690\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2691\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2692\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2694\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/lightonocr/lib/python3.10/site-packages/transformers/generation/utils.py:2877\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2870\u001b[0m model_forward \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2871\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_compiled_call(generation_config\u001b[38;5;241m.\u001b[39mcompile_config)\n\u001b[1;32m   2872\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_valid_auto_compile_criteria(model_kwargs, generation_config)\n\u001b[1;32m   2873\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m\n\u001b[1;32m   2874\u001b[0m )\n\u001b[1;32m   2876\u001b[0m prefill_consumed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 2877\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prefill\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2879\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_unfinished_sequences(this_peer_finished, synced_gpus, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice):\n\u001b[1;32m   2880\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m prefill_consumed:\n",
      "File \u001b[0;32m~/anaconda3/envs/lightonocr/lib/python3.10/site-packages/transformers/generation/utils.py:3853\u001b[0m, in \u001b[0;36mGenerationMixin._prefill\u001b[0;34m(self, input_ids, generation_config, model_kwargs)\u001b[0m\n\u001b[1;32m   3851\u001b[0m     model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_initial_cache_position(input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], input_ids\u001b[38;5;241m.\u001b[39mdevice, model_kwargs)\n\u001b[1;32m   3852\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m-> 3853\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3854\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Chunked prefill\u001b[39;00m\n\u001b[1;32m   3855\u001b[0m     \u001b[38;5;66;03m# Even if we are not compiling the forward, flex is always compiled when used. With chunked prefill, we may\u001b[39;00m\n\u001b[1;32m   3856\u001b[0m     \u001b[38;5;66;03m# end up needing just a bit more graphs than the default (which is 8). Doing this avoids very cryptic warnings\u001b[39;00m\n\u001b[1;32m   3857\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mcache_size_limit \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/lightonocr/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lightonocr/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/lightonocr/lib/python3.10/site-packages/transformers/utils/generic.py:954\u001b[0m, in \u001b[0;36mcheck_model_inputs.<locals>.wrapped_fn.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    952\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs_without_recordable)\n\u001b[1;32m    953\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m--> 954\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m original_exception\n\u001b[1;32m    955\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    956\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing `**kwargs` in the signature of the `@check_model_inputs`-decorated function \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    957\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    958\u001b[0m     )\n\u001b[1;32m    960\u001b[0m \u001b[38;5;66;03m# Restore original forward methods\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/lightonocr/lib/python3.10/site-packages/transformers/utils/generic.py:945\u001b[0m, in \u001b[0;36mcheck_model_inputs.<locals>.wrapped_fn.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    943\u001b[0m             outputs \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    944\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 945\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[1;32m    948\u001b[0m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[1;32m    949\u001b[0m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[1;32m    950\u001b[0m     kwargs_without_recordable \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "File \u001b[0;32m~/anaconda3/envs/lightonocr/lib/python3.10/site-packages/transformers/models/lighton_ocr/modeling_lighton_ocr.py:1143\u001b[0m, in \u001b[0;36mLightOnOcrForConditionalGeneration.forward\u001b[0;34m(self, input_ids, pixel_values, image_sizes, inputs_embeds, position_ids, past_key_values, cache_position, use_cache, labels, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;129m@check_model_inputs\u001b[39m\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[1;32m   1130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1141\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Unpack[TransformersKwargs],\n\u001b[1;32m   1142\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CausalLMOutputWithPast:\n\u001b[0;32m-> 1143\u001b[0m     outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1155\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(outputs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlast_hidden_state\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1156\u001b[0m     logits: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m~/anaconda3/envs/lightonocr/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lightonocr/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/lightonocr/lib/python3.10/site-packages/transformers/utils/generic.py:954\u001b[0m, in \u001b[0;36mcheck_model_inputs.<locals>.wrapped_fn.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    952\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs_without_recordable)\n\u001b[1;32m    953\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m--> 954\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m original_exception\n\u001b[1;32m    955\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    956\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing `**kwargs` in the signature of the `@check_model_inputs`-decorated function \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    957\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    958\u001b[0m     )\n\u001b[1;32m    960\u001b[0m \u001b[38;5;66;03m# Restore original forward methods\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/lightonocr/lib/python3.10/site-packages/transformers/utils/generic.py:945\u001b[0m, in \u001b[0;36mcheck_model_inputs.<locals>.wrapped_fn.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    943\u001b[0m             outputs \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    944\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 945\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[1;32m    948\u001b[0m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[1;32m    949\u001b[0m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[1;32m    950\u001b[0m     kwargs_without_recordable \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "File \u001b[0;32m~/anaconda3/envs/lightonocr/lib/python3.10/site-packages/transformers/models/lighton_ocr/modeling_lighton_ocr.py:1100\u001b[0m, in \u001b[0;36mLightOnOcrModel.forward\u001b[0;34m(self, input_ids, pixel_values, image_sizes, inputs_embeds, position_ids, past_key_values, cache_position, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m   1096\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlanguage_model\u001b[38;5;241m.\u001b[39mget_input_embeddings()(input_ids)\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pixel_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1099\u001b[0m     \u001b[38;5;66;03m# Note: image_sizes is automatically expanded by the generation framework during beam search\u001b[39;00m\n\u001b[0;32m-> 1100\u001b[0m     image_features_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_image_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1101\u001b[0m     image_features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(image_features_list, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   1102\u001b[0m     image_features \u001b[38;5;241m=\u001b[39m image_features\u001b[38;5;241m.\u001b[39mto(inputs_embeds\u001b[38;5;241m.\u001b[39mdevice, inputs_embeds\u001b[38;5;241m.\u001b[39mdtype)\n",
      "File \u001b[0;32m~/anaconda3/envs/lightonocr/lib/python3.10/site-packages/transformers/models/lighton_ocr/modeling_lighton_ocr.py:1046\u001b[0m, in \u001b[0;36mLightOnOcrModel.get_image_features\u001b[0;34m(self, pixel_values, image_sizes)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03mObtains image features from the vision encoder and projection.\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;124;03m    List of image feature tensors, one per image\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1044\u001b[0m visual_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvision_encoder(pixel_values, image_sizes\u001b[38;5;241m=\u001b[39mimage_sizes)\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[0;32m-> 1046\u001b[0m image_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvision_projection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvisual_features\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# Split features per image based on the effective patch size\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m downsample_ratio \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mvision_config\u001b[38;5;241m.\u001b[39mpatch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mspatial_merge_size\n",
      "File \u001b[0;32m~/anaconda3/envs/lightonocr/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lightonocr/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/lightonocr/lib/python3.10/site-packages/transformers/models/lighton_ocr/modeling_lighton_ocr.py:126\u001b[0m, in \u001b[0;36mLightOnOcrVisionProjector.forward\u001b[0;34m(self, image_features, image_sizes)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, image_features: torch\u001b[38;5;241m.\u001b[39mTensor, image_sizes: Union[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;28mlist\u001b[39m]):\n\u001b[1;32m    125\u001b[0m     image_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(image_features)\n\u001b[0;32m--> 126\u001b[0m     image_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch_merger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_1(image_features)\n\u001b[1;32m    128\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(hidden_states)\n",
      "File \u001b[0;32m~/anaconda3/envs/lightonocr/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/lightonocr/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/lightonocr/lib/python3.10/site-packages/transformers/models/lighton_ocr/modeling_lighton_ocr.py:78\u001b[0m, in \u001b[0;36mLightOnOcrPatchMerger.forward\u001b[0;34m(self, image_features, image_sizes)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, image_features: torch\u001b[38;5;241m.\u001b[39mTensor, image_sizes: Union[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;28mlist\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m---> 78\u001b[0m     image_sizes_in_patches \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     79\u001b[0m         (image_size[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_size, image_size[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_size) \u001b[38;5;28;01mfor\u001b[39;00m image_size \u001b[38;5;129;01min\u001b[39;00m image_sizes\n\u001b[1;32m     80\u001b[0m     ]\n\u001b[1;32m     82\u001b[0m     tokens_per_image \u001b[38;5;241m=\u001b[39m [patch_height \u001b[38;5;241m*\u001b[39m patch_width \u001b[38;5;28;01mfor\u001b[39;00m patch_height, patch_width \u001b[38;5;129;01min\u001b[39;00m image_sizes_in_patches]\n\u001b[1;32m     83\u001b[0m     hidden_dim \u001b[38;5;241m=\u001b[39m image_features\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "–Ø–ß–ï–ô–ö–ê 3: –¢–µ—Å—Ç OCR\n",
    "\"\"\"\n",
    "import torch\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìù –¢–ï–°–¢ OCR\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "\n",
    "if not IMAGE_PATHS:\n",
    "    print(f\"‚ùå –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ {IMAGES_DIR}\")\n",
    "    raise FileNotFoundError(f\"–ù–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ {IMAGES_DIR}\")\n",
    "\n",
    "print(f\"‚úÖ –ù–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(IMAGE_PATHS)}\")\n",
    "\n",
    "# –¢–µ—Å—Ç –Ω–∞ –ø–µ—Ä–≤–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏\n",
    "test_image = IMAGE_PATHS[0]\n",
    "image = Image.open(test_image).convert(\"RGB\")\n",
    "\n",
    "print(f\"\\nüì∑ –§–∞–π–ª: {test_image.name}\")\n",
    "print(f\"   –†–∞–∑–º–µ—Ä: {image.size}\")\n",
    "\n",
    "# –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\n",
    "print(f\"\\n1Ô∏è‚É£ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è...\")\n",
    "image_inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "pixel_values = image_inputs['pixel_values'].to(\"cuda:0\", dtype=torch.bfloat16)\n",
    "print(f\"   ‚úÖ pixel_values: {pixel_values.shape}\")\n",
    "\n",
    "# –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å —Ç–µ–∫—Å—Ç–æ–≤—ã–π prompt\n",
    "print(f\"\\n2Ô∏è‚É£ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ prompt...\")\n",
    "if hasattr(tokenizer, 'apply_chat_template'):\n",
    "    messages = [{\"role\": \"user\", \"content\": [{\"type\": \"image\"}]}]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    print(f\"   ‚úÖ Chat template: {prompt[:50]}...\")\n",
    "else:\n",
    "    # Fallback: –ø—Ä–æ—Å—Ç–æ–π prompt\n",
    "    prompt = \"\"\n",
    "    print(f\"   ‚ö†Ô∏è  Chat template –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—É—Å—Ç–æ–π prompt\")\n",
    "\n",
    "# –¢–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å\n",
    "text_inputs = tokenizer(\n",
    "    prompt,\n",
    "    return_tensors=\"pt\",\n",
    "    add_special_tokens=True\n",
    ").to(\"cuda:0\")\n",
    "print(f\"   ‚úÖ input_ids: {text_inputs['input_ids'].shape}\")\n",
    "\n",
    "# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è\n",
    "print(f\"\\n3Ô∏è‚É£ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞...\")\n",
    "start_time = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        input_ids=text_inputs['input_ids'],\n",
    "        pixel_values=pixel_values,\n",
    "        max_new_tokens=2000,\n",
    "        temperature=0.2,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "generation_time = time.time() - start_time\n",
    "print(f\"   ‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {generation_time:.2f}—Å\")\n",
    "\n",
    "# –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
    "print(f\"\\n4Ô∏è‚É£ –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ...\")\n",
    "input_length = text_inputs['input_ids'].shape[1]\n",
    "generated_text = tokenizer.decode(\n",
    "    outputs[0, input_length:],\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "# –†–µ–∑—É–ª—å—Ç–∞—Ç\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"üìù –†–ï–ó–£–õ–¨–¢–ê–¢ OCR\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"‚è±Ô∏è  –í—Ä–µ–º—è: {generation_time:.2f}—Å\")\n",
    "print(f\"üìä –°–∏–º–≤–æ–ª–æ–≤: {len(generated_text)}\")\n",
    "print(f\"üìä –¢–æ–∫–µ–Ω–æ–≤: {outputs.shape[1] - input_length}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
    "    print(f\"üíæ VRAM: {allocated:.2f} GB\")\n",
    "\n",
    "print(f\"\\nüìÑ –¢–ï–ö–°–¢:\")\n",
    "print(generated_text)\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "if len(generated_text) > 100:\n",
    "    print(f\"\\nüéâ –£–°–ü–ï–•! LightOnOCR —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ RX 7800 XT!\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  –†–µ–∑—É–ª—å—Ç–∞—Ç –∫–æ—Ä–æ—Ç–∫–∏–π ({len(generated_text)} —Å–∏–º–≤–æ–ª–æ–≤)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13d88c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üìù –¢–ï–°–¢ OCR (–§–ò–ù–ê–õ–¨–ù–ê–Ø –í–ï–†–°–ò–Ø)\n",
      "======================================================================\n",
      "\n",
      "üì∑ –§–∞–π–ª: page_001.png\n",
      "   –ò—Å—Ö–æ–¥–Ω—ã–π —Ä–∞–∑–º–µ—Ä: (1211, 1713)\n",
      "   ‚úÖ Resized –¥–æ: (565, 800)\n",
      "\n",
      "1Ô∏è‚É£ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è...\n",
      "   ‚úÖ pixel_values: torch.Size([1, 3, 812, 574])\n",
      "   ‚úÖ image_sizes: tensor([[800, 565]], device='cuda:0')\n",
      "\n",
      "   üìä Patches: 57√ó40 = 2280\n",
      "   üìä –ü–æ—Å–ª–µ merge: 570 tokens\n",
      "\n",
      "2Ô∏è‚É£ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ prompt...\n",
      "   ‚ö†Ô∏è  Image token –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥\n",
      "   ‚úÖ input_ids: torch.Size([1, 0])\n",
      "\n",
      "3Ô∏è‚É£ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞...\n",
      "   (–û–∂–∏–¥–∞–π—Ç–µ ~30-60 —Å–µ–∫—É–Ω–¥...)\n",
      "\n",
      "‚ùå –û–®–ò–ë–ö–ê: index -1 is out of bounds for dimension 0 with size 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1145959/112502302.py\", line 82, in <module>\n",
      "    outputs = model.generate(\n",
      "  File \"/home/gna/anaconda3/envs/lightonocr/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/gna/anaconda3/envs/lightonocr/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2684, in generate\n",
      "    result = decoding_method(\n",
      "  File \"/home/gna/anaconda3/envs/lightonocr/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2877, in _sample\n",
      "    outputs = self._prefill(input_ids, generation_config, model_kwargs)\n",
      "  File \"/home/gna/anaconda3/envs/lightonocr/lib/python3.10/site-packages/transformers/generation/utils.py\", line 3852, in _prefill\n",
      "    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
      "  File \"/home/gna/anaconda3/envs/lightonocr/lib/python3.10/site-packages/transformers/models/lighton_ocr/modeling_lighton_ocr.py\", line 1182, in prepare_inputs_for_generation\n",
      "    model_inputs = super().prepare_inputs_for_generation(\n",
      "  File \"/home/gna/anaconda3/envs/lightonocr/lib/python3.10/site-packages/transformers/generation/utils.py\", line 623, in prepare_inputs_for_generation\n",
      "    inputs_embeds, input_ids = self._cache_dependant_input_preparation(\n",
      "  File \"/home/gna/anaconda3/envs/lightonocr/lib/python3.10/site-packages/transformers/generation/utils.py\", line 519, in _cache_dependant_input_preparation\n",
      "    or (cache_position[-1] >= input_ids.shape[1])  # Exception 3\n",
      "IndexError: index -1 is out of bounds for dimension 0 with size 0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "–Ø–ß–ï–ô–ö–ê 3 –§–ò–ù–ê–õ–¨–ù–ê–Ø: OCR —Å –ü—Ä–∞–≤–∏–ª—å–Ω—ã–º Prompt\n",
    "\"\"\"\n",
    "import torch\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìù –¢–ï–°–¢ OCR (–§–ò–ù–ê–õ–¨–ù–ê–Ø –í–ï–†–°–ò–Ø)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\n",
    "test_image = IMAGE_PATHS[0]\n",
    "image = Image.open(test_image).convert(\"RGB\")\n",
    "\n",
    "print(f\"\\nüì∑ –§–∞–π–ª: {test_image.name}\")\n",
    "print(f\"   –ò—Å—Ö–æ–¥–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {image.size}\")\n",
    "\n",
    "# Resize\n",
    "max_size = 800\n",
    "if max(image.size) > max_size:\n",
    "    ratio = max_size / max(image.size)\n",
    "    new_size = (int(image.size[0] * ratio), int(image.size[1] * ratio))\n",
    "    image = image.resize(new_size, Image.Resampling.LANCZOS)\n",
    "    print(f\"   ‚úÖ Resized –¥–æ: {image.size}\")\n",
    "\n",
    "# –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\n",
    "print(f\"\\n1Ô∏è‚É£ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è...\")\n",
    "image_inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "pixel_values = image_inputs['pixel_values'].to(\"cuda:0\", dtype=torch.bfloat16)\n",
    "image_sizes = torch.tensor([[image.height, image.width]], device=\"cuda:0\")\n",
    "\n",
    "print(f\"   ‚úÖ pixel_values: {pixel_values.shape}\")\n",
    "print(f\"   ‚úÖ image_sizes: {image_sizes}\")\n",
    "\n",
    "# –í—ã—á–∏—Å–ª–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ image tokens\n",
    "# –§–æ—Ä–º—É–ª–∞: (height // patch_size) * (width // patch_size) // spatial_merge_size^2\n",
    "patch_size = model.config.vision_config.patch_size  # –æ–±—ã—á–Ω–æ 14\n",
    "spatial_merge = model.config.spatial_merge_size  # –æ–±—ã—á–Ω–æ 2\n",
    "\n",
    "h_patches = image.height // patch_size\n",
    "w_patches = image.width // patch_size\n",
    "n_image_tokens = (h_patches * w_patches) // (spatial_merge * spatial_merge)\n",
    "\n",
    "print(f\"\\n   üìä Patches: {h_patches}√ó{w_patches} = {h_patches * w_patches}\")\n",
    "print(f\"   üìä –ü–æ—Å–ª–µ merge: {n_image_tokens} tokens\")\n",
    "\n",
    "# –°–æ–∑–¥–∞—Ç—å prompt —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º image tokens\n",
    "print(f\"\\n2Ô∏è‚É£ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ prompt...\")\n",
    "\n",
    "# –ü–æ–ª—É—á–∏—Ç—å image token\n",
    "image_token = \"<|image|>\"\n",
    "image_token_id = tokenizer.convert_tokens_to_ids(image_token)\n",
    "\n",
    "if image_token_id == tokenizer.unk_token_id:\n",
    "    # Fallback: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π —Ç–æ–∫–µ–Ω\n",
    "    print(f\"   ‚ö†Ô∏è  Image token –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥\")\n",
    "    # –°–æ–∑–¥–∞—Ç—å prompt –±–µ–∑ image tokens (–º–æ–¥–µ–ª—å —Å–∞–º–∞ –≤—Å—Ç–∞–≤–∏—Ç)\n",
    "    prompt = \"\"\n",
    "    input_ids = tokenizer(\"\", return_tensors=\"pt\", add_special_tokens=False)[\"input_ids\"].to(\"cuda:0\")\n",
    "else:\n",
    "    # –°–æ–∑–¥–∞—Ç—å prompt —Å N image tokens\n",
    "    prompt = image_token * n_image_tokens\n",
    "    print(f\"   ‚úÖ Prompt: {len(prompt)} —Å–∏–º–≤–æ–ª–æ–≤ ({n_image_tokens} image tokens)\")\n",
    "    \n",
    "    input_ids = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        add_special_tokens=False\n",
    "    )[\"input_ids\"].to(\"cuda:0\")\n",
    "\n",
    "print(f\"   ‚úÖ input_ids: {input_ids.shape}\")\n",
    "\n",
    "# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è\n",
    "print(f\"\\n3Ô∏è‚É£ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞...\")\n",
    "print(f\"   (–û–∂–∏–¥–∞–π—Ç–µ ~30-60 —Å–µ–∫—É–Ω–¥...)\")\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            pixel_values=pixel_values,\n",
    "            image_sizes=image_sizes,\n",
    "            max_new_tokens=512,\n",
    "            temperature=0.2,\n",
    "            top_p=0.9,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    generation_time = time.time() - start_time\n",
    "    print(f\"   ‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {generation_time:.2f}—Å\")\n",
    "    \n",
    "    # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
    "    input_length = input_ids.shape[1]\n",
    "    generated_text = tokenizer.decode(\n",
    "        outputs[0, input_length:],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    # –†–µ–∑—É–ª—å—Ç–∞—Ç\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üìù –†–ï–ó–£–õ–¨–¢–ê–¢ OCR\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"‚è±Ô∏è  –í—Ä–µ–º—è: {generation_time:.2f}—Å\")\n",
    "    print(f\"üìä –°–∏–º–≤–æ–ª–æ–≤: {len(generated_text)}\")\n",
    "    print(f\"üìä –¢–æ–∫–µ–Ω–æ–≤: {outputs.shape[1] - input_length}\")\n",
    "    print(f\"üíæ VRAM: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "    \n",
    "    print(f\"\\nüìÑ –¢–ï–ö–°–¢:\")\n",
    "    print(generated_text)\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    if len(generated_text) > 50:\n",
    "        print(f\"\\nüéâ –£–°–ü–ï–•! LightOnOCR —Ä–∞–±–æ—Ç–∞–µ—Ç\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  –†–µ–∑—É–ª—å—Ç–∞—Ç –∫–æ—Ä–æ—Ç–∫–∏–π\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå –û–®–ò–ë–ö–ê: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "707f566d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå LightOnOcrProcessor –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: transformers.tokenization_utils_tokenizers.TokenizersBackend._patch_mistral_regex() got multiple values for keyword argument 'fix_mistral_regex'\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥ —á–µ—Ä–µ–∑ Processor\n",
    "\"\"\"\n",
    "from transformers.models.lighton_ocr.processing_lighton_ocr import LightOnOcrProcessor\n",
    "\n",
    "# –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø–æ–ª–Ω—ã–π processor\n",
    "try:\n",
    "    processor = LightOnOcrProcessor.from_pretrained(str(MODEL_PATH))\n",
    "    print(f\"‚úÖ LightOnOcrProcessor –∑–∞–≥—Ä—É–∂–µ–Ω\")\n",
    "    \n",
    "    # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å processor –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ inputs\n",
    "    inputs = processor(\n",
    "        images=image,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda:0\")\n",
    "    \n",
    "    inputs[\"pixel_values\"] = inputs[\"pixel_values\"].to(torch.bfloat16)\n",
    "    \n",
    "    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            temperature=0.2,\n",
    "            do_sample=False,\n",
    "        )\n",
    "    \n",
    "    # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
    "    text = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"üìù –¢–ï–ö–°–¢: {text}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå LightOnOcrProcessor –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bbd9c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è LightOnOCR:\n",
      "  image_token_id: 151655\n",
      "  patch_size: 14\n",
      "  spatial_merge_size: 2\n",
      "\n",
      "üìã –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã:\n",
      "  vocab_size: 151643\n",
      "  eos_token: <|im_end|> (id: 151645)\n",
      "  pad_token: <|endoftext|> (id: 151643)\n",
      "  bos_token: None (id: None)\n",
      "\n",
      "üìã Image-related tokens: ['.BackgroundImage', 'ƒ†setBackgroundImage', '.ImageIcon', 'imageUrl', '.StretchImage', 'ƒ†loadImage', 'ƒ†imagen', '_images', 'ImageUrl', ',image']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏\n",
    "\"\"\"\n",
    "import json\n",
    "\n",
    "config_path = MODEL_PATH / \"config.json\"\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "print(\"üìã –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è LightOnOCR:\")\n",
    "print(f\"  image_token_id: {config.get('image_token_id', '–ù–ï –ù–ê–ô–î–ï–ù')}\")\n",
    "print(f\"  patch_size: {config['vision_config']['patch_size']}\")\n",
    "print(f\"  spatial_merge_size: {config.get('spatial_merge_size', '–ù–ï –ù–ê–ô–î–ï–ù')}\")\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å tokenizer\n",
    "print(f\"\\nüìã –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã:\")\n",
    "print(f\"  vocab_size: {tokenizer.vocab_size}\")\n",
    "print(f\"  eos_token: {tokenizer.eos_token} (id: {tokenizer.eos_token_id})\")\n",
    "print(f\"  pad_token: {tokenizer.pad_token} (id: {tokenizer.pad_token_id})\")\n",
    "print(f\"  bos_token: {tokenizer.bos_token} (id: {tokenizer.bos_token_id})\")\n",
    "\n",
    "# –ù–∞–π—Ç–∏ image token –≤ vocab\n",
    "vocab = tokenizer.get_vocab()\n",
    "image_tokens = [k for k in vocab.keys() if 'image' in k.lower()]\n",
    "print(f\"\\nüìã Image-related tokens: {image_tokens[:10]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ef78f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üöÄ LIGHTONOCR - –§–ò–ù–ê–õ–¨–ù–´–ô –¢–ï–°–¢\n",
      "======================================================================\n",
      "\n",
      "üì∑ –§–∞–π–ª: page_001.png\n",
      "   –†–∞–∑–º–µ—Ä: (1211, 1713)\n",
      "   ‚úÖ Resized: (542, 768)\n",
      "\n",
      "1Ô∏è‚É£ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è...\n",
      "   ‚úÖ pixel_values: torch.Size([1, 3, 770, 546])\n",
      "   ‚úÖ image_sizes: [[768, 542]]\n",
      "   üìä Patches: 54√ó38\n",
      "   üìä Image tokens: 513\n",
      "\n",
      "2Ô∏è‚É£ –°–æ–∑–¥–∞–Ω–∏–µ input_ids...\n",
      "   ‚úÖ input_ids: torch.Size([1, 513])\n",
      "   ‚úÖ image_token_id: 151655\n",
      "\n",
      "3Ô∏è‚É£ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞...\n",
      "   (–û–∂–∏–¥–∞–Ω–∏–µ ~30-60 —Å–µ–∫—É–Ω–¥ –Ω–∞ RX 7800 XT...)\n",
      "   ‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ 17.75—Å\n",
      "\n",
      "======================================================================\n",
      "‚ú® –†–ï–ó–£–õ–¨–¢–ê–¢ OCR ‚ú®\n",
      "======================================================================\n",
      "‚è±Ô∏è  –í—Ä–µ–º—è: 17.75—Å\n",
      "üìä –°–∏–º–≤–æ–ª–æ–≤: 1644\n",
      "üìä –¢–æ–∫–µ–Ω–æ–≤: 1024\n",
      "‚ö° –°–∫–æ—Ä–æ—Å—Ç—å: 57.7 tok/s\n",
      "üíæ VRAM: 2.20 GB\n",
      "\n",
      "üìÑ –†–ê–°–ü–û–ó–ù–ê–ù–ù–´–ô –¢–ï–ö–°–¢:\n",
      "======================================================================\n",
      "raz****************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************ÔøΩ~**ÔøΩ~**ÔøΩ~**ÔøΩ~**ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ~ÔøΩ\n",
      "======================================================================\n",
      "\n",
      "üéâ üéâ üéâ –£–°–ü–ï–•! üéâ üéâ üéâ\n",
      "\n",
      "‚úÖ LightOnOCR —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ AMD RX 7800 XT!\n",
      "‚úÖ ROCm + PyTorch —É—Å–ø–µ—à–Ω–æ —Å–ø—Ä–∞–≤–∏–ª–∏—Å—å!\n",
      "\n",
      "üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:\n",
      "   ‚Ä¢ –í—Ä–µ–º—è: 17.75—Å\n",
      "   ‚Ä¢ –°–∫–æ—Ä–æ—Å—Ç—å: 57.7 —Ç–æ–∫–µ–Ω–æ–≤/—Å\n",
      "   ‚Ä¢ VRAM: 2.20 GB / 16 GB\n",
      "\n",
      "üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤: ocr_result.txt\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "–Ø–ß–ï–ô–ö–ê: –†–ê–ë–û–ß–ò–ô OCR –° –ü–†–ê–í–ò–õ–¨–ù–´–ú image_token_id\n",
    "\"\"\"\n",
    "import torch\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üöÄ LIGHTONOCR - –§–ò–ù–ê–õ–¨–ù–´–ô –¢–ï–°–¢\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\n",
    "test_image = IMAGE_PATHS[0]\n",
    "image = Image.open(test_image).convert(\"RGB\")\n",
    "\n",
    "print(f\"\\nüì∑ –§–∞–π–ª: {test_image.name}\")\n",
    "print(f\"   –†–∞–∑–º–µ—Ä: {image.size}\")\n",
    "\n",
    "# Resize\n",
    "max_size = 768\n",
    "if max(image.size) > max_size:\n",
    "    ratio = max_size / max(image.size)\n",
    "    new_size = (int(image.size[0] * ratio), int(image.size[1] * ratio))\n",
    "    image = image.resize(new_size, Image.Resampling.LANCZOS)\n",
    "    print(f\"   ‚úÖ Resized: {image.size}\")\n",
    "\n",
    "# –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
    "print(f\"\\n1Ô∏è‚É£ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è...\")\n",
    "image_inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "pixel_values = image_inputs['pixel_values'].to(\"cuda:0\", dtype=torch.bfloat16)\n",
    "\n",
    "# image_sizes –∫–∞–∫ LIST (–Ω–µ tensor!)\n",
    "image_sizes = [[image.height, image.width]]\n",
    "\n",
    "print(f\"   ‚úÖ pixel_values: {pixel_values.shape}\")\n",
    "print(f\"   ‚úÖ image_sizes: {image_sizes}\")\n",
    "\n",
    "# –í—ã—á–∏—Å–ª–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ image tokens\n",
    "patch_size = 14\n",
    "spatial_merge = 2\n",
    "\n",
    "h_patches = image.height // patch_size\n",
    "w_patches = image.width // patch_size\n",
    "n_image_tokens = (h_patches * w_patches) // (spatial_merge ** 2)\n",
    "\n",
    "print(f\"   üìä Patches: {h_patches}√ó{w_patches}\")\n",
    "print(f\"   üìä Image tokens: {n_image_tokens}\")\n",
    "\n",
    "# –°–æ–∑–¥–∞—Ç—å input_ids —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º image_token_id\n",
    "print(f\"\\n2Ô∏è‚É£ –°–æ–∑–¥–∞–Ω–∏–µ input_ids...\")\n",
    "\n",
    "# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å image_token_id –∏–∑ config\n",
    "image_token_id = 151655  # –ò–∑ –≤–∞—à–µ–≥–æ config\n",
    "\n",
    "# –ü–†–û–í–ï–†–ö–ê: —É–±–µ–¥–∏—Ç—å—Å—è —á—Ç–æ –º–æ–¥–µ–ª—å –∑–Ω–∞–µ—Ç —ç—Ç–æ—Ç —Ç–æ–∫–µ–Ω\n",
    "if image_token_id >= len(tokenizer):\n",
    "    print(f\"   ‚ö†Ô∏è  image_token_id ({image_token_id}) > vocab_size ({len(tokenizer)})\")\n",
    "    print(f\"   ‚úÖ –†–∞—Å—à–∏—Ä—è–µ–º embeddings...\")\n",
    "    \n",
    "    # –†–∞—Å—à–∏—Ä–∏—Ç—å embeddings –º–æ–¥–µ–ª–∏\n",
    "    model.resize_token_embeddings(image_token_id + 1)\n",
    "    print(f\"   ‚úÖ –ù–æ–≤—ã–π —Ä–∞–∑–º–µ—Ä: {len(tokenizer)} ‚Üí {image_token_id + 1}\")\n",
    "\n",
    "# –°–æ–∑–¥–∞—Ç—å input_ids\n",
    "input_ids = torch.full(\n",
    "    (1, n_image_tokens),\n",
    "    image_token_id,\n",
    "    dtype=torch.long,\n",
    "    device=\"cuda:0\"\n",
    ")\n",
    "\n",
    "print(f\"   ‚úÖ input_ids: {input_ids.shape}\")\n",
    "print(f\"   ‚úÖ image_token_id: {image_token_id}\")\n",
    "\n",
    "# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è\n",
    "print(f\"\\n3Ô∏è‚É£ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞...\")\n",
    "print(f\"   (–û–∂–∏–¥–∞–Ω–∏–µ ~30-60 —Å–µ–∫—É–Ω–¥ –Ω–∞ RX 7800 XT...)\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            pixel_values=pixel_values,\n",
    "            image_sizes=image_sizes,  # LIST!\n",
    "            max_new_tokens=1024,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    generation_time = time.time() - start_time\n",
    "    print(f\"   ‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {generation_time:.2f}—Å\")\n",
    "    \n",
    "    # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
    "    input_length = input_ids.shape[1]\n",
    "    generated_text = tokenizer.decode(\n",
    "        outputs[0, input_length:],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    # –†–µ–∑—É–ª—å—Ç–∞—Ç\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"‚ú® –†–ï–ó–£–õ–¨–¢–ê–¢ OCR ‚ú®\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"‚è±Ô∏è  –í—Ä–µ–º—è: {generation_time:.2f}—Å\")\n",
    "    print(f\"üìä –°–∏–º–≤–æ–ª–æ–≤: {len(generated_text)}\")\n",
    "    print(f\"üìä –¢–æ–∫–µ–Ω–æ–≤: {outputs.shape[1] - input_length}\")\n",
    "    print(f\"‚ö° –°–∫–æ—Ä–æ—Å—Ç—å: {(outputs.shape[1] - input_length) / generation_time:.1f} tok/s\")\n",
    "    print(f\"üíæ VRAM: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "    \n",
    "    print(f\"\\nüìÑ –†–ê–°–ü–û–ó–ù–ê–ù–ù–´–ô –¢–ï–ö–°–¢:\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(generated_text[:2000])  # –ü–µ—Ä–≤—ã–µ 2000 —Å–∏–º–≤–æ–ª–æ–≤\n",
    "    if len(generated_text) > 2000:\n",
    "        print(f\"\\n... (–µ—â—ë {len(generated_text) - 2000} —Å–∏–º–≤–æ–ª–æ–≤)\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    if len(generated_text) > 50:\n",
    "        print(f\"\\nüéâ üéâ üéâ –£–°–ü–ï–•! üéâ üéâ üéâ\")\n",
    "        print(f\"\")\n",
    "        print(f\"‚úÖ LightOnOCR —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ AMD RX 7800 XT!\")\n",
    "        print(f\"‚úÖ ROCm + PyTorch —É—Å–ø–µ—à–Ω–æ —Å–ø—Ä–∞–≤–∏–ª–∏—Å—å!\")\n",
    "        print(f\"\")\n",
    "        print(f\"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:\")\n",
    "        print(f\"   ‚Ä¢ –í—Ä–µ–º—è: {generation_time:.2f}—Å\")\n",
    "        print(f\"   ‚Ä¢ –°–∫–æ—Ä–æ—Å—Ç—å: {(outputs.shape[1] - input_length) / generation_time:.1f} —Ç–æ–∫–µ–Ω–æ–≤/—Å\")\n",
    "        print(f\"   ‚Ä¢ VRAM: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB / 16 GB\")\n",
    "        \n",
    "        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
    "        output_file = Path(\"ocr_result.txt\")\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"–§–∞–π–ª: {test_image.name}\\n\")\n",
    "            f.write(f\"–í—Ä–µ–º—è: {generation_time:.2f}—Å\\n\")\n",
    "            f.write(f\"–°–∏–º–≤–æ–ª–æ–≤: {len(generated_text)}\\n\")\n",
    "            f.write(f\"\\n{'='*70}\\n\")\n",
    "            f.write(generated_text)\n",
    "        print(f\"\\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤: {output_file}\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  –†–µ–∑—É–ª—å—Ç–∞—Ç –∫–æ—Ä–æ—Ç–∫–∏–π ({len(generated_text)} —Å–∏–º–≤–æ–ª–æ–≤)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå –û–®–ò–ë–ö–ê: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    print(f\"\\nüîç Debug info:\")\n",
    "    print(f\"   input_ids: {input_ids.shape}\")\n",
    "    print(f\"   pixel_values: {pixel_values.shape}\")\n",
    "    print(f\"   image_sizes: {image_sizes}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lightonocr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
