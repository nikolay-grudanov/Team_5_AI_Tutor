{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GTv0Jrl0Rv1J"
   },
   "source": [
    "## Local Inference on GPU\n",
    "Model page: https://huggingface.co/intfloat/multilingual-e5-base\n",
    "\n",
    "‚ö†Ô∏è If the generated code snippets do not work, please open an issue on either the [model repo](https://huggingface.co/intfloat/multilingual-e5-base)\n",
    "\t\t\tand/or on [huggingface.js](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries-snippets.ts) üôè"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.16\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TORCH_BLAS_PREFER_HIPBLASLT'] = '0'\n",
    "os.environ['PYTORCH_TUNABLEOP_ENABLED'] = '0'\n",
    "\n",
    "# –ó–∞—Ç–µ–º –∏–º–ø–æ—Ä—Ç–∏—Ä—É–π—Ç–µ torch –∏ –¥—Ä—É–≥–∏–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üîç –ü–†–û–í–ï–†–ö–ê GPU\n",
      "======================================================================\n",
      "PyTorch –≤–µ—Ä—Å–∏—è: 2.5.1+rocm6.2\n",
      "ROCm –≤–µ—Ä—Å–∏—è: 6.2.41133-dd7f95766\n",
      "–î–æ—Å—Ç—É–ø–Ω—ã—Ö GPU: 1\n",
      "\n",
      "GPU 0:\n",
      "  –ò–º—è: AMD Radeon RX 7800 XT\n",
      "  VRAM: 16.0 GB\n",
      "  –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: gfx1100\n",
      "  Compute Units: 30\n",
      "\n",
      "‚úÖ –¢–µ–∫—É—â–∞—è GPU: 0\n",
      "‚úÖ –ò–º—è: AMD Radeon RX 7800 XT\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üîç –ü–†–û–í–ï–†–ö–ê GPU\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"PyTorch –≤–µ—Ä—Å–∏—è: {torch.__version__}\")\n",
    "print(f\"ROCm –≤–µ—Ä—Å–∏—è: {torch.version.hip}\")\n",
    "print(f\"–î–æ—Å—Ç—É–ø–Ω—ã—Ö GPU: {torch.cuda.device_count()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"\\nGPU {i}:\")\n",
    "        print(f\"  –ò–º—è: {props.name}\")\n",
    "        print(f\"  VRAM: {props.total_memory / 1024**3:.1f} GB\")\n",
    "        print(f\"  –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: {props.gcnArchName}\")\n",
    "        print(f\"  Compute Units: {props.multi_processor_count}\")\n",
    "\n",
    "    print(f\"\\n‚úÖ –¢–µ–∫—É—â–∞—è GPU: {torch.cuda.current_device()}\")\n",
    "    print(f\"‚úÖ –ò–º—è: {torch.cuda.get_device_name()}\")\n",
    "else:\n",
    "    print(\"‚ùå CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞!\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "MEaBlyqORv1K"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"intfloat/multilingual-e5-base\")\n",
    "\n",
    "sentences = [\n",
    "    \"The weather is lovely today.\",\n",
    "    \"It's so sunny outside!\",\n",
    "    \"He drove to the stadium.\"\n",
    "]\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "similarities = model.similarity(embeddings, embeddings)\n",
    "print(similarities.shape)\n",
    "# [3, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T1K3awr-Rv1L"
   },
   "source": [
    "## Remote Inference via Inference Providers\n",
    "Ensure you have a valid **HF_TOKEN** set in your environment. You can get your token from [your settings page](https://huggingface.co/settings/tokens). Note: running this may incur charges above the free tier.\n",
    "The following Python example shows how to run the model remotely on HF Inference Providers, automatically selecting an available inference provider for you.\n",
    "For more information on how to use the Inference Providers, please refer to our [documentation and guides](https://huggingface.co/docs/inference-providers/en/index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º .env —Ñ–∞–π–ª\n",
    "load_dotenv()\n",
    "hf_key = os.getenv(\"HF_KEY\", \"EMPTY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "odl8BcDCRv1L"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9053035974502563, 0.9653543829917908, 0.8199769854545593]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(\n",
    "    provider=\"auto\",\n",
    "    api_key=hf_key,\n",
    ")\n",
    "\n",
    "result = client.sentence_similarity(\n",
    "    \"That is a happy person\",  # –ü–µ—Ä–≤—ã–π –∞—Ä–≥—É–º–µ–Ω—Ç: –∏—Å—Ö–æ–¥–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ\n",
    "    other_sentences=[          # –í—Ç–æ—Ä–æ–π –∞—Ä–≥—É–º–µ–Ω—Ç: —Å–ø–∏—Å–æ–∫ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\n",
    "        \"That is a happy dog\",\n",
    "        \"That is a very happy person\",\n",
    "        \"Today is a sunny day\"\n",
    "    ],\n",
    "    model=\"intfloat/multilingual-e5-base\"\n",
    ")\n",
    "\n",
    "print(result)\n",
    "# –í—ã–≤–æ–¥: [0.xx, 0.yy, 0.zz] - –æ—Ü–µ–Ω–∫–∏ —Å—Ö–æ–∂–µ—Å—Ç–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
