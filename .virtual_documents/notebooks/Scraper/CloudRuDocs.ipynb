import os
import sys
import json
import base64
import logging
import time
import csv
from pathlib import Path
from io import BytesIO
from datetime import datetime
from typing import List, Dict, Optional, Tuple, Any
from dataclasses import dataclass, field, asdict




# ============================================================================
# –ì–õ–û–ë–ê–õ–¨–ù–´–ï –ü–ï–†–ï–ú–ï–ù–ù–´–ï
# ============================================================================
# –î–æ–±–∞–≤–∏—Ç—å –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ –≤ PYTHONPATH
PROJECT_ROOT = Path.cwd().resolve()
if PROJECT_ROOT.name != "Team_5_AI_Tutor":
    PROJECT_ROOT = PROJECT_ROOT.parents[1]
sys.path.insert(0, str(PROJECT_ROOT))

DATA_DIR = PROJECT_ROOT / "data"
OUTPUT_DIR = DATA_DIR / "raw" / "cloud_ru"

# –°–æ–∑–¥–∞—ë–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

print("‚úÖ –Ø—á–µ–π–∫–∞ 1: –ò–º–ø–æ—Ä—Ç—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
print(f"   üìÇ –ö–æ—Ä–Ω–µ–≤–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {PROJECT_ROOT}")
print(f"   üìÇ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {OUTPUT_DIR}")



import asyncio
from playwright.async_api import async_playwright
from pathlib import Path
import json
from datetime import datetime
import re
from urllib.parse import urlparse
from PIL import Image

class CloudRuDocsScraper:
    def __init__(self, output_dir=OUTPUT_DIR, hide_sidebar=True, 
                 scale_factor=0.75, jpeg_quality=80):
        """
        Args:
            scale_factor: –ú–∞—Å—à—Ç–∞–± DPI (0.5 = 50%, 0.75 = 75%, 1.0 = 100%)
            jpeg_quality: –ö–∞—á–µ—Å—Ç–≤–æ JPEG (1-100, —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 70-85)
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.base_url = "https://cloud.ru/docs/tutorials-evolution/list/doc-contents"
        self.domain = "https://cloud.ru"
        self.hide_sidebar = hide_sidebar
        self.scale_factor = scale_factor
        self.jpeg_quality = jpeg_quality
        
    def sanitize_filename(self, url):
        parsed = urlparse(url)
        path = parsed.path.strip('/')
        filename = path.replace('/', '_')
        filename = re.sub(r'[<>:"|?*]', '', filename)
        if len(filename) > 200:
            filename = filename[:200]
        return filename or "index"
    
    def extract_section_info(self, url):
        parsed = urlparse(url)
        path_parts = parsed.path.strip('/').split('/')
        
        section_info = {
            'category': path_parts[2] if len(path_parts) >= 3 else None,
            'subcategory': path_parts[4] if len(path_parts) >= 5 else None,
            'topic': path_parts[5] if len(path_parts) >= 6 else None
        }
        return section_info
    
    async def collect_tutorial_links(self, page):
        print("–ó–∞–≥—Ä—É–∂–∞–µ–º –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É...")
        await page.goto(self.base_url, wait_until='networkidle')
        await page.wait_for_selector('a', timeout=10000)
        
        links = await page.evaluate(f'''() => {{
            const domain = "{self.domain}";
            const links = Array.from(document.querySelectorAll('a'));
            return links
                .map(link => link.href)
                .filter(href => 
                    href && 
                    href.startsWith(domain) && 
                    href.includes('/docs/') &&
                    get_ipython().getoutput("href.includes('#')")
                );
        }}''')
        
        unique_links = sorted(list(set(links)))
        print(f"–ù–∞–π–¥–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫: {len(unique_links)}")
        return unique_links
    
    async def hide_sidebar_elements(self, page):
        if not self.hide_sidebar:
            return
        
        await page.evaluate('''() => {
            function isInsideMainContent(el) {
                const main = document.querySelector('main');
                const article = document.querySelector('article');
                return (main && main.contains(el)) || (article && article.contains(el));
            }
            
            // –õ–µ–≤–æ–µ –º–µ–Ω—é
            const navs = document.querySelectorAll('nav');
            navs.forEach(nav => {
                if (!nav.closest('main, article')) {
                    nav.style.display = 'none';
                }
            });
            
            // –°–∞–π–¥–±–∞—Ä—ã
            document.querySelectorAll('[class*="sidebar"]').forEach(el => {
                if (!isInsideMainContent(el)) el.style.display = 'none';
            });
            
            // –ü—Ä–∞–≤–∞—è –ø–∞–Ω–µ–ª—å
            document.querySelectorAll('aside').forEach(aside => {
                if (!isInsideMainContent(aside)) aside.style.display = 'none';
            });
            
            // –í–µ—Ä—Ö–Ω–∏–π —Ö–µ–¥–µ—Ä
            document.querySelectorAll('header').forEach(header => {
                if (!isInsideMainContent(header)) header.style.display = 'none';
            });
            
            // –•–ª–µ–±–Ω—ã–µ –∫—Ä–æ—à–∫–∏
            document.querySelectorAll('[class*="breadcrumb"]').forEach(el => {
                el.style.display = 'none';
            });
            
            // –§—É—Ç–µ—Ä
            document.querySelectorAll('footer').forEach(f => f.style.display = 'none');
            
            // "–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø–æ–ª–µ–∑–Ω–∞?"
            document.querySelectorAll('*').forEach(el => {
                if (el.textContent.includes('–≠—Ç–∞ —Å—Ç–∞—Ç—å—è –ø–æ–ª–µ–∑–Ω–∞') && 
                    el.textContent.length < 100) {
                    let parent = el.closest('div, section');
                    if (parent) parent.style.display = 'none';
                }
            });
            
            // –†–∞—Å—à–∏—Ä—è–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
            const main = document.querySelector('main');
            if (main) {
                main.style.marginLeft = '0';
                main.style.marginRight = '0';
                main.style.maxWidth = '100%';
                main.style.width = '100%';
                main.style.padding = '40px';
            }
        }''')
        
        await page.wait_for_timeout(500)
    
    async def get_page_title(self, page):
        try:
            title = await page.evaluate('''() => {
                const h1 = document.querySelector('h1');
                return h1 ? h1.textContent.trim() : '';
            }''')
            return title or "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞"
        except:
            return "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞"
    
    async def expand_all_containers(self, page):
        """–†–∞–∑–≤–æ—Ä–∞—á–∏–≤–∞–µ—Ç –í–°–ï –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã –Ω–∞ –ø–æ–ª–Ω—É—é –≤—ã—Å–æ—Ç—É"""
        result = await page.evaluate("""
            async () => {
                document.documentElement.style.height = 'auto';
                document.documentElement.style.overflow = 'visible';
                document.body.style.height = 'auto';
                document.body.style.minHeight = '100%';
                document.body.style.overflow = 'visible';
                
                const appWrapper = document.querySelector('.styles_app_wrapper__Xfzgu');
                if (appWrapper) {
                    appWrapper.style.height = 'auto';
                    appWrapper.style.minHeight = '100%';
                    appWrapper.style.overflow = 'visible';
                }
                
                const layoutWrapper = document.querySelector('.styles_layout__PDzxx');
                if (layoutWrapper) {
                    layoutWrapper.style.height = 'auto';
                    layoutWrapper.style.overflow = 'visible';
                }
                
                const main = document.querySelector('main');
                if (main) {
                    main.style.height = 'auto';
                    main.style.overflow = 'visible';
                }
                
                const wrappers = document.querySelectorAll(
                    '.styles_page_wrapper__FHeMA, ' +
                    '.styles_sectionWrapper__tHdBi, ' +
                    '.styles_contentWrapper__LTcY9, ' +
                    '.styles_article_wrapper__3p7lX, ' +
                    '.styles_content_with_bread_crumbs__4pp9u'
                );
                
                wrappers.forEach(wrapper => {
                    wrapper.style.height = 'auto';
                    wrapper.style.maxHeight = 'none';
                    wrapper.style.overflow = 'visible';
                });
                
                const scrollContainer = document.querySelector('[data-overlayscrollbars-viewport]');
                const contentContainer = document.querySelector('[data-overlayscrollbars-contents]');
                
                if (scrollContainer) {
                    scrollContainer.style.height = 'auto';
                    scrollContainer.style.maxHeight = 'none';
                    scrollContainer.style.overflow = 'visible';
                    scrollContainer.style.position = 'static';
                }
                
                if (contentContainer) {
                    contentContainer.style.transform = 'none';
                    contentContainer.style.height = 'auto';
                    contentContainer.style.position = 'static';
                    contentContainer.style.top = '0';
                    contentContainer.style.left = '0';
                    contentContainer.style.margin = '0';
                    contentContainer.style.padding = '0';
                }
                
                const articleScroll = document.querySelector('.styles_article_scroll__ELGYB');
                if (articleScroll) {
                    articleScroll.style.height = 'auto';
                    articleScroll.style.maxHeight = 'none';
                    articleScroll.style.overflow = 'visible';
                }
                
                await new Promise(resolve => setTimeout(resolve, 1000));
                
                const finalHeight = Math.max(
                    document.body.scrollHeight,
                    document.documentElement.scrollHeight,
                    main ? main.scrollHeight : 0
                );
                
                return {
                    success: true,
                    bodyHeight: document.body.scrollHeight,
                    htmlHeight: document.documentElement.scrollHeight,
                    mainHeight: main ? main.scrollHeight : 0,
                    finalHeight: finalHeight
                };
            }
        """)
        return result
    
    def compress_image(self, png_path):
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç PNG –≤ JPEG —Å –∑–∞–¥–∞–Ω–Ω—ã–º –∫–∞—á–µ—Å—Ç–≤–æ–º"""
        try:
            # –û—Ç–∫—Ä—ã–≤–∞–µ–º PNG
            with Image.open(png_path) as img:
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ RGB (JPEG –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ø—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç—å)
                if img.mode in ('RGBA', 'LA', 'P'):
                    background = Image.new('RGB', img.size, (255, 255, 255))
                    if img.mode == 'P':
                        img = img.convert('RGBA')
                    background.paste(img, mask=img.split()[-1] if img.mode == 'RGBA' else None)
                    img = background
                elif img.mode != 'RGB':
                    img = img.convert('RGB')
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ JPEG
                jpeg_path = png_path.with_suffix('.jpg')
                img.save(
                    jpeg_path,
                    'JPEG',
                    quality=self.jpeg_quality,
                    optimize=True
                )
                
                original_size = png_path.stat().st_size
                compressed_size = jpeg_path.stat().st_size
                
                # –£–¥–∞–ª—è–µ–º PNG
                png_path.unlink()
                
                return {
                    'original_size': original_size,
                    'compressed_size': compressed_size,
                    'compression_ratio': compressed_size / original_size,
                    'path': str(jpeg_path),
                    'filename': jpeg_path.name
                }
        except Exception as e:
            print(f"    ‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–∂–∞—Ç–∏—è: {e}")
            return None
    
    async def take_full_page_screenshot(self, page, url, filename):
        """–°–æ–∑–¥–∞–µ—Ç –ø–æ–ª–Ω–æ—Å—Ç—Ä–∞–Ω–∏—á–Ω—ã–π —Å–∫—Ä–∏–Ω—à–æ—Ç —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π"""
        try:
            await page.goto(url, wait_until='networkidle', timeout=60000)
            await page.wait_for_timeout(2000)
            
            await self.hide_sidebar_elements(page)
            await page.wait_for_timeout(1000)
            
            page_title = await self.get_page_title(page)
            section_info = self.extract_section_info(url)
            
            print(f"  –†–∞–∑–≤–æ—Ä–∞—á–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã...")
            expand_result = await self.expand_all_containers(page)
            
            print(f"  ‚úì –í—ã—Å–æ—Ç–∞: {expand_result.get('finalHeight')}px")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º PNG
            png_path = self.output_dir / f"{filename}.png"
            
            await page.screenshot(
                path=str(png_path),
                full_page=True
            )
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ JPEG
            print(f"  –°–∂–∏–º–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (–∫–∞—á–µ—Å—Ç–≤–æ={self.jpeg_quality})...")
            compression_result = self.compress_image(png_path)
            
            if compression_result:
                original_mb = compression_result['original_size'] / 1024 / 1024
                compressed_mb = compression_result['compressed_size'] / 1024 / 1024
                ratio = compression_result['compression_ratio'] * 100
                
                # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä—ã
                with Image.open(compression_result['path']) as img:
                    width, height = img.size
                
                print(f"  ‚úì {page_title}")
                print(f"  ‚úì –†–∞–∑–º–µ—Ä: {width}x{height}px")
                print(f"  ‚úì –§–∞–π–ª: {original_mb:.1f}MB ‚Üí {compressed_mb:.1f}MB ({ratio:.0f}%)\n")
                
                return {
                    'success': True,
                    'mode': 'playwright_full_page_compressed',
                    'title': page_title,
                    'section_info': section_info,
                    'width': width,
                    'height': height,
                    'original_size_mb': original_mb,
                    'compressed_size_mb': compressed_mb,
                    'compression_ratio': ratio,
                    'screenshot': {
                        'path': compression_result['path'],
                        'filename': compression_result['filename']
                    }
                }
            else:
                return {'success': False, 'error': 'Compression failed'}
                
        except Exception as e:
            print(f"  ‚úó –û—à–∏–±–∫–∞: {e}")
            import traceback
            traceback.print_exc()
            return {'success': False, 'error': str(e)}
    
    async def scrape_all(self, max_pages=None):
        start_time = datetime.now()
        results = []
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            
            # device_scale_factor –≤–ª–∏—è–µ—Ç –Ω–∞ DPI
            context = await browser.new_context(
                viewport={'width': 1920, 'height': 1080},
                device_scale_factor=self.scale_factor  # 0.75 = 75% –∫–∞—á–µ—Å—Ç–≤–∞
            )
            page = await context.new_page()
            
            links = await self.collect_tutorial_links(page)
            if max_pages:
                links = links[:max_pages]
            
            print(f"\n{'='*60}")
            print(f"–°–∫—Ä–∏–Ω—à–æ—Ç—ã –¥–ª—è {len(links)} —Å—Ç—Ä–∞–Ω–∏—Ü")
            print(f"DPI: {self.scale_factor*100:.0f}%, JPEG –∫–∞—á–µ—Å—Ç–≤–æ: {self.jpeg_quality}")
            print(f"{'='*60}\n")
            
            for i, url in enumerate(links, 1):
                print(f"[{i}/{len(links)}] {url}")
                
                filename = self.sanitize_filename(url)
                screenshot_result = await self.take_full_page_screenshot(page, url, filename)
                
                result = {
                    'index': i,
                    'url': url,
                    'filename': filename,
                    'timestamp': datetime.now().isoformat(),
                    **screenshot_result
                }
                
                results.append(result)
                await asyncio.sleep(1.5)
            
            await browser.close()
        
        total_original = sum(r.get('original_size_mb', 0) for r in results if r['success'])
        total_compressed = sum(r.get('compressed_size_mb', 0) for r in results if r['success'])
        
        metadata = {
            'scrape_date': start_time.isoformat(),
            'duration_seconds': (datetime.now() - start_time).total_seconds(),
            'total_pages': len(links),
            'successful': len([r for r in results if r['success']]),
            'method': 'playwright_full_page_compressed',
            'scale_factor': self.scale_factor,
            'jpeg_quality': self.jpeg_quality,
            'sidebar_hidden': self.hide_sidebar,
            'total_size_mb': {
                'original': total_original,
                'compressed': total_compressed,
                'saved': total_original - total_compressed
            },
            'pages': results
        }
        
        metadata_path = self.output_dir / 'metadata.json'
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        print(f"\n{'='*60}")
        print(f"–ì–æ—Ç–æ–≤–æ! {metadata['successful']}/{metadata['total_pages']}")
        print(f"–≠–∫–æ–Ω–æ–º–∏—è: {total_original:.1f}MB ‚Üí {total_compressed:.1f}MB")
        print(f"–í—Ä–µ–º—è: {metadata['duration_seconds']:.1f} —Å–µ–∫")
        print(f"{'='*60}\n")
        
        return metadata


# ============================================================
# –ó–ê–ü–£–°–ö –° –†–ê–ó–ù–´–ú–ò –ù–ê–°–¢–†–û–ô–ö–ê–ú–ò
# ============================================================

# –í–∞—Ä–∏–∞–Ω—Ç 1: –°—Ä–µ–¥–Ω—è—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)
scraper = CloudRuDocsScraper(
    output_dir=OUTPUT_DIR,
    hide_sidebar=True,
    scale_factor=0.75,    # 75% DPI (1440px –≤–º–µ—Å—Ç–æ 1920px)
    jpeg_quality=70       # –ö–∞—á–µ—Å—Ç–≤–æ JPEG 70%
)

# –í–∞—Ä–∏–∞–Ω—Ç 2: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
# scraper = CloudRuDocsScraper(
#     output_dir=OUTPUT_DIR,
#     hide_sidebar=True,
#     scale_factor=0.5,     # 50% DPI (960px –≤–º–µ—Å—Ç–æ 1920px)
#     jpeg_quality=75       # –ö–∞—á–µ—Å—Ç–≤–æ JPEG 75%
# )

# –í–∞—Ä–∏–∞–Ω—Ç 3: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
# scraper = CloudRuDocsScraper(
#     output_dir=OUTPUT_DIR,
#     hide_sidebar=True,
#     scale_factor=1.0,     # 100% DPI
#     jpeg_quality=85       # –ö–∞—á–µ—Å—Ç–≤–æ JPEG 85%
# )

# metadata = await scraper.scrape_all(max_pages=3)
# –ü–æ–ª–Ω—ã–π –∑–∞–ø—É—Å–∫
metadata = await scraper.scrape_all()



