import os
import sys
import json
import base64
import logging
import time
import csv
from pathlib import Path
from io import BytesIO
from datetime import datetime
from typing import List, Dict, Optional, Tuple, Any
from dataclasses import dataclass, field, asdict




# ============================================================================
# –ì–õ–û–ë–ê–õ–¨–ù–´–ï –ü–ï–†–ï–ú–ï–ù–ù–´–ï
# ============================================================================
# –î–æ–±–∞–≤–∏—Ç—å –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ –≤ PYTHONPATH
PROJECT_ROOT = Path.cwd().resolve()
if PROJECT_ROOT.name != "Team_5_AI_Tutor":
    PROJECT_ROOT = PROJECT_ROOT.parents[1]
sys.path.insert(0, str(PROJECT_ROOT))

DATA_DIR = PROJECT_ROOT / "data"
OUTPUT_DIR = DATA_DIR / "raw" / "cloud_ru"

# –°–æ–∑–¥–∞—ë–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

print("‚úÖ –Ø—á–µ–π–∫–∞ 1: –ò–º–ø–æ—Ä—Ç—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
print(f"   üìÇ –ö–æ—Ä–Ω–µ–≤–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {PROJECT_ROOT}")
print(f"   üìÇ –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {OUTPUT_DIR}")



import asyncio
from playwright.async_api import async_playwright
from pathlib import Path
import json
from datetime import datetime
import re
from urllib.parse import urlparse

class CloudRuDocsScraper:
    def __init__(self, output_dir=OUTPUT_DIR):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.base_url = "https://cloud.ru/docs/tutorials-evolution/list/doc-contents?source-platform=Evolution"
        self.domain = "https://cloud.ru"
        
    def sanitize_filename(self, url):
        """–°–æ–∑–¥–∞–µ—Ç –±–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞ –∏–∑ URL"""
        parsed = urlparse(url)
        path = parsed.path.strip('/')
        filename = path.replace('/', '_')
        filename = re.sub(r'[<>:"|?*]', '', filename)
        
        if len(filename) > 200:
            filename = filename[:200]
        
        return filename or "index"
    
    def extract_section_info(self, url):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ä–∞–∑–¥–µ–ª–µ –∏–∑ URL"""
        parsed = urlparse(url)
        path_parts = parsed.path.strip('/').split('/')
        
        # –ü—Ä–∏–º–µ—Ä: /docs/tutorials-evolution/infrastructure/list/topics/...
        section_info = {
            'category': None,
            'subcategory': None,
            'topic': None
        }
        
        if len(path_parts) >= 3:
            section_info['category'] = path_parts[2]  # infrastructure, ai-factory, etc.
        
        if len(path_parts) >= 5:
            section_info['subcategory'] = path_parts[4]  # topics, etc.
        
        if len(path_parts) >= 6:
            section_info['topic'] = path_parts[5]  # wordpress, nextcloud, etc.
        
        return section_info
    
    async def collect_tutorial_links(self, page):
        """–°–æ–±–∏—Ä–∞–µ—Ç –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞"""
        print("–ó–∞–≥—Ä—É–∂–∞–µ–º –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É...")
        await page.goto(self.base_url, wait_until='networkidle')
        await page.wait_for_selector('a', timeout=10000)
        
        links = await page.evaluate(f'''() => {{
            const domain = "{self.domain}";
            const links = Array.from(document.querySelectorAll('a'));
            return links
                .map(link => link.href)
                .filter(href => 
                    href && 
                    href.startsWith(domain) && 
                    href.includes('/docs/') &&
                    get_ipython().getoutput("href.includes('#')")
                );
        }}''')
        
        unique_links = list(set(links))
        print(f"–ù–∞–π–¥–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫: {len(unique_links)}")
        return unique_links
    
    async def get_page_title(self, page):
        """–ü–æ–ª—É—á–∞–µ—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            title = await page.evaluate('''() => {
                const h1 = document.querySelector('h1');
                return h1 ? h1.textContent.trim() : '';
            }''')
            return title or "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞"
        except:
            return "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞"
    
    async def scroll_page(self, page):
        """–ü—Ä–æ–∫—Ä—É—á–∏–≤–∞–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ lazy-loaded –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        await page.evaluate("""
            async () => {
                await new Promise((resolve) => {
                    let totalHeight = 0;
                    const distance = 100;
                    const timer = setInterval(() => {
                        const scrollHeight = document.body.scrollHeight;
                        window.scrollBy(0, distance);
                        totalHeight += distance;

                        if(totalHeight >= scrollHeight){
                            clearInterval(timer);
                            resolve();
                        }
                    }, 100);
                });
            }
        """)
        await page.wait_for_timeout(2000)
    
    async def take_screenshot(self, page, url, filename):
        """–°–æ–∑–¥–∞–µ—Ç —Å–∫—Ä–∏–Ω—à–æ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã (full-page –∏–ª–∏ –ø–æ —á–∞—Å—Ç—è–º)"""
        try:
            await page.goto(url, wait_until='networkidle', timeout=60000)
            await page.wait_for_timeout(1000)
            
            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
            page_height = await page.evaluate("document.body.scrollHeight")
            viewport_height = page.viewport_size["height"]
            page_title = await self.get_page_title(page)
            section_info = self.extract_section_info(url)
            
            # –ï—Å–ª–∏ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –¥–ª–∏–Ω–Ω–∞—è (> 15000px), –¥–µ–ª–∏–º –Ω–∞ —á–∞—Å—Ç–∏
            if page_height > 15000:
                print(f"  –î–ª–∏–Ω–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ ({page_height}px), –¥–µ–ª–∏–º –Ω–∞ —á–∞—Å—Ç–∏")
                return await self.take_chunked_screenshots(
                    page, url, filename, page_title, section_info, page_height, viewport_height
                )
            else:
                # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π full-page —Å–∫—Ä–∏–Ω—à–æ—Ç
                await self.scroll_page(page)
                await page.evaluate("window.scrollTo(0, 0)")
                await page.wait_for_timeout(500)
                
                output_path = self.output_dir / f"{filename}.png"
                await page.screenshot(
                    path=output_path,
                    full_page=True,
                    type='png'
                )
                
                return {
                    'success': True,
                    'mode': 'full_page',
                    'title': page_title,
                    'section_info': section_info,
                    'height': page_height,
                    'screenshots': [{
                        'path': str(output_path),
                        'filename': output_path.name,
                        'index': 0,
                        'scroll_position': 0
                    }]
                }
                
        except Exception as e:
            print(f"  ‚úó –û—à–∏–±–∫–∞: {e}")
            return {
                'success': False,
                'error': str(e)
            }
    
    async def take_chunked_screenshots(self, page, url, base_filename, page_title, section_info, page_height, viewport_height):
        """–°–æ–∑–¥–∞–µ—Ç —Å–∫—Ä–∏–Ω—à–æ—Ç—ã –ø–æ —á–∞—Å—Ç—è–º"""
        try:
            screenshots = []
            scroll_position = 0
            chunk_index = 0
            overlap = 50  # –ù–µ–±–æ–ª—å—à–æ–µ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ
            
            while scroll_position < page_height:
                await page.evaluate(f"window.scrollTo(0, {scroll_position})")
                await page.wait_for_timeout(300)
                
                # –§–æ—Ä–º–∏—Ä—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞ —Å –∏–Ω–¥–µ–∫—Å–æ–º —á–∞—Å—Ç–∏
                chunk_filename = f"{base_filename}_part_{chunk_index:03d}.png"
                chunk_path = self.output_dir / chunk_filename
                
                await page.screenshot(path=chunk_path, type='png')
                
                screenshots.append({
                    'path': str(chunk_path),
                    'filename': chunk_filename,
                    'index': chunk_index,
                    'scroll_position': scroll_position
                })
                
                scroll_position += viewport_height - overlap
                chunk_index += 1
            
            print(f"  ‚úì –°–æ–∑–¥–∞–Ω–æ {chunk_index} —á–∞—Å—Ç–µ–π")
            
            return {
                'success': True,
                'mode': 'chunked',
                'title': page_title,
                'section_info': section_info,
                'height': page_height,
                'total_parts': chunk_index,
                'screenshots': screenshots
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e)
            }
    
    async def scrape_all(self, max_pages=None):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –¥–ª—è —Å–±–æ—Ä–∞ –≤—Å–µ—Ö —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤"""
        start_time = datetime.now()
        results = []
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(
                headless=True,
                args=['--disable-blink-features=AutomationControlled']
            )
            
            context = await browser.new_context(
                viewport={'width': 1920, 'height': 1080},
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            )
            
            page = await context.new_page()
            
            # –°–æ–±–∏—Ä–∞–µ–º —Å—Å—ã–ª–∫–∏
            links = await self.collect_tutorial_links(page)
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            if max_pages:
                links = links[:max_pages]
            
            print(f"\n{'='*60}")
            print(f"–ù–∞—á–∏–Ω–∞–µ–º —Å–æ–∑–¥–∞–Ω–∏–µ —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤ –¥–ª—è {len(links)} —Å—Ç—Ä–∞–Ω–∏—Ü")
            print(f"{'='*60}\n")
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—É—é —Å—Å—ã–ª–∫—É
            for i, url in enumerate(links, 1):
                print(f"[{i}/{len(links)}] {url}")
                
                filename = self.sanitize_filename(url)
                screenshot_result = await self.take_screenshot(page, url, filename)
                
                result = {
                    'index': i,
                    'url': url,
                    'base_filename': filename,
                    'timestamp': datetime.now().isoformat(),
                    **screenshot_result
                }
                
                results.append(result)
                
                if screenshot_result['success']:
                    title = screenshot_result.get('title', 'N/A')
                    section = screenshot_result.get('section_info', {})
                    mode = screenshot_result['mode']
                    
                    print(f"  –ó–∞–≥–æ–ª–æ–≤–æ–∫: {title}")
                    print(f"  –†–∞–∑–¥–µ–ª: {section.get('category', 'N/A')} / {section.get('topic', 'N/A')}")
                    
                    if mode == 'chunked':
                        print(f"  ‚úì –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {screenshot_result['total_parts']} —á–∞—Å—Ç–µ–π")
                    else:
                        print(f"  ‚úì –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ (full-page)")
                
                # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                await asyncio.sleep(1)
            
            await browser.close()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        metadata = {
            'scrape_date': start_time.isoformat(),
            'duration_seconds': (datetime.now() - start_time).total_seconds(),
            'total_pages': len(links),
            'successful': len([r for r in results if r['success']]),
            'failed': len([r for r in results if not r['success']]),
            'pages': results
        }
        
        metadata_path = self.output_dir / 'metadata.json'
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        # –°–æ–∑–¥–∞–µ–º —É–¥–æ–±–Ω—ã–π –∏–Ω–¥–µ–∫—Å –¥–ª—è –Ω–∞–≤–∏–≥–∞—Ü–∏–∏
        self._create_index(results)
        
        print(f"\n{'='*60}")
        print(f"–ì–æ—Ç–æ–≤–æ!")
        print(f"–£—Å–ø–µ—à–Ω–æ: {metadata['successful']}/{metadata['total_pages']}")
        print(f"–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {metadata['duration_seconds']:.1f} —Å–µ–∫")
        print(f"–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: {metadata_path}")
        print(f"{'='*60}\n")
        
        return metadata
    
    def _create_index(self, results):
        """–°–æ–∑–¥–∞–µ—Ç —É–ø—Ä–æ—â–µ–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
        index = {}
        
        for result in results:
            if not result['success']:
                continue
            
            section = result.get('section_info', {}).get('category', 'unknown')
            
            if section not in index:
                index[section] = []
            
            index[section].append({
                'title': result.get('title', 'N/A'),
                'url': result['url'],
                'base_filename': result['base_filename'],
                'mode': result['mode'],
                'parts': result.get('total_parts', 1),
                'screenshots': [s['filename'] for s in result.get('screenshots', [])]
            })
        
        index_path = self.output_dir / 'index.json'
        with open(index_path, 'w', encoding='utf-8') as f:
            json.dump(index, f, ensure_ascii=False, indent=2)
        
        print(f"–ò–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {index_path}")





# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
scraper = CloudRuDocsScraper(output_dir=OUTPUT_DIR)
    
# –î–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è'
metadata = await scraper.scrape_all(max_pages=3)
    
# –ü–æ–ª–Ω—ã–π –∑–∞–ø—É—Å–∫
# metadata = await scraper.scrape_all()



